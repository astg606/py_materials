{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://github.com/astg606/py_materials/blob/master/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Image Classification with Scikit-Learn</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "  <head> </head>\n",
    "  <body>\n",
    "<script src=\"https://bot.voiceatlas.mysmce.com/v1/chatlas.js\"></script>\n",
    "<app-chatlas\n",
    "\tatlas-id=\"f759a188-f8bb-46bb-9046-3b1b961bd6aa\"\n",
    "\twidget-background-color=\"#3f51b5ff\"\n",
    "\twidget-text-color=\"#ffffffff\"\n",
    "\twidget-title=\"Chatlas\">\n",
    "</app-chatlas>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "- <a href=\"https://www.dataquest.io/blog/sci-kit-learn-tutorial/\">Scikit-learn Tutorial: Machine Learning in Python</a>\n",
    "- <a href=\"https://debuggercafe.com/image-classification-with-mnist-dataset/\">Image Classification with MNIST Dataset</a>\n",
    "- <a href=\"https://davidburn.github.io/notebooks/mnist-numbers/MNIST%20Handwrititten%20numbers/\">MNIST handwritten number identification</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy version:        {np.__version__}\")\n",
    "print(f\"Pandas version:       {pd.__version__}\")\n",
    "print(f\"Seaborn version:      {sns.__version__}\")\n",
    "print(f\"Scikit-Learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> MNIST Dataset</font>\n",
    "\n",
    "- The <A HREF=\"https://en.wikipedia.org/wiki/MNIST_database\"> MNIST database</A> (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "- The database is also widely used for training and testing in the field of machine learning.\n",
    "- The dataset we will be using contains 70000 images of handwritten digits among which 10000 are reserved for testing.\n",
    "- This dataset is  suitable for anyone who wants to get started with image classification using Scikit-Learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist_data = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys: \", mnist_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the `data` and `target` already separated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of Data: {mnist_data.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datatype of Data: {type(mnist_data.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the Target Data: {mnist_data.target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datatype of Target Data: {type(mnist_data.target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature Names: {mnist_data.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Url: {mnist_data.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the feature and target arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data, np_target = mnist_data['data'], mnist_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Shape of data:   {np_data.shape}')\n",
    "print(f' Shape of target: {np_target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(np_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(np_data.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datatype of the target values: {np_target.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print few values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_target[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the labels from string to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target = np_target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_target[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of unique labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = np_target.value_counts().sort_values(ascending=False)\n",
    "percent = (np_target.value_counts()/np_target.count()).sort_values(ascending=False)*100\n",
    "percent_data = pd.concat([total, percent], axis=1, \n",
    "                         keys=['Total', 'Percent'])\n",
    "percent_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "There are 70000 numbers, each stored as an array of 784 numbers depicting the opacity of each pixel, it can be displayed by reshaping the data into a 28x28 array and plotting using matplotlib. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 15657\n",
    "some_digit = np_data.values[some_index]\n",
    "some_digit_image = some_digit.reshape(28,28)\n",
    "\n",
    "plt.imshow(some_digit_image, \n",
    "           cmap = matplotlib.cm.binary, \n",
    "           interpolation='nearest')\n",
    "plt.axis=('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the target for row `some_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target[some_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display few images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def display_digits(X, y):\n",
    "    \"\"\"\n",
    "      Given an array of images of digits X and \n",
    "      the corresponding values of the digit y,\n",
    "      this function plots 96 unique randomly selected images \n",
    "      and their values.\n",
    "    \"\"\"\n",
    "    # Figure size (width, height) in inches\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Adjust the subplots \n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, \n",
    "                        hspace=0.05, wspace=0.05)\n",
    "\n",
    "    num_images = X.shape[0]\n",
    "    \n",
    "    num_selected_images = 96\n",
    "    row_indices = random.sample(range(num_images), num_selected_images)\n",
    "    \n",
    "    i = 0\n",
    "    for idx in row_indices:\n",
    "        # Initialize the subplots: \n",
    "        # Add a subplot in the grid of 8 by 12, at the i+1-th position\n",
    "        ax = fig.add_subplot(8, 12, i + 1, xticks=[], yticks=[])\n",
    "        \n",
    "        # Display an image at the i-th position\n",
    "        ax.imshow(X[idx].reshape(28, 28), cmap=plt.cm.binary, \n",
    "                  interpolation='nearest')\n",
    "       \n",
    "        # label the image with the target value\n",
    "        ax.text(0, 7, str(y[idx]))\n",
    "        i += 1\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(np_data.values, np_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Model Selection Process</font>\n",
    "\n",
    "![fig_skl](https://miro.medium.com/max/1400/1*LixatBxkewppAhv1Mm5H2w.jpeg)\n",
    "Image Source: Christophe Bourguignat\n",
    "\n",
    "- A Machine Learning algorithm needs to be trained on a set of data to learn the relationships between different features and how these features affect the target variable. \n",
    "- We need to divide the entire data set into two sets:\n",
    "    + Training set on which we are going to train our algorithm to build a model. \n",
    "    + Testing set on which we will test our model to see how accurate its predictions are.\n",
    "    \n",
    "Before we create the two sets, we need to identify the algorithm we will use for our model.\n",
    "We can use the `machine_learning_map` map (shown at the top of this page) as a cheat sheet to shortlist the algorithms that we can try out to build our prediction model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Separating the Training and Testing Set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first 60000 (among the 70000) images are used for training.\n",
    "- The remaining 10000 images are used for validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 60000\n",
    "\n",
    "X_train = np_data.values[:num_train]\n",
    "X_test  = np_data.values[num_train:]\n",
    "y_train = np_target[:num_train]\n",
    "y_test  = np_target[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Train Data:  {X_train.shape}')\n",
    "print(f' Test Data:   {X_test.shape}')\n",
    "print(f' Train label: {y_train.shape}')\n",
    "print(f' Test Label:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle the training set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = X_train.shape[0]\n",
    "shuffle_index = np.random.permutation(nn)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=\"red\">Training a Binary Classifier</font>\n",
    "\n",
    "- Binary classification means there are two classes to work with that relate to one another as `true` and `false`.\n",
    "- Here, we want to identify a single digit: looking at `9`s.\n",
    "- The classification will tell us if we have a `9` (true) or not (false)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the target arrays as boolean arrays:** true if 9 otherwise false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_9 = (y_train == 9)\n",
    "y_test_9 = (y_test == 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and train the model:**\n",
    "\n",
    "- We use the SGDClassifier that applies regularized linear model with SGD (Stochastic Gradient Descent) learning to build an estimator.\n",
    "- The method helps building an estimator for classification and regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf =SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make an initial prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 15657\n",
    "some_digit = np_data.values[some_index]\n",
    "print(np_target[some_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_predict = sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measuring accuracy using cross validation**\n",
    "\n",
    "- The `stratifiedKfold` class performs stratified sampling to produce folds that contain a representative ratio of each class. \n",
    "- At each iteration the code creates a clone of the classifier, trains that clone on the training fold and then makes predictions on the test fold. \n",
    "- It then counts the number of correct predictions and outputs the ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "n_splits = 3\n",
    "skfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "kfold_scores = list()\n",
    "idx = 0\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_9):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_9[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_9[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    scr = n_correct/len(y_pred)\n",
    "    kfold_scores.append(scr)\n",
    "    print(f\"Test: {idx} -- Score: {scr}\")\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{n_splits}-fold average score: {np.mean(np.array(kfold_scores))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "validation = cross_val_score(sgd_clf, X_train, y_train_9, \n",
    "                             cv=3, \n",
    "                             scoring='accuracy', \n",
    "                             verbose=1)\n",
    "\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn cross_val_score in action returning the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (sum(np_target==9)/len(np_target))*100\n",
    "print(f'{np.max(validation)*100}% accuracy might not as impressive as it sounds \\n where there are {accuracy :.2f}% of 9s in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "- A confusion matrix is a tabular summary of the number of correct and incorrect predictions made by a classifier. \n",
    "- It can be used to evaluate the performance of a classification model through the calculation of performance metrics like accuracy, precision, recall, and F1-score.\n",
    "- The confusion matrix is a much better way to evaluate the performance of a classifier, especially when there is a skewed dataset as we have here with only 10% of the dataset being the target.\n",
    "- Each row represents a class, each column a prediction:\n",
    "   * The first row is negative cases (non-9s) with the top left containing all the correctly classified non-9s (True Negatives), the top right the 9s incorrectly classified as non-9s (False-Positves).\n",
    "   * The second row represents the positive class, 9s in this case, bottom left contains the 9s incorrectly classified as non-9s (False Negatives), the bottom right containing the correctly classified 9s (True Positives)\n",
    "   \n",
    "| | Actual | |\n",
    "| --- |: --- |: --- |\n",
    "| **Prediction** | True Positive | False Positive |\n",
    "| | False Negative | True Negative |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need a set of predictions to compare to the actual targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_9, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = metrics.confusion_matrix(y_train_9, y_train_pred)\n",
    "\n",
    "print(f\"Confusion Matrix: \\n{cf_matrix}\")\n",
    "print(f\"\\n Number of images: {np.sum(cf_matrix)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_train_9, y_train_pred, \n",
    "                              labels=sgd_clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=sgd_clf.classes_)\n",
    "disp.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision/Recall\n",
    "- Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced.\n",
    "- Precision measures the number of true positives (correctly classified 9s) as a ratio of the total samples classified as a 9: $\\frac{T_P}{T_P + F_P}$\n",
    "- Recall measures the number of true positives as a ratio of the total number of positives: $\\frac{T_P}{T_P + F_N}$.\n",
    "- The precision-recall curve shows the tradeoff between precision and recall for different threshold. \n",
    "  - A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. \n",
    "  - A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. \n",
    "  - An ideal system with high precision and high recall will return many results, with all results labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_9, cv=3, method='decision_function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_9, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('Precision and recall vs decision threshold')\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Training and Prediction on the Entire Dataset</font>\n",
    "\n",
    "- We will use the Stochastic Gradient Descent classifier (SGD). \n",
    "- Scikit-Learnâ€™s SGDClassifier is a good starting point for linear classifiers. \n",
    "- Using the loss parameter we will see how Support Vector Machine (Linear SVM) and Logistic Regression perform for the same dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Linear Support Vector Machine (SVM)\n",
    "- We use linear SVM with stochastic gradient descent (SGD) learning.\n",
    "- The gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule.\n",
    "- To use the Linear SVM Classifier, we need to set the loss parameter to `hinge`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    " \n",
    "sgd_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before testing the model, it is a good practice to first see the cross-validation scores on the training data. \n",
    "- That you will give you a very good projection of how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = np.min(valid)*100\n",
    "max_val = np.max(valid)*100\n",
    "print(f'For three-fold Cross-Validation you are getting around: {min_val}%-{max_val}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the actual test scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreSVM = sgd_clf.score(X_test, y_test)\n",
    "print(\"Test score of the Linear SVM: \", scoreSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = sgd_clf.predict(X_test)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_predict, \n",
    "                              labels=sgd_clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=sgd_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(loss='log', random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreLR = sgd_clf.score(X_test, y_test)\n",
    "print(\"Test score of the Logistic Regression: \", scoreLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = sgd_clf.predict(X_test)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_predict, \n",
    "                              labels=sgd_clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=sgd_clf.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "- Random forests is a supervised learning algorithm. \n",
    "- A forest is comprised of trees. \n",
    "- It is said that the more trees it has, the more robust a forest is. \n",
    "- Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. \n",
    "- It also provides a pretty good indicator of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 500)\n",
    "forest = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_output = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest with n_estimators:500\")\n",
    "print(accuracy_score(y_test, forest_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, forest_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, forest_output, \n",
    "                              labels=forest.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=forest.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "                                 max_depth=1, random_state=0).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_output = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gbc_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, gbc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, gbc_output, \n",
    "                              labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier\n",
    "\n",
    "- The Multi-layer Perceptron classifier relies on an underlying Neural Network to perform the task of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the Stochastic Gradient Descent (`sgd`) Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(10,), \n",
    "                    random_state=1)\n",
    "clf.fit(X_train, y_train)   \n",
    "neural_output = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MLP sgd Accuracy: {accuracy_score(y_test, neural_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, neural_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, neural_output, \n",
    "                              labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the Quasi-Newton (`lbfgs`) Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,), \n",
    "                    random_state=1)\n",
    "clf.fit(X_train, y_train)   \n",
    "neural_output = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MLP lbfgs Accuracy: {accuracy_score(y_test, neural_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, neural_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, neural_output, \n",
    "                              labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
