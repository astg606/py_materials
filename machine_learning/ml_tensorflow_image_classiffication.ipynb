{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Image Classification with Tensorflow</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Reference\n",
    "\n",
    "- [MNIST digits classification with TensorFlow 2](https://github.com/antonio-f/TensorFlow2_digits_classification-Linear_Classifier-MLP/blob/master/TensorFlow2_digits_classification-Linear_Classifier-MLP/digits_classification.ipynb)\n",
    "- [Mnist handwritten digit classification using tensorflow](https://milindsoorya.site/blog/handwritten-digits-classification)\n",
    "- [A real example – recognizing handwritten digits](https://subscription.packtpub.com/book/data/9781838823412/1/ch01lvl1sec08/a-real-example-recognizing-handwritten-digits)\n",
    "- [DIFFERENCE BETWEEN SOFTMAX FUNCTION AND SIGMOID FUNCTION](https://dataaspirant.com/difference-between-softmax-function-and-sigmoid-function/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Version of Numpy:      {np.__version__}\")\n",
    "print(f\"Version of Pandas:     {pd.__version__}\")\n",
    "print(f\"Version of Keras:      {tf.keras.__version__}\")\n",
    "print(f\"Version of TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Image Classification</font> \n",
    "\n",
    "We use the [MNIST data set](http://yann.lecun.com/exdb/mnist/) (Modified National Institute of Standards and Technology database).\n",
    "\n",
    "* Is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "* The database is also widely used for training and testing in the field of machine learning.\n",
    "* The dataset we will be using contains 70000 images of handwritten digits among which 10000 are reserved for testing.\n",
    "* It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TSF](https://static.javatpoint.com/tutorial/tensorflow/images/mnist-dataset-in-cnn.jpg)\n",
    "Image Source: [https://www.javatpoint.com/tensorflow-mnist-dataset-in-cnn](https://www.javatpoint.com/tensorflow-mnist-dataset-in-cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Load MNiST Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape train inputs:  \", x_train.shape)\n",
    "print(\"Shape train outputs: \", y_train.shape)\n",
    "print(\"Shape test  inputs:  \", x_test.shape)\n",
    "print(\"Shape test  outputs: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image parameters to the constants that we will use later \n",
    "# for data re-shaping and for model traning.\n",
    "(_, IMAGE_WIDTH, IMAGE_HEIGHT) = x_train.shape\n",
    "print(f'IMAGE_WIDTH:  {IMAGE_WIDTH}')\n",
    "print(f'IMAGE_HEIGHT: {IMAGE_HEIGHT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type train inputs:  \", x_train.dtype)\n",
    "print(\"Type train outputs: \", y_train.dtype)\n",
    "print(\"Type test  inputs:  \", x_test.dtype)\n",
    "print(\"Type test  outputs: \", y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check an arbitrary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 15657\n",
    "some_digit = x_train[some_index]\n",
    "some_digit_image = some_digit.reshape(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "plt.imshow(some_digit_image, \n",
    "           cmap = matplotlib.cm.binary, \n",
    "           interpolation='nearest')\n",
    "plt.axis=('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[some_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Preprocess the Training and Test Datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the data type\n",
    "\n",
    "- Change the type from integer to floating point. \n",
    "- This will reduce our memory requirements by forcing the precision of the pixel values to be 32 bit, the default precision used by `Keras` anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train --> min = {np.min(x_train)} max = {np.max(x_train)}\")\n",
    "print(f\"Test  --> min = {np.min(x_test)} max = {np.max(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data\n",
    "- The values are from 0.0 to 255.0.\n",
    "- We want to have values between 0.0 and 1.0.\n",
    "- Normalizing the data generally speeds up trainning and leads to faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train --> min = {np.min(x_train)} max = {np.max(x_train)}\")\n",
    "print(f\"Test  --> min = {np.min(x_test)} max = {np.max(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape the data\n",
    "\n",
    "- The training and test datasets are structured as a 3-dimensional array of instance, image width and image height. \n",
    "- For a multi-layer perceptron model we must reduce the images down into a vector of pixels. In this case the `IMAGE_WIDTH*IMAGE_HEIGHT` sized images will be 784 pixel input values.\n",
    "- We can do this transform easily using the `reshape()` function on the NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reshape = x_train.reshape(x_train.shape[0], \n",
    "                                  IMAGE_WIDTH*IMAGE_HEIGHT)\n",
    "x_test_reshape = x_test.reshape(x_test.shape[0], \n",
    "                                IMAGE_WIDTH*IMAGE_HEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert class vectors to binary class matrices\n",
    "\n",
    "- The targets have 10 possible integer values: `0, 1, 2, ..., 9`.\n",
    "- We use the `to_categorical` function to convert integer targets into categorical.\n",
    "  - For instance, `2` would become `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]` (it’s zero-indexed).\n",
    "- We do it because `Keras` will expect the training targets to be 10-dimensional vectors, since there will be 10 nodes in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train_convert = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_convert = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_convert[some_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Model 1: Simple Sequential Model</font>\n",
    "\n",
    "Architecture of the Network is:\n",
    "\n",
    "1. Input layer for `IMAGE_WIDTH*IMAGE_HEIGHT = 28x28 = 784` images in MNiST dataset\n",
    "2. Dense layer with 128 neurons and ReLU activation function\n",
    "3. Output layer with 10 neurons for classification of input images as one of ten digits(0 to 9)\n",
    "\n",
    "#### Remarks\n",
    "- We add a regularization `Dropout` layer to randomly exclude a portion of the neurons (here 20%) in the layer in order to reduce overfitting.\n",
    "- A `softmax` activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the model’s output prediction.\n",
    "  - Converts the result into a probability distribution.\n",
    "  - Calculates probabilities of each target class over all possible target classes\n",
    "  - The values of the output vector are in range (0, 1) and sum to 1. \n",
    "  - `softmax` of input `x` is calculated by function `exp(x)/tf.reduce_sum(exp(x))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_model = tf.keras.models.Sequential()\n",
    "one_layer_model.add(tf.keras.layers.Dense(\n",
    "    128, \n",
    "    activation='relu', \n",
    "    input_shape=(IMAGE_WIDTH*IMAGE_HEIGHT,)))\n",
    "one_layer_model.add(tf.keras.layers.Dropout(0.2))\n",
    "one_layer_model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(\n",
    "#    one_layer_model,\n",
    "#    show_shapes=True,\n",
    "#    show_layer_names=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Compile the Model</font>\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    "- Loss function This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "- Optimizer This is how the model is updated based on the data it sees and its loss function.\n",
    "- Metrics Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n",
    "\n",
    "![MLP](https://m0nads.files.wordpress.com/2021/01/linear_classifier.png)\n",
    "Image Source: m0nads.wordpress.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Training and Validation</font>\n",
    "\n",
    "The `one_layer_model.fit` method adjusts the model parameters to minimize the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_history = one_layer_model.fit(\n",
    "    x_train_reshape, \n",
    "    y_train_convert,\n",
    "    batch_size = batch_size,\n",
    "    epochs = num_epochs,\n",
    "    verbose = 1, \n",
    "    validation_data = (x_test_reshape, y_test_convert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Plot the Deceasing Loss over Epochs</font>\n",
    "\n",
    "Use Pandas to plot a graph showing the decrease in mean squared error (mse) as training improves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(one_layer_history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df[['loss', 'val_loss']].plot(xlabel='Epochs', \n",
    "                                   ylabel='Loss', \n",
    "                                   title='Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df[['accuracy', 'val_accuracy']].plot(xlabel='Epochs', \n",
    "                                   ylabel='Accuracy', \n",
    "                                   title='Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Evaluate the Model</font>\n",
    "\n",
    "The `mnist_model.evaluate` method checks the models performance, usually on a \"Validation-set\" or \"Test-set\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = one_layer_model.evaluate(x_test_reshape,  \n",
    "                                 y_test_convert, \n",
    "                                 verbose=0)\n",
    "print(f'Test loss:     {score[0]}')\n",
    "print(f'Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Visualize Predictions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_digits(X, y):\n",
    "    \"\"\"\n",
    "      Given an array of images of digits X and \n",
    "      the corresponding values of the digit y,\n",
    "      this function plots the first 96 images and their values.\n",
    "    \"\"\"\n",
    "    # Figure size (width, height) in inches\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Adjust the subplots \n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(96):\n",
    "        # Initialize the subplots: \n",
    "        #    Add a subplot in the grid of 8 by 12, at the i+1-th position\n",
    "        ax = fig.add_subplot(8, 12, i + 1, xticks=[], yticks=[])\n",
    "        \n",
    "        # Display an image at the i-th position\n",
    "        ax.imshow(X[i].reshape(28, 28), cmap=plt.cm.binary, interpolation='nearest')\n",
    "       \n",
    "        # label the image with the target value\n",
    "        ax.text(0, 7, str(y[i]))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = one_layer_model.predict(x_test_reshape, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `numpy.argmax` function to return the indices of the maximum values along an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_predicted_labels = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_predicted_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(x_test_reshape, one_layer_predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Confusion Matrix for Validation</font>\n",
    "\n",
    "- We can use the confusion matrix to have a picture of our prediction.\n",
    "- A number `n` in a cell means that we predicted the value in the truth row as the value in the predicted column, `n` times. \n",
    "- All the diagonal elements are correct predictions.\n",
    "- In the example below, the black cells, value shows the wrong predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(labels=y_test, \n",
    "                              predictions=one_layer_predicted_labels)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d');\n",
    "plt.xlabel('Predicted');\n",
    "plt.ylabel('Truth');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_accuracy(cmatrix):\n",
    "    cm_values = cmatrix.numpy()\n",
    "    l = cm_values.shape[0]\n",
    "    diag = [cm_values[i, i] for i in range(l)]\n",
    "    \n",
    "    pds = pd.Series(y_test)\n",
    "\n",
    "    org_total = pds.value_counts().sort_index()\n",
    "    org_percent = (pds.value_counts()/pds.count()).sort_values().sort_index()*100\n",
    "    pred_accu = (pd.Series(diag)/org_total)*100\n",
    "    idx = ['zero', 'one', 'two', 'three', 'four',\n",
    "          'five', 'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "    keys = ['Original Total', 'Predicted Total', 'Predicted Accuracy']\n",
    "    accu_data = pd.concat([org_total, pd.Series(diag), pred_accu], \n",
    "                             axis=1, keys=keys)\n",
    "\n",
    "    accu_data.index = idx\n",
    "    return accu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = determine_accuracy(cm)\n",
    "accuracy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Save the Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_model.save('one_layer_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to reload the model later, we can use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('one_layer_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Model 2: Add a Hidden Layer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(IMAGE_WIDTH*IMAGE_HEIGHT, )),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.sigmoid),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.sigmoid),\n",
    "    tf.keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_history = two_layer_model.fit(\n",
    "    x_train_reshape, \n",
    "    y_train, \n",
    "    batch_size = batch_size,\n",
    "    epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(two_layer_history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df['loss'].plot(xlabel='Epochs', \n",
    "                     ylabel='Loss', \n",
    "                     title='Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df['accuracy'].plot(xlabel='Epochs', \n",
    "                     ylabel='Accuracy', \n",
    "                     title='Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = two_layer_model.evaluate(x_test_reshape,  \n",
    "                                 y_test, \n",
    "                                 verbose=0)\n",
    "print(f'Test loss:     {score[0]}')\n",
    "print(f'Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = two_layer_model.predict(x_test_reshape)\n",
    "two_layer_predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[:10])\n",
    "print(two_layer_predicted_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(labels=y_test, \n",
    "                              predictions=two_layer_predicted_labels)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d');\n",
    "plt.xlabel('Predicted');\n",
    "plt.ylabel('Truth');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = determine_accuracy(cm)\n",
    "accuracy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
