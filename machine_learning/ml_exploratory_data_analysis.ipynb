{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg)\n",
    "\n",
    "<center>\n",
    "<h1><font size=\"+3\">NCCS Training Course Series</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Introduction to Exploratory Data Analysis</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "- <a href=\"https://www.itl.nist.gov/div898/handbook/eda/eda.htm\"> Exploratory Data Analysis </a> (from NIST)\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Exploratory_data_analysis\">Exploratory Data Analysis</a> (from Wikipedia)\n",
    "- <a href=\"https://www.edureka.co/blog/exploratory-data-analysis-in-python/\">The Why And How Of Exploratory Data Analysis In Python</a>\n",
    "- <a href=\"https://kite.com/blog/python/data-analysis-visualization-python/\"> Exploratory Data Analysis (EDA) and Data Visualization with Python</a>\n",
    "- <a href=\"https://datajournalism.com/read/handbook/one/understanding-data/using-data-visualization-to-find-insights-in-data\">Using Data Visualization to Find Insights in Data</a>\n",
    "\n",
    "![fig_cartoon](https://i2.wp.com/timewellspent.kronos.com/wp-content/uploads/2013/07/tws18-600-bigdatainsights.jpg?resize=600%2C343)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">What is Exploratory Data Analysis?</font>\n",
    "\n",
    "> Exploratory Data Analysis (EDA) is an attitude, a state of flexibility, a willingness to look for those things that we believe are not there, as well as those we believe to be there. -- John W. Tukey \n",
    "\n",
    "EDA is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. \n",
    "- EDA is an important step to first understand the data (identify the trends and patterns within the data, detect outliers or anomalous events, find interesting relations among the variables, points of interesct, etc.) before using them for modeling, machine learning, etc.\n",
    "- By skipping this first exploratory step, data scientists might not be able to immediately understand key issues in the data or be able to guide deeper analysis in the right direction. \n",
    "- The gathering of information on the data are:\n",
    "    + To make sure the data is valid\n",
    "    + To verify that there are no obvious problems.\n",
    "- EDA employs a variety of techniques (mostly graphical) to:\n",
    "    1. Maximize insight into a data set\n",
    "    2. Uncover underlying structure\n",
    "    3. Extract important variables\n",
    "    4. Detect outliers and anomalies\n",
    "    5. Test underlying assumptions\n",
    "    6. Develop parsimonious models, and\n",
    "    7. Determine optimal factor settings.\n",
    "\n",
    "![FIG_AXES](https://blog.camelot-group.com/wp-content/uploads/2019/03/Picture2-580x330.png)\n",
    "Image Source: <a href=\"https://blog.camelot-group.com/2019/03/exploratory-data-analysis-an-important-step-in-data-science/\">Frank Kienle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Some Graphical Techniques in EDA</font>\n",
    "\n",
    "Graphical techniques employed in EDA are often quite simple, consisting of:\n",
    "\n",
    "- Plotting the raw data:\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/runseqpl.htm\">Data traces</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/histogra.htm\">Histograms</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/bihistog.htm\">Bihistograms</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/probplot.htm\">Probability plots</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/lagplot.htm\">Lag plots</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/blockplo.htm\">Block plots</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/youdplot.htm\">Youden plots</a>.\n",
    "- Plotting simple statistics\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/meanplot.htm\">Mean plots</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/sdplot.htm\">Standard deviation plots</a>\n",
    "     + <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/boxplot.htm\">Box plots</a>\n",
    "     + Main effects plots of the raw data.\n",
    "- Positioning such plots so as to maximize our natural pattern-recognition abilities, such as using multiple plots per page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Basic Processes of Finding Insights in Data</font>\n",
    "\n",
    "![fig_insights](https://s3.eu-central-1.amazonaws.com/datajournalismcom/handbooks/Data-Handbook-1/using-data-visualization-to-find-insights-in-data/05-BB.png)\n",
    "Image Source: Gregor Aisch\n",
    "\n",
    "1. **Visualize the Data**\n",
    "     + Use tools to have a graphical representation of the data.\n",
    "2. **Analyze and Interpret**\n",
    "     + What can you learn from the different visualizations?\n",
    "3. **Transform the Data**: \n",
    "     + Take care of missing values\n",
    "     + Deal with outliers\n",
    "     + Do you need to perform any dimension reduction?\n",
    "4. **Document**\n",
    "     + Keep track of all the generated plots and statistics in order to better understand their meanings and their unsefulness in finding insights in the available data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Understanding the Data Types</font>\n",
    "\n",
    "There are two basic types of structured data:\n",
    "\n",
    "* **Numeric Data**\n",
    "    - Continuous: Data can take on any value in an interval.\n",
    "    - Discrete: Data can take only integer values, such as counts.\n",
    "* **Categorical Data**\n",
    "    - Nominal: Numbers are used simply to distinguish between different properties (for instance the marital status or the state name (Arizona, California, etc.). The numbers can only be compared but cannot be ordered.\n",
    "    - Ordinal: Numerical values serve to place categories in some meaningful order (for example, Customer Satisfaction Scales)\n",
    "    \n",
    "![fig_data](https://bioquest.org/numberscount/wp-content/blogs.dir//files//2010/01/Screen-shot-2010-01-04-at-1.12.19-PM.png)\n",
    "Image Source: bioquest.org \n",
    "    \n",
    "The data type you have will determine the EDA graphical method to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Goals</font>\n",
    "\n",
    "We want to use Pandas and Seaborn to:\n",
    "\n",
    "- Explore a numerical dataset and create visual distributions\n",
    "- Identify and eliminate outliers (if any)\n",
    "- Uncover correlations between two datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Obtain the Dataset</font>\n",
    "- We use the Bostoon dataset.\n",
    "- Contains information about different houses in Boston.\n",
    "- There are 506 samples and 13 feature variables in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Information:\n",
    "| Acronym | Description |\n",
    "| --- | --- |\n",
    "| **CRIM** |    Per capita crime rate by town |\n",
    "|**ZN** |   Proportion of residential land zoned for lots over 25,000 sq.ft. |\n",
    "| **INDUS** | Proportion of non-retail business acres per town |\n",
    "| **CHAS** |  Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |\n",
    "| **NOX** |  Nitric oxides concentration (parts per 10 million) |\n",
    "| **RM** |    Average number of rooms per dwelling |\n",
    "| **AGE** |   roportion of owner-occupied units built prior to 1940 |\n",
    "| **DIS** |  weighted distances to five Boston employment centres |\n",
    "| **RAD** |   index of accessibility to radial highways |\n",
    "| **TAX** |  full-value property-tax rate per \\$10,000 |\n",
    "| **PTRATIO** |  pupil-teacher ratio by town |\n",
    "| **B** |       1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town |\n",
    "| **LSTAT** |    % lower status of the population |\n",
    "| **MEDV** |    Median value of owner-occupied homes in $1000's |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_dst = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of the dataset: \", type(boston_dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Features of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys: \", boston_dst.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset Key | Description |\n",
    "| --- | --- |\n",
    "| **data** | the data to learn |\n",
    "| **target** | the regression target |\n",
    "| **feature_names** | the meaning of the labels |\n",
    "| **DESCR** | the full description of the dataset |\n",
    "| **filename** | csv file contains the data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the file name (full path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dst.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dst.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type  of data: \", type(boston_dst.data))\n",
    "print(\"Shape of data: \", boston_dst.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Names: \", boston_dst.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the array containing home prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type  of target: \", type(boston_dst.target))\n",
    "print(\"Shape of target: \", boston_dst.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the Data into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `boston_dst.data` is a Numpy array and can be used to create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd = pd.DataFrame(boston_dst.data)\n",
    "boston_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relabel the columns using the Boston dataset feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.columns = boston_dst.feature_names\n",
    "boston_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add home prices to the Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd['PRICE'] = boston_dst.target\n",
    "boston_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Exploratory Data Analysis</font>\n",
    "\n",
    "- Important step before training the model. \n",
    "- We use statistical analysis and visualizations to understand the relationship of the target variable with other features.\n",
    "\n",
    "![fig_EDA](https://cdn-images-1.medium.com/max/1200/1*3sr-fMg4S_yEncGsIjyG9A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">STEP 1: Description of Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Overview of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics\n",
    "- A helpful way to understand characteristics of the data and to get a quick summary of it.\n",
    "- Provide basic statistical computations on the dataset like extreme values, count of data points, maximum value, minimum value, mean, standard deviation, etc.\n",
    "- Any missing value or NaN value is automatically skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the method `value_counts` to get count of each category in a categorical attributed series of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.PRICE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the number of unique elements in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_pd.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">STEP 2: Dealing with Missing Data </font>\n",
    "\n",
    "- Missing values in the dataset refer to those fields which are empty or no values assigned to them, these usually occur due to data entry errors, faults that occur with data collection processes.\n",
    "- They need to be handled carefully because they reduce the quality of the data.\n",
    "- They can lead to wrong prediction or classification and can also cause a high bias for any given model being used. \n",
    "- It is a good practice to see if there are any missing values in the data. \n",
    "- The way to handle missing values depends on the nature of our data and the missing values. Techniques may include:\n",
    "     + Drop NULL or missing values\n",
    "     + Fill Missing Values\n",
    "     + Predict Missing values with an ML Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count the number of missing values for each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = boston_pd.isnull().sum().sort_values(ascending=False)\n",
    "percent = (boston_pd.isnull().sum()/boston_pd.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, \n",
    "                         keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NULL or Missing Values\n",
    "- Fastest and easiest step to handle missing values.\n",
    "- Not generally advised for it can deteriorate the quality of the model as it reduces sample size because it works by deleting all other observations where any of the variables is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd = boston_pd.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd = boston_pd.drop((missing_data[missing_data['Total'] > 1]).index, 1)\n",
    "boston_pd = boston_pd.drop(boston_pd.loc[boston_pd['CRIM'].isnull()].index)\n",
    "boston_pd.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill Missing Values\n",
    "- Most common method of handling missing values.\n",
    "- Missing values are replaced with a test statistic like mean, median or mode of the particular feature the missing value belongs to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a missing value of age in the boston data set. Then the below code will fill the missing value with the 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.AGE = boston_pd.AGE.fillna(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the Missing Values with a ML Algorithm\n",
    "- Best and most efficient methods for handling missing data.\n",
    "- Depending on the class of data that is missing, one can either use a regression or classification model to predict missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">STEP 3: Handling Outliers </font>\n",
    "\n",
    "- An outlier is something which is separate or different from the crowd.\n",
    "- Outliers can be a result of a mistake during data collection or it can be just an indication of variance in your data. \n",
    "- It is wise to be correct or remove any outlier from the data before calculating summary statistics or deriving insights from the data.\n",
    "- Failing to properly handle outliers might lead to incorrect analysis.\n",
    "- For detecting and handling outliers, we can consider the following techniques:\n",
    "     + Scatterplot\n",
    "     + BoxPlot\n",
    "     + IQR (Inter-Quartile Range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoxPlot\n",
    "- Pictorial representation of distribution of data which shows extreme values, median and quartiles.\n",
    "- Boxplots show robust measures of location and spread as well as providing information about symmetry and outliers.\n",
    "    + The range of the data provides us with a measure of spread and is equal to a value between the smallest data point (min) and the largest one (Max)\n",
    "    + The interquartile range (IQR), which is the range covered by the middle 50% of the data.\n",
    "    + IQR = Q3 – Q1, the difference between the third and first quartiles. The first quartile (Q1) is the value such that one quarter (25%) of the data points fall below it, or the median of the bottom half of the data. The third quartile is the value such that three quarters (75%) of the data points fall below it, or the median of the top half of the data.\n",
    "    + The IQR can be used to detect outliers using the 1.5(IQR) criteria. Outliers are observations that fall below Q1 – 1.5(IQR) or above Q3 + 1.5(IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(list(boston_pd.PRICE)); \n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  ax = plt.subplots(len(list(boston_pd.columns)), figsize=(8,40))\n",
    "\n",
    "for i, feature_name in enumerate(list(boston_pd.columns)):\n",
    "    sns.boxplot(y=boston_pd[feature_name], ax=ax[i]);\n",
    "    ax[i].set_xlabel(feature_name, fontsize=8);\n",
    "    #ax[i].set_title(\"Box plot {} \".format(feature_name), fontsize=8);\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot\n",
    "- A mathematical diagram using Cartesian coordinates to display values for two variables for a set of data. \n",
    "- The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. \n",
    "- **The points that are far from the population can be termed as an outlier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in boston_dst.feature_names:\n",
    "    plt.figure(figsize=(5, 4));\n",
    "    plt.scatter(boston_pd[feature_name], boston_pd['PRICE']);\n",
    "    plt.ylabel('Price', size=12);\n",
    "    plt.xlabel(feature_name, size=12);\n",
    "    \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The prices increase as the value of RM increases linearly. There are few outliers and the data seems to be capped at 50.\n",
    "- The prices tend to decrease with an increase in LSTAT. Though it doesn’t look to be following exactly a linear line.\n",
    "- We can use the **seaborn** `lmplot` function to clearly see the relationship between **RM** and **PRICE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'RM', y = 'PRICE', data = boston_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "- The interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles.\n",
    "- **IQR = Q3 - Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = boston_pd.quantile(0.25)\n",
    "Q3 = boston_pd.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have IQR scores below code will remove all the outliers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_outlier_IQR = boston_pd[~((boston_pd < (Q1 - 1.5 * IQR)) | \n",
    "                                 (boston_pd > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "boston_outlier_IQR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Understanding Relationships and New Insights Through Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "- Great tool for quickly assessing a probability distribution that is easy for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6));\n",
    "plt.hist(boston_pd['PRICE']);\n",
    "plt.title('Boston Housing Prices and Count Histogram');\n",
    "plt.xlabel('price ($1000s)');\n",
    "plt.ylabel('count');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "sns.distplot(boston_pd['PRICE'], fit=stats.norm);\n",
    "plt.figure(figsize=(8, 6));\n",
    "res = stats.probplot(boston_pd['PRICE'], plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that the values of PRICE is normally distributed with some of the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot (histograms and the estimated PDF) the univariate distribution of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  ax = plt.subplots(len(list(boston_pd.columns)), figsize=(12,46))\n",
    "\n",
    "for i, feature_name in enumerate(list(boston_pd.columns)):\n",
    "    if (feature_name != 'CHAS'):\n",
    "       sns.distplot(boston_pd[feature_name], hist=True, ax=ax[i]);\n",
    "       ax[i].set_ylabel('Count', fontsize=8);\n",
    "       ax[i].set_xlabel(\" {}\".format(feature_name), fontsize=8);\n",
    "       #ax[i].set_title(\"Freq dist \"+feature_name, fontsize=8);\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bivariate distribution plots help us to study the relationship between two variables by analyzing the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(boston_pd);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select a few features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "for i in range(0, len(boston_pd.columns), n):\n",
    "    sns.pairplot(data=boston_pd,\n",
    "                x_vars=boston_pd.columns[i:i+n],\n",
    "                y_vars=['PRICE']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6)); # define plot area\n",
    "ax = fig.gca(); # define axis \n",
    "sns.set_style(\"whitegrid\");\n",
    "sns.kdeplot(boston_pd[['RM', 'PRICE']], ax = ax, cmap=\"Blues_d\");\n",
    "ax.set_title('KDE plot of RM and PRICE'); # Give the plot a main title\n",
    "ax.set_xlabel('RM');  # Set text for the x axis\n",
    "ax.set_ylabel('PRICE'); # Set text for y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap: Two-Dimensional Graphical Representation\n",
    "- Represent the individual values that are contained in a matrix as colors.\n",
    "- Create a correlation matrix that measures the linear relationships between the variables.\n",
    "- The pairs which are highly correlated represent the same variance of the dataset thus we can further analyze them to understand which attribute among the pairs are most significant for building the model.\n",
    "- A number on the map indicates a strong inverse relationship, no relationship, and a strong direct relationship, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9));\n",
    "correlation_matrix = boston_pd.corr().round(2);\n",
    "sns.heatmap(correlation_matrix, cmap=\"YlGnBu\", annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to select only correlations that verify specific conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_matrix[(correlation_matrix >= 0.5) | (correlation_matrix <= -0.4)], \n",
    "            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **RM** has a strong positive correlation with **PRICE** (0.7) where as **LSTAThas** a high negative correlation with **PRICE** (-0.74).\n",
    "- The features **RAD**, **TAX** have a correlation of 0.91. These feature pairs are strongly correlated to each other. This can affect the model. Same goes for the features **DIS** and **AGE** which have a correlation of -0.75.\n",
    "- The predictor variables such as **CRIM**, **INDUS**, **NOX**, **Age**, **RAD**, **TAX**, **PTRATIO**, **LSTAT** have a negative correlation on the target. Increase of any of them leads to the decrease in the price of the housing.\n",
    "- The predictor variables such as **ZN**, **RM**, **DIS**, **B** have good positive correlation with the target. Increase in any of them leads to the increase in the price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also calculate the correlation between individual features and PRICE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "individual_features_df = []\n",
    "for i in range(0, len(boston_pd.columns) - 1): # -1 because the last column is PRICE\n",
    "    tmpDf = boston_pd[[boston_pd.columns[i], 'PRICE']]\n",
    "    #tmpDf = tmpDf[tmpDf[boston_pd.columns[i]] != 0] # remove values that are zero\n",
    "    individual_features_df.append(tmpDf)\n",
    "\n",
    "all_correlations = {feature.columns[0]: feature.corr()['PRICE'][0] for feature in individual_features_df}\n",
    "all_correlations = sorted(all_correlations.items(), key=operator.itemgetter(1))\n",
    "for (key, value) in all_correlations:\n",
    "    print(\"{:>15}: {:>15}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can select which feautures you might consider for your model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_features_list = [key for key, value in all_correlations if abs(value) >= 0.5]\n",
    "n = len(golden_features_list)\n",
    "print(\"There is {} strongly correlated values with PRICE:\".format(n))\n",
    "for var in golden_features_list:\n",
    "    print(\"\\t {}\".format(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Use Pandas Profiling</font>\n",
    "- Simple and fast EDA of a Pandas dataframe\n",
    "- Automatically perform operations such as:\n",
    "     + boxplot and histogram for a continous variable\n",
    "     + Measure missing values\n",
    "     + Calculate frequency if it's categorical variable\n",
    "- Generate an interactive HTML report containing statistical data on each feature.\n",
    "\n",
    "![fig_prof](https://skappal7.files.wordpress.com/2019/07/eda-cycle.jpg?w=748)\n",
    "Image Source: Sunil Kappal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "pandas_profiling.ProfileReport(boston_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Exercise</font>\n",
    "\n",
    "Visit the <a href=\"https://archive.ics.uci.edu/ml/datasets.php\">UCI Machine Learning Repository</a> or the <a href=\"http://www.ee.columbia.edu/~cylin/course/bigdata/getdatasetinfo.html\">Columbia University Big Data Analytics</a> to select a dataset you want to perform EDA on.\n",
    "\n",
    "An alternative is to Perform EDA on the automobile price dataset which description is available at the <a href=\"https://archive.ics.uci.edu/ml/datasets/automobile\">UCI Machine Learning Repository</a>.\n",
    "- You may want to follow the link: <a href=\"https://www.kaggle.com/toramky/eda-for-automobile-dataset\">kaggle.com</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/StephenElston/ExploringDataWithPython/master/PlottingWithPytonTools/Automobile%20price%20data.csv'\n",
    "import pandas as pd\n",
    "automobile = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
