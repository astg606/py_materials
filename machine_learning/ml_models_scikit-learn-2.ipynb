{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Machine Learning with Scikit-Learn</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "  <head> </head>\n",
    "  <body>\n",
    "<script src=\"https://bot.voiceatlas.mysmce.com/v1/chatlas.js\"></script>\n",
    "<app-chatlas\n",
    "\tatlas-id=\"f759a188-f8bb-46bb-9046-3b1b961bd6aa\"\n",
    "\twidget-background-color=\"#3f51b5ff\"\n",
    "\twidget-text-color=\"#ffffffff\"\n",
    "\twidget-title=\"Chatlas\">\n",
    "</app-chatlas>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "- <a href=\"https://medium.com/towards-artificial-intelligence/calculating-simple-linear-regression-and-linear-best-fit-an-in-depth-tutorial-with-math-and-python-804a0cb23660\">Calculating Simple Linear Regression and Linear Best Fit an In-depth Tutorial with Math and Python</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/tutorial/index.html\">scikit-learn Tutorials</a>\n",
    "- <a href=\"https://medium.com/@amitg0161/sklearn-linear-regression-tutorial-with-boston-house-dataset-cde74afd460a\">Sklearn Linear Regression Tutorial with Boston House Dataset</a>\n",
    "- <a href=\"https://www.dataquest.io/blog/sci-kit-learn-tutorial/\">Scikit-learn Tutorial: Machine Learning in Python</a>\n",
    "- <a href=\"https://debuggercafe.com/image-classification-with-mnist-dataset/\">Image Classification with MNIST Dataset</a>\n",
    "- <a href=\"https://davidburn.github.io/notebooks/mnist-numbers/MNIST%20Handwrititten%20numbers/\">MNIST handwritten number identification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Scikit-Learn</font>\n",
    "\n",
    "- Scikit-learn is a free machine learning library for Python. \n",
    "- Provides a selection of efficient tools for machine learning and statistical modeling including: \n",
    "     - **Classification:** Identifying which category an object belongs to. Example: Spam detection\n",
    "     - **Regression:** Predicting a continuous variable based on relevant independent variables. Example: Stock price predictions\n",
    "     - **Clustering:** Automatic grouping of similar objects into different clusters. Example: Customer segmentation \n",
    "     - **Dimensionality Reduction:** Seek to reduce the number of input variables in training data by preserving the salient relationships in the data\n",
    "- Features various algorithms like support vector machine, random forests, and k-neighbours.\n",
    "- Supports Python numerical and scientific libraries like NumPy and SciPy.\n",
    "\n",
    "\n",
    "Some popular groups of models provided by scikit-learn include:\n",
    "\n",
    "- **Clustering:** Group unlabeled data such as KMeans.\n",
    "- **Cross Validation:** Estimate the performance of supervised models on unseen data.\n",
    "- **Datasets:** for test datasets and for generating datasets with specific properties for investigating model behavior.\n",
    "- **Dimensionality Reduction:** Reduce the number of attributes in data for summarization, visualization and feature selection such as Principal component analysis.\n",
    "- **Ensemble Methods:** Combine the predictions of multiple supervised models.\n",
    "- **Feature Extraction:** Define attributes in image and text data.\n",
    "- **Feature Selection:** Identify meaningful attributes from which to create supervised models.\n",
    "- **Parameter Tuning:** Get the most out of supervised models.\n",
    "- **Manifold Learning:** Summarize and depicting complex multi-dimensional data.\n",
    "- **Supervised Models:** A vast array not limited to generalized linear models, discriminate analysis, naive bayes, lazy methods, neural networks, support vector machines and decision trees.\n",
    "- **Unsupervised Learning Algorithms:** − They include clustering, factor analysis, PCA (Principal Component Analysis), unsupervised neural networks.\n",
    "\n",
    "\n",
    "![fig_sckl](https://ulhpc-tutorials.readthedocs.io/en/latest/python/advanced/scikit-learn/images/scikit.png)\n",
    "Image Source: ulhpc-tutorials.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Requirements\n",
    "\n",
    "- Numpy\n",
    "- scipy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy version:        {np.__version__}\")\n",
    "print(f\"Pandas version:       {pd.__version__}\")\n",
    "print(f\"Seaborn version:      {sns.__version__}\")\n",
    "print(f\"Scikit-Learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Numerical Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Diabetes Dataset</font>\n",
    "- Contains information about the health data of diabetes patients.\n",
    "- There are 442 samples and 10 feature variables in this dataset. \n",
    "- Data have been modified (so that the only contain numerical values) and normalized (to have values between -0.2 and 0.2).\n",
    "\n",
    "We want to predict the diabetes progression (values between 25 and 346) one year after the baseline, given the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Keys: {diabetes_data.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {diabetes_data.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature Names: {diabetes_data.feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Information:\n",
    "| Acronym | Description |\n",
    "| --- | --- |\n",
    "| **age** | age in years |\n",
    "| **sex** | male or female |\n",
    "| **bmi** | body mass index |\n",
    "| **bp** | average blood pressure |\n",
    "| **s3** | hdl, high-density lipoproteins |\n",
    "| **s4** | tch, total cholesterol / HDL |\n",
    "| **s5** | ltg, possibly log of serum triglycerides level |\n",
    "| **s6** | glu, blood sugar level |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Extract Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pass the data into a Pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = pd.DataFrame(diabetes_data.data)\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relabel the columns using the Boston dataset feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.columns = diabetes_data.feature_names\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Diabetes `progression` data  to the Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the target data:  {diabetes_data.target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df['diabetes_progression']=diabetes_data.target\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the types of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Exploratory Data Analysis</font>\n",
    "\n",
    "- Important step before training the model. \n",
    "- We use statistical analysis and visualizations to understand the relationship of the target variable with other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Missing Values\n",
    "It is a good practice to see if there are any missing values in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of missing values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique elements in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain basic statistics on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6));\n",
    "plt.hist(diabetes_df['diabetes_progression']);\n",
    "plt.title('Diabetes Progression and Count Histogram');\n",
    "plt.xlabel('Diabetes Progression');\n",
    "plt.ylabel('count');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6));\n",
    "sns.distplot(diabetes_df['diabetes_progression']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap: Two-Dimensional Graphical Representation\n",
    "- Represent the individual values that are contained in a matrix as colors.\n",
    "- Create a correlation matrix that measures the linear relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9));\n",
    "correlation_matrix = diabetes_df.corr().round(2);\n",
    "sns.heatmap(correlation_matrix, cmap=\"YlGnBu\", annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_matrix[(correlation_matrix >= 0.5) | (correlation_matrix <= -0.4)], \n",
    "            cmap='YlGnBu', annot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **bmi** (0.59) and **s6** (0.57) have a moderate positive correlation with **diabetes_progression**.\n",
    "- The features **s1** and **s2** have a correlation of 0.9. These feature pairs are strongly correlated to each other. This can affect the model. Same goes for the features **s3** and **s4** which have a correlation of -0.74.\n",
    "- The predictor variable **s3** has a negative correlation on the target. Increase it leads to the decrease in the progression of the disease.\n",
    "- The predictor variables such as **bmi**, **bp**, **s4**, **s5** have good positive correlation with the target. Increase in any of them leads to the increase in the progression of the disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in diabetes_data.feature_names:\n",
    "    plt.figure(figsize=(5, 4));\n",
    "    plt.scatter(diabetes_df[feature_name], \n",
    "                diabetes_df['diabetes_progression']);\n",
    "    plt.ylabel('Diabetes Progression', size=12);\n",
    "    plt.xlabel(feature_name, size=12);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The progression of the disease appears to increase as the value of **bmi** increases. \n",
    "- The same can be said with **bp**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above observations we will plot an `lmplot` between **bmi** and **diabetes_progression** to see the relationship between the two more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'bmi', y = 'diabetes_progression', data = diabetes_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Model Selection Process</font>\n",
    "\n",
    "![fig_skl](https://miro.medium.com/max/1400/1*LixatBxkewppAhv1Mm5H2w.jpeg)\n",
    "Image Source: Christophe Bourguignat\n",
    "\n",
    "- A Machine Learning algorithm needs to be trained on a set of data to learn the relationships between different features and how these features affect the target variable. \n",
    "- We need to divide the entire data set into two sets:\n",
    "    + Training set on which we are going to train our algorithm to build a model. \n",
    "    + Testing set on which we will test our model to see how accurate its predictions are.\n",
    "    \n",
    "Before we create the two sets, we need to identify the algorithm we will use for our model.\n",
    "We can use the `machine_learning_map` map (shown at the top of this page) as a cheat sheet to shortlist the algorithms that we can try out to build our prediction model. Using the checklist let’s see under which category our current dataset falls into:\n",
    "- We have 442 samples: >50? (**Yes**)\n",
    "- Are we predicting a category? (**No**)\n",
    "- Are we predicting a quantity? (**Yes**)\n",
    "\n",
    "Based on the checklist that we prepared above and going by the `machine_learning_map` we can try out **regression methods** such as:\n",
    "\n",
    "- Linear Regression \n",
    "- Lasso\n",
    "- ElasticNet Regression\n",
    "- Ridge Regression: \n",
    "- K Neighbors Regressor\n",
    "- Decision Tree Regressor\n",
    "- Simple Vector Regression (SVR)\n",
    "- Ada Boost Regressor\n",
    "- Gradient Boosting Regressor\n",
    "- Random Forest Regression\n",
    "- Extra Trees Regressor\n",
    "\n",
    "Check the following documents on regresssion: \n",
    "<a href=\"https://scikit-learn.org/stable/supervised_learning.html\">Supervised learning--scikit-learn</a>,\n",
    "<a href=\"https://developer.ibm.com/technologies/data-science/tutorials/learn-regression-algorithms-using-python-and-scikit-learn/\">Learn regression algorithms using Python and scikit-learn</a>,\n",
    "<a href=\"https://www.pluralsight.com/guides/non-linear-\">Non-Linear Regression Trees with scikit-learn</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Simple Linear Model</font>\n",
    "- It is difficult to visualize the multiple features.\n",
    "- We want to predict the diabetes progression with just one variable and then move to the regression with all features.\n",
    "- Because **bmi** shows positive correlation with the **diabetes_progression**, we will use **bmi** for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bmi = diabetes_df.bmi\n",
    "y_dis = diabetes_df.diabetes_progression\n",
    "\n",
    "X_bmi = np.array(X_bmi).reshape(-1,1)\n",
    "y_dis = np.array(y_dis).reshape(-1,1)\n",
    "\n",
    "print(X_bmi.shape)\n",
    "print(y_dis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing sets\n",
    "- We split the data into training and testing sets. \n",
    "- We train the model with 80% of the samples and test with the remaining 20%. \n",
    "- We do this to assess the model’s performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = \\\n",
    "        train_test_split(X_bmi, y_dis, test_size = 0.2, random_state=5)\n",
    "\n",
    "print(X_train_1.shape)\n",
    "print(Y_train_1.shape)\n",
    "print(X_test_1.shape)\n",
    "print(Y_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing the model\n",
    "- We use scikit-learn’s LinearRegression to train our model on both the training and check it on the test sets.\n",
    "- We check the model performance on the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_1 = LinearRegression()\n",
    "reg_1.fit(X_train_1, Y_train_1)\n",
    "\n",
    "y_train_predict_1 = reg_1.predict(X_train_1)\n",
    "rmse = (np.sqrt(metrics.mean_squared_error(Y_train_1, y_train_predict_1)))\n",
    "r2 = round(reg_1.score(X_train_1, Y_train_1),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f'RMSE is {rmse}')\n",
    "print(f'R2 score is {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = reg_1.predict(X_test_1)\n",
    "rmse = (np.sqrt(metrics.mean_squared_error(Y_test_1, y_pred_1)))\n",
    "r2 = round(reg_1.score(X_test_1, Y_test_1),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination: 1 is perfect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Coefficient of determination: {metrics.r2_score(Y_test_1, y_pred_1) :.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45-Degree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5));\n",
    "plt.scatter(Y_test_1, y_pred_1);\n",
    "plt.plot([np.min(diabetes_data.target), np.max(diabetes_data.target)], \n",
    "         [np.min(diabetes_data.target), np.max(diabetes_data.target)], '--k');\n",
    "plt.axis('tight');\n",
    "plt.xlabel(\"Actual diabetes progression\");\n",
    "plt.ylabel(\"Predicted diabetes progression\");\n",
    "#plt.xticks(range(0, int(max(y_test)),2));\n",
    "#plt.yticks(range(0, int(max(y_test)),2));\n",
    "plt.title(\"Actual Progression vs Predicted Progression\");\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Linear Regression Model with All Variables</font>\n",
    "- We want to create a model considering all the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.drop('diabetes_progression', axis = 1)\n",
    "y = diabetes_df['diabetes_progression']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the `train_test_split` to split the data into random train and test subsets.\n",
    "- Everytime you run it without specifying `random_state`, you will get a different result.\n",
    "- If you use `random_state=some_number`, then you can guarantee the split will be always the same.\n",
    "- It doesn't matter what the value of `random_state` is:  42, 0, 21, ...\n",
    "- This is useful if you want reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_all = LinearRegression()\n",
    "reg_all.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = reg_all.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(metrics.mean_squared_error(y_train, y_train_predict)))\n",
    "r2 = round(reg_all.score(X_train, y_train),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f'RMSE is {rmse}')\n",
    "print(f'R2 score is {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_all.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "r2 = round(reg_all.score(X_test, y_test),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination: 1 is perfect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Coefficient of determination: {metrics.r2_score(y_test, y_pred) :.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test - y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45-Degree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5));\n",
    "plt.scatter(y_test, y_pred);\n",
    "plt.plot([np.min(diabetes_data.target), np.max(diabetes_data.target)], \n",
    "         [np.min(diabetes_data.target), np.max(diabetes_data.target)], '--k');\n",
    "plt.axis('tight');\n",
    "plt.xlabel(\"Actual progression\");\n",
    "plt.ylabel(\"Predicted progression\");\n",
    "#plt.xticks(range(0, int(max(y_test)),2));\n",
    "#plt.yticks(range(0, int(max(y_test)),2));\n",
    "plt.title(\"Actual progression vs Predicted progression\");\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMS: %r \" % np.sqrt(np.mean((y_test - y_pred) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df2 = df1.head(10)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Choosing the Best Model:</font> k-Fold Cross-Validation\n",
    "\n",
    "- Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "- It is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data.\n",
    "- We use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "- The biggest advantage of this method is that every data point is used for validation exactly once and for training `k-1` times.\n",
    "- To choose the final model to use, we select the one that has the lowest validation error.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into `k` groups\n",
    "3. For each unique group:\n",
    "       3.1 Take the group as a hold out or test data set\n",
    "       3.2 Take the remaining k-1 groups as a training data set\n",
    "       3.3 Fit a model on the training set and evaluate it on the test set\n",
    "       3.4 Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "How to choose **k**?\n",
    "- A poorly chosen value for **k** may result in a mis-representative idea of the skill of the model, such as a score with a high variance, or a high bias.\n",
    "- The choice of **k** is usually 5 or 10, but there is no formal rule. As **k** gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller.\n",
    "- A value of **k=10** is very common in the field of applied machine learning, and is recommend if you are struggling to choose a value for your dataset.\n",
    "\n",
    "Below is the visualization of a k-fold validation when k=5.\n",
    "![FIG_kFold](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "Image Source: https://scikit-learn.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# user variables to tune\n",
    "seed    = 9\n",
    "folds   = 10\n",
    "metric  = \"neg_mean_squared_error\"\n",
    "\n",
    "# hold different regression models in a single dictionary\n",
    "\n",
    "models = dict()\n",
    "models[\"SVR\"] = SVR()\n",
    "models[\"KNN\"] = KNeighborsRegressor()\n",
    "models[\"Lasso\"] = Lasso()\n",
    "models[\"Ridge\"] = Ridge()\n",
    "models[\"LassoCV\"] = LassoCV()\n",
    "models[\"Linear\"] = LinearRegression()\n",
    "models[\"AdaBoost\"] = AdaBoostRegressor()\n",
    "models[\"ElasticNet\"] = ElasticNet()\n",
    "models[\"DecisionTree\"] = DecisionTreeRegressor()\n",
    "models[\"RandomForest\"] = RandomForestRegressor()\n",
    "models[\"BayesianRidge\"] = BayesianRidge()\n",
    "models[\"GradientBoost\"] = GradientBoostingRegressor()\n",
    "models[\"LogisticRegression\"] = LogisticRegression()\n",
    "\n",
    "# 10-fold cross validation for each model\n",
    "model_results = list()\n",
    "model_names   = list()\n",
    "for model_name in models:\n",
    "    model   = models[model_name]\n",
    "    k_fold  = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    #results = cross_val_score(model, X_train, y_train, \n",
    "    results = cross_val_score(model, X, y, \n",
    "                              cv=k_fold, scoring=metric)\n",
    "    \n",
    "    model_results.append(results)\n",
    "    model_names.append(model_name)\n",
    "    print(f\"{model_name :>20}: {round(results.mean(), 3):8.2f} {round(results.std(), 3):8.2f}\")\n",
    "\n",
    "# box-whisker plot to compare regression models\n",
    "figure = plt.figure();\n",
    "figure.suptitle('Regression models comparison');\n",
    "ax = figure.add_subplot(111);\n",
    "plt.boxplot(model_results);\n",
    "ax.set_xticklabels(model_names, rotation = 45, ha=\"right\");\n",
    "ax.set_ylabel(\"Mean Squared Error (MSE)\");\n",
    "plt.margins(0.05, 0.1);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the above comparison, we can see that `Bayesian Ridge Regression` model appears to show the best performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Model with Bayesian Ridge</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brg = BayesianRidge()\n",
    "brg.fit(X_train, y_train)\n",
    "\n",
    "brg_predicted = brg.predict(X_test)\n",
    "brg_expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Root Mean Square Error:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMS: %r \" % np.sqrt(np.mean((brg_predicted - brg_expected) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The coefficient of determination**: (1 is perfect prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coeff of determination: {:.4f}'.format(metrics.r2_score(brg_expected, brg_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(brg_expected - brg_predicted);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45-Degree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5));\n",
    "plt.scatter(brg_expected, brg_predicted)\n",
    "plt.plot([np.min(diabetes_data.target), np.max(diabetes_data.target)], \n",
    "         [np.min(diabetes_data.target), np.max(diabetes_data.target)], '--k');\n",
    "plt.axis('tight');\n",
    "plt.xlabel('True diabetes progression');\n",
    "plt.ylabel('Predicted diabetes progression');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zoom in:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Actual': brg_expected, 'Predicted': brg_predicted})\n",
    "df2 = df1.head(10)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "- Once we have a trained model, we can understand feature importance (or variable importance) of the dataset which tells us how important each feature is, to predict the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = brg.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos        = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center');\n",
    "plt.yticks(pos, diabetes_data.feature_names[sorted_idx]);\n",
    "plt.xlabel('Relative Importance');\n",
    "plt.title('Variable Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot training deviance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(brg.staged_predict(X_test)):\n",
    "    test_score[i] = brg.loss_(gbr_expected, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6));\n",
    "plt.subplot(1, 1, 1);\n",
    "plt.title('Deviance');\n",
    "plt.plot(np.arange(n_estimators) + 1, \n",
    "         brg.train_score_, 'b-',\n",
    "         label='Training Set Deviance');\n",
    "plt.plot(np.arange(n_estimators) + 1, \n",
    "         test_score, 'r-',\n",
    "         label='Test Set Deviance');\n",
    "plt.legend(loc='upper right');\n",
    "plt.xlabel('Boosting Iterations');\n",
    "plt.ylabel('Deviance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Image Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> MNIST Dataset</font>\n",
    "\n",
    "- The <A HREF=\"https://en.wikipedia.org/wiki/MNIST_database\"> MNIST database</A> (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "- The database is also widely used for training and testing in the field of machine learning.\n",
    "- The dataset we will be using contains 70000 images of handwritten digits among which 10000 are reserved for testing.\n",
    "- This dataset is  suitable for anyone who wants to get started with image classification using Scikit-Learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist_data = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys: \", mnist_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the `data` and `target` already separated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of Data: {mnist_data.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datatype of Data: {type(mnist_data.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the Target Data: {mnist_data.target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datatype of Target Data: {type(mnist_data.target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature Names: {mnist_data.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Url: {mnist_data.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data, np_target = mnist_data['data'], mnist_data['target']\n",
    "\n",
    "print(f' Shape of data:   {np_data.shape} \\n Shape of target: {np_target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(np_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(np_data.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datatype of the target values: {np_target.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print few values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_target[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the labels from string to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target = np_target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_target[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of unique labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target.value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "There are 70000 numbers, each stored as an array of 784 numbers depicting the opacity of each pixel, it can be displayed by reshaping the data into a 28x28 array and plotting using matplotlib. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 15657\n",
    "some_digit = np_data.values[some_index]\n",
    "some_digit_image = some_digit.reshape(28,28)\n",
    "\n",
    "plt.imshow(some_digit_image, \n",
    "           cmap = matplotlib.cm.binary, \n",
    "           interpolation='nearest')\n",
    "plt.axis=('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the target for row `some_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_target[some_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display few images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def display_digits(X, y):\n",
    "    \"\"\"\n",
    "      Given an array of images of digits X and \n",
    "      the corresponding values of the digit y,\n",
    "      this function plots 96 unique randomly selected images \n",
    "      and their values.\n",
    "    \"\"\"\n",
    "    # Figure size (width, height) in inches\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Adjust the subplots \n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, \n",
    "                        hspace=0.05, wspace=0.05)\n",
    "\n",
    "    num_images = X.shape[0]\n",
    "    \n",
    "    num_selected_images = 96\n",
    "    row_indices = random.sample(range(num_images), num_selected_images)\n",
    "    \n",
    "    i = 0\n",
    "    for idx in row_indices:\n",
    "        # Initialize the subplots: \n",
    "        # Add a subplot in the grid of 8 by 12, at the i+1-th position\n",
    "        ax = fig.add_subplot(8, 12, i + 1, xticks=[], yticks=[])\n",
    "        \n",
    "        # Display an image at the i-th position\n",
    "        ax.imshow(X[idx].reshape(28, 28), cmap=plt.cm.binary, \n",
    "                  interpolation='nearest')\n",
    "       \n",
    "        # label the image with the target value\n",
    "        ax.text(0, 7, str(y[idx]))\n",
    "        i += 1\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(np_data.values, np_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Separating the Training and Testing Set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first 60000 (among the 70000) images are used for training.\n",
    "- The remaining 10000 images are used for validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 60000\n",
    "\n",
    "X_train = np_data.values[:num_train]\n",
    "X_test  = np_data.values[num_train:]\n",
    "y_train = np_target[:num_train]\n",
    "y_test  = np_target[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Train Data:  {X_train.shape} \\n Test Data:   {X_test.shape} \\n \\\n",
    "Train label: {y_train.shape} \\n Test Label:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle the training set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = X_train.shape[0]\n",
    "shuffle_index = np.random.permutation(nn)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=\"red\">Training a Binary Classifier</font>\n",
    "\n",
    "- Binary classification means there are two classes to work with that relate to one another as `true` and `false`.\n",
    "- Here, we want to identify a single digit: looking at `9`s.\n",
    "- The classification will tell us if we have a `9` (true) or not (false)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the target arrays as boolean arrays:** true if 9 otherwise false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_9 = (y_train == 9)\n",
    "y_test_9 = (y_test == 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and train the model:**\n",
    "\n",
    "- We use the SGDClassifier that applies regularized linear model with SGD (Stochastic Gradient Descent) learning to build an estimator.\n",
    "- The method helps building an estimator for classification and regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf =SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make an initial prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 15657\n",
    "some_digit = np_data.values[some_index]\n",
    "print(np_target[some_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_predict = sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measuring accuracy using cross validation**\n",
    "\n",
    "- The `stratifiedKfold` class performs stratified sampling to produce folds that contain a representative ratio of each class. \n",
    "- At each iteration the code creates a clone of the classifier, trains that clone on the training fold and then makes predictions on the test fold. \n",
    "- It then counts the number of correct predictions and outputs the ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_9):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_9[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_9[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf, X_train, y_train_9, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn cross_val_score in action returning the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (sum(np_target==9)/len(np_target))*100\n",
    "print(f'94-96% accuracy might not as impressive as it sounds \\n where there are {accuracy :.2f}% of 9s in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "- A confusion matrix is a tabular summary of the number of correct and incorrect predictions made by a classifier. \n",
    "- It can be used to evaluate the performance of a classification model through the calculation of performance metrics like accuracy, precision, recall, and F1-score.\n",
    "- The confusion matrix is a much better way to evaluate the performance of a classifier, especially when there is a skewed dataset as we have here with only 10% of the dataset being the target.\n",
    "- Each row represents a class, each column a prediction:\n",
    "   * The first row is negative cases (non-9s) with the top left containing all the correctly classified non-9s (True Negatives), the top right the 9s incorrectly classified as non-9s (False-Positves).\n",
    "   * The second row represents the positive class, 9s in this case, bottom left contains the 9s incorrectly classified as non-9s (False Negatives), the bottom right containing the correctly classified 9s (True Positives)\n",
    "   \n",
    "| | Actual | |\n",
    "| --- |: --- |: --- |\n",
    "| **Prediction** | True Positive | False Positive |\n",
    "| | False Negative | True Negative |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need a set of predictions to compare to the actual targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_9, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = metrics.confusion_matrix(y_train_9, y_train_pred)\n",
    "\n",
    "print(f\"Confusion Matrix: \\n{cf_matrix}\")\n",
    "print(f\"\\n Number of images: {np.sum(cf_matrix)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_train_9, y_train_pred, \n",
    "                              labels=sgd_clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=sgd_clf.classes_)\n",
    "disp.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision/Recall**\n",
    "\n",
    "- Precision measures the number of true positives (correctly classified 9s) as a ratio of the total samples classified as a 9: $\\frac{TP}{TP + FP}$\n",
    "- Recall measueres the number of true positives as a ratio of the total number of positives: $\\frac{TP}{TP + FN}$\n",
    "- Depending on the scenario the model may be modified to try and maximise one or the other, catching all positive instances at the expense of catching some false positives. Or making sure a positive instance is never falsely identified as a negative at the expense of missing some of the positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_9, cv=3, method='decision_function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_9, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('Precision and recall vs decision threshold')\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Training and Prediction on the Entire Dataset</font>\n",
    "\n",
    "- We will use the Stochastic Gradient Descent classifier (SGD). \n",
    "- Scikit-Learn’s SGDClassifier is a good starting point for linear classifiers. \n",
    "- Using the loss parameter we will see how Support Vector Machine (Linear SVM) and Logistic Regression perform for the same dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Linear Support Vector Machine (SVM)\n",
    "- We use linear SVM with stochastic gradient descent (SGD) learning.\n",
    "- The gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule.\n",
    "- To use the Linear SVM Classifier, we need to set the loss parameter to `hinge`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    " \n",
    "sgd_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before testing the model, it is a good practice to first see the cross-validation scores on the training data. \n",
    "- That you will give you a very good projection of how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For three-fold Cross-Validation you are getting around 87% – 88% accuracy. \n",
    "* Not too bad, not too good either.\n",
    "\n",
    "We can now compute the actual test scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreSVM = sgd_clf.score(X_test, y_test)\n",
    "print(\"Test score of the Linear SVM: \", scoreSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = sgd_clf.predict(X_test)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_predict, \n",
    "                              labels=sgd_clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=sgd_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(loss='log', random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreLR = sgd_clf.score(X_test, y_test)\n",
    "print(\"Test score of the Logistic Regression: \", scoreLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = sgd_clf.predict(X_test)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_predict, \n",
    "                              labels=sgd_clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=sgd_clf.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "- Random forests is a supervised learning algorithm. \n",
    "- A forest is comprised of trees. \n",
    "- It is said that the more trees it has, the more robust a forest is. \n",
    "- Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. \n",
    "- It also provides a pretty good indicator of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 500)\n",
    "forest = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_output = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest with n_estimators:500\")\n",
    "print(accuracy_score(y_test, forest_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, forest_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, forest_output, \n",
    "                              labels=forest.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=forest.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "                                 max_depth=1, random_state=0).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_output = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, gradient_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, gradient_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, gradient_output, \n",
    "                              labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier\n",
    "\n",
    "- The Multi-layer Perceptron classifier relies on an underlying Neural Network to perform the task of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(10,), random_state=1)\n",
    "clf.fit(X_train, y_train)   \n",
    "neural_output = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sgd: \", accuracy_score(y_test, neural_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, neural_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,), random_state=1)\n",
    "clf.fit(X_train, y_train)   \n",
    "neural_output = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy on the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lbfgs: \", accuracy_score(y_test, neural_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few true images against predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(X_test, neural_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, neural_output, \n",
    "                              labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=clf.classes_)\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
