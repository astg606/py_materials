{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Machine Learning Modeling with Tensorflow</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Reference\n",
    "\n",
    "- <a href=\"https://www.mygreatlearning.com/blog/what-is-tensorflow-machine-learning-library-explained/\">What is TensorFlow? The Machine Learning Library Explained</a>\n",
    "- <a href=\"https://www.tensorflow.org/tutorials/keras/regression\">Basic regression: Predict fuel efficiency</a>\n",
    "- <a href=\"https://stackabuse.com/tensorflow-2-0-solving-classification-and-regression-problems/\">Tensorflow 2.0: Solving Classification and Regression Problems</a>\n",
    "- <a href=\"https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial\">Getting Started with TensorFlow: A Machine Learning Tutorial</a>\n",
    "- <a href=\"https://sebastianraschka.com/faq/docs/tensorflow-vs-scikitlearn.html\">What is the main difference between TensorFlow and scikit-learn?</a>\n",
    "- <a href=\"https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\">Python TensorFlow Tutorial – Build a Neural Network</a>\n",
    "- <a href=\"https://steadforce.com/en/first-steps-tensorflow-part-3/\">A simple neural network with TensorFlow</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Overview of Neural Networks</font>\n",
    "\n",
    "- Neural networks are developed to mimic the neural connection in a brain. \n",
    "- A neural network consists of a number of layers, and each layer consists of a number of units (or neurons). \n",
    "- The task of every neuron is to process the information received and then transmit it to the neurons in the next layer.\n",
    "\n",
    "\n",
    "**How do we choose the number of hidden layers and hidden neurons?**\n",
    "\n",
    "There is not any precise rule, in general the number of hidden layers depends strongly on the problem, at odds to the input and output layers. In particular:\n",
    "\n",
    "- **Input layer:** The input layer is where the network starts. The number of units in this layer is fixed and corresponds exactly to the number of input features.\n",
    "- **Hidden layers:** The number of hidden layer and units per layer are the free parameters that one has to fix. There is no rule to decide these two parameters but it depends strongly on the problem.\n",
    "- **Output layer:** The output layer is where the network ends and the predictions are given. In the case of a classification problem, the number of units in the output layer corresponds exactly to the number of classes.\n",
    "\n",
    "These layers form the base for all new networks regardless of complexity.\n",
    "Once the architecture of a neural network has been set, we can proceed to the training of the neural network.\n",
    "\n",
    "- The input features are fed to the input layer. \n",
    "- The network goes through all the hidden layer(s) until the output layer, where the predictions are produced.\n",
    "- Ecah layer has an activation function which activates a neuron and can have several expressions, but in most the cases it is either a Rectified Linear Unit (ReLU) or a logistic function.\n",
    "- Between two adjacent layers there is always a weight matrix which is responsible to transmit the information. For an N-layer neural network, we have N-1 weight matrices and the j-th matrix, i.e. the matrix of the j-th layer, will be a function of all the j-1 matrices in the previous layers.\n",
    "- The output layer delivers a prediction, which must be compared to the real values. This comparison is an estimate of the error. We want to minimize the error, hence optimize the parameters of the problems, i.e. the weight matrices. One of the best known optimizing algorithms is the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">What is TensorFlow?</font>\n",
    "- Tensorflow is an open-source library for numerical computation and large-scale machine learning that ease `Google Brain TensorFlow`, the process of acquiring data, training models, serving predictions, and refining future results.\n",
    "- Tensorflow bundles together Machine Learning and Deep Learning models and algorithms.\n",
    "- Tensorflow allows developers to create a graph of computations to perform. Nodes in the graph represent mathematical operations and connections (edges) represent data which usually are multidimensional data arrays or tensors, that are communicated between these edges.\n",
    "- The name `TensorFlow` is derived from the operations which neural networks perform on multidimensional data arrays or tensors! It’s literally a flow of tensors.\n",
    "\n",
    "\n",
    "**First Example of TensorFlow Graph**\n",
    "\n",
    "Consider the expression:\n",
    "<center>\n",
    "    a = (b + c) * (c + 2)\n",
    "</center>\n",
    "We can break this down into:\n",
    "<center>\n",
    "    d = b + c\n",
    "    \n",
    "    e = c + 2\n",
    "    \n",
    "    a = d * e\n",
    "</center>\n",
    "Now we can represent these operations graphically as:\n",
    "\n",
    "![fig_gr1](https://i1.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example.png)\n",
    "Image Source: adventuresinmachinelearning.com\n",
    "\n",
    "Note that the operations `d = b + c` and `e = c + 2` can be performed in parallel: potential of distributing such calcultions across CPUs and GPUs. \n",
    "\n",
    "**Second Example of TensorFlow Graph**\n",
    "\n",
    "The graph below shows the computational graph of a three-layer neural network.\n",
    "The animated data flows between different nodes in the graph are tensors which are multi-dimensional data arrays. \n",
    "\n",
    "![fig_gr2](https://i1.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2017/03/TensorFlow-data-flow-graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Main Steps of a ML Program</font>\n",
    "    \n",
    "![FIG_AXES](https://www.altudo.co/-/media/altudo/images/resources/blogs/5-steps-to-define-ml-flow-to-deliver-custom-user-experience/2.ashx?la=en&hash=0A8E8BEC05A4C64C37908FB87757285E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">First Problem:</font> Regression Analysis\n",
    "\n",
    "## <font color=\"blue\">Problem Statement</font>\n",
    "\n",
    "We consider the function: <br>\n",
    "$$\n",
    "f(x,y) = (1-(x^2 + y^3))e^{-\\frac{1}{2}(x^2 + y^2)}\n",
    "$$\n",
    "<br>\n",
    "defined in the domain $D=[-3,3] \\times [-3,3]$.\n",
    "<OL>\n",
    "<LI> We randomnly select $n$ points in the domain $D$ and compute the function on those points to create a (training) dataset containing $n$ pairs points/values.\n",
    "<LI> We use the dataset for training a ML algorithm.\n",
    "<LI> We generate a uniform set of points (testing set) in $D$ to test the algorithm.\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Generating the Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,y):\n",
    "    return (1-(x**2+y**3))*np.exp(-(x**2+y**2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 2\n",
    "nx = 30\n",
    "ny = 30\n",
    "num_points = nx * ny\n",
    "\n",
    "# Boundary of the domain\n",
    "a_min = -3.0\n",
    "a_max = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Generate dataset for training</font>\n",
    "- The grid points are randomly generated over the domain\n",
    "- The arrays are 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.zeros(num_points)  # 1D targets for training\n",
    "xt = np.zeros((num_points, num_dims))  # grid points for training\n",
    "\n",
    "x = np.random.uniform(a_min, a_max, nx) # Feature vectors\n",
    "y = np.random.uniform(a_min, a_max, ny) # Labels\n",
    "\n",
    "k = 0\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        xt[k, 0] = x[i]\n",
    "        xt[k, 1] = y[j]\n",
    "        yt[k] = ff(x[i], y[j])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Add noise in the training targets</font>\n",
    "\n",
    "Gaussian normal distribution with `noise_mean` as mean and `noise_std` as standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_mean = 0.0\n",
    "noise_std  = 1.0e-2\n",
    "noise = np.random.normal(noise_mean, noise_std, num_points)\n",
    "yt = yt + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Generate dataset for validation</font>\n",
    "- The grid points are uniformly distributed over the domain\n",
    "- The arrays are 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yv = np.zeros(num_points)  # 1D targets for validation\n",
    "xv = np.zeros((num_points, num_dims))  # grid points for validation\n",
    "\n",
    "x = np.linspace(-3.0, 3.0, nx)\n",
    "y = np.linspace(-3.0, 3.0, ny)\n",
    "\n",
    "k = 0\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        xv[k,0] = x[i]\n",
    "        xv[k,1] = y[j]\n",
    "        yv[k] = ff(x[i],y[j])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Data Gathering and Basic Analyses</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\"x0\": xt[:,0], \"x1\": xt[:,1], \n",
    "                           \"TargetValues\": yt[:]})\n",
    "print(train_data.head(5))                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data  = pd.DataFrame({\"x0\": xv[:,0], \"x1\": xv[:,1], \n",
    "                            \"TargetValues\": yv[:]})\n",
    "print(valid_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d');\n",
    "threedee.scatter(train_data['x0'], train_data['x1'], \n",
    "                 train_data['TargetValues']);\n",
    "threedee.set_xlabel('x');\n",
    "threedee.set_ylabel('y');\n",
    "threedee.set_zlabel('f(x,y)');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the joint distribution of the columns from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data.drop(columns=[\"TargetValues\"]), diag_kind=\"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do something similar for the data used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(valid_data.drop(columns=[\"TargetValues\"]), diag_kind=\"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the overall statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_data.describe()\n",
    "train_stats.pop(\"TargetValues\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split features from labels\n",
    "- Separate the target value, or `label`, from the features.\n",
    "- This `label` is the value that you will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data.pop('TargetValues')\n",
    "valid_labels = valid_data.pop('TargetValues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Normailized the Data</font>\n",
    "\n",
    "- It is good practice to normalize features that use different scales and ranges. \n",
    "- Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x):\n",
    "    \"Function to normalize the data\"\n",
    "    return (x - train_stats['mean']) / train_stats['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the data that will be used to train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_train_data = normalize_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also need to normalize the validation dataset by projecting it into the same distribution that the model has been trained on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_valid_data = normalize_data(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**The same normalization will have to be applied to any other data used in this model.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Build the Model</font>\n",
    "\n",
    "#### Instantiate a sequential model using `keras`\n",
    "- `keras` is TensorFlow's high-level API for building and training deep learning models. It's used for fast prototyping, state-of-the-art research, and production.\n",
    "- <font color=\"red\">The sequential model is the simplest model to use, especially when getting started.</font>\n",
    "- It involves defining a Sequential class and adding layers to the model one by one in a linear manner, from input to output.\n",
    "- The model needs to know what input shape (`input_shape`) it should expect. The first layer of the `Sequential` model needs to receive the information.\n",
    "\n",
    "In the model below:\n",
    "\n",
    "- The model expects rows of data with `num_shape` variables (the `input_shape=num_shape` argument)\n",
    "- The first hidden layer has 64 nodes and uses the `relu` activation function.\n",
    "- The second hidden layer has 64 nodes and uses the `relu` activation function.\n",
    "- The output layer has one node and uses no activation function.\n",
    "\n",
    "The rectified linear activation function (`relu`) is a piecewise linear function that will output the input directly if is positive, otherwise, it will output zero. \n",
    "- Because rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well.\n",
    "- It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shape = len(train_data.keys())\n",
    "num_nodes = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "             layers.Dense(num_nodes, activation=tf.nn.relu, \n",
    "                          input_shape=[num_shape]),\n",
    "             layers.Dense(num_nodes, activation=tf.nn.relu),\n",
    "             layers.Dense(1) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model creation can also be written as:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(num_nodes, activation=tf.nn.relu, input_shape=[num_shape]))\n",
    "model.add(layers.Dense(num_nodes, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(1))\n",
    "```\n",
    "\n",
    "Dense layers represent a function that maps the input tensor `x` to an output tensor `y` via the equation `y = Ax + b` where `A` (the kernel) and `b` (the bias) are parameters of the dense layer.\n",
    "\n",
    "![nn](tensorflow_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "- Once you have specified the architecture of the network, you need to specify the method for back-propagation by choosing an optimizer and specify the loss.\n",
    "- Compiling the model uses the efficient numerical libraries (Theano or TensorFlow) in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required to provide a loss function and an optimizer: \n",
    "- We are asking the network to use the `rmsprop` optimizer to change weights in such a way that the loss `mse` (mean squared error) is minimized at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse',\n",
    "              optimizer = optimizer,\n",
    "              metrics = ['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the model\n",
    "\n",
    "`model.summary()` is a useful method if you want to get an overview of your model and see the total number of parameters.\n",
    "It prints:\n",
    "\n",
    "- Name and type of all layers in the model.\n",
    "- Output shape for each layer.\n",
    "- Number of weight parameters of each layer.\n",
    "-  If the model has general topology, the inputs each layer receives\n",
    "- The total number of trainable and non-trainable parameters of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "trainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "non_trainable_count = np.sum([K.count_params(w) for w in model.non_trainable_weights])\n",
    "\n",
    "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Let](https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889):\n",
    "\n",
    "- **i**: input size (2 in this case)\n",
    "- **h**: size of hidden layers (64, 64 here)\n",
    "- **o**: output size (1 in this case)\n",
    "\n",
    "We have:\n",
    "\n",
    "    num_params = connections between layers + biases in every layer\n",
    "               = (i×h + h×o) + (h+o)\n",
    "               = (2x16 + 16x16 + 16*1) + (16 + 16 + 1)\n",
    "               = (2x16 + 16) + (16x16 + 16) + (16x1 + 1)\n",
    "               = 48 + 272 + 17\n",
    "               = 337\n",
    "               \n",
    "   input = **Input**((None, 2))\n",
    "   <br>\n",
    "   dense = **Dense**(16)(input)\n",
    "      <br>\n",
    "   dense = **Dense**(16)(dense)\n",
    "    <br>\n",
    "  output = **Dense**(1)(dense)\n",
    "   <br>\n",
    "   model = Model(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the model\n",
    "\n",
    "10 samples from the training data and call `model.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "print(example_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be working, and it produces a result of the expected shape and type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Train the Model</font>\n",
    "\n",
    "Training occurs over epochs and each epoch is split into batches.\n",
    "\n",
    "- **Epoch**: One pass through all of the rows in the training dataset.\n",
    "- **Batch**: One or more samples considered by the model within an epoch before weights are updated.\n",
    "- One epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs. \n",
    "- The model is \"fit\" to the training data using the `fit` method. We also specify the `batch_size` and the maximum number of `epochs` we want training to go on.\n",
    "- The callback function is applied at given stages of the training procedure. We use it to get a view on internal states and statistics of the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 1000 epochs, and record the training and validation accuracy in the history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a \n",
    "# single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs):\n",
    "          if epoch % 100 == 0: \n",
    "             print('')\n",
    "          print('.', end='')\n",
    "\n",
    "# How many times we go through the entire dataset\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(normed_train_data, train_labels,    \n",
    "                    epochs=EPOCHS, verbose=1, \n",
    "                    callbacks=[PrintDot()])\n",
    "#epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the model's training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the stats stored in the history object.\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Target]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "             label='Train Error')\n",
    "#    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "#             label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([min(hist['mean_absolute_error']) ,max(hist['mean_absolute_error'])])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$Target^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "             label='Train Error')\n",
    "#    plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "#             label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,max(hist['mean_squared_error'])])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Evaluate the Model on Test Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(normed_valid_data, valid_labels, verbose=1)\n",
    "#print(\"Testing set Mean Abs Error: {} \".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = model.predict(normed_valid_data).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the 45-degree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(valid_labels, valid_predictions);\n",
    "plt.xlabel('True Values');\n",
    "plt.ylabel('Predictions');\n",
    "plt.axis('equal');\n",
    "plt.axis('square');\n",
    "plt.xlim([0,plt.xlim()[1]]);\n",
    "plt.ylim([0,plt.ylim()[1]]);\n",
    "_ = plt.plot([-100, 100], [-100, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(valid_predictions - valid_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Function Using Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d');\n",
    "threedee.scatter(valid_data['x0'], valid_data['x1'], valid_predictions);\n",
    "threedee.set_xlabel('x');\n",
    "threedee.set_ylabel('y');\n",
    "threedee.set_zlabel('f(x,y)');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Exercise</font>\n",
    "\n",
    "Consider the 2D problem presented here.\n",
    "- Create a dataset of 1000 randomly selected points (in the domain) and their associated targets.\n",
    "- Randomly choose 80% of the data for training and the remaining for testing\n",
    "- Create your ML model and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Second Problem:</font> Image Classification\n",
    "\n",
    "We use the [MNIST data set](http://yann.lecun.com/exdb/mnist/) (Modified National Institute of Standards and Technology database).\n",
    "\n",
    "* Is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "* The database is also widely used for training and testing in the field of machine learning.\n",
    "* The dataset we will be using contains 70000 images of handwritten digits among which 10000 are reserved for testing.\n",
    "* It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/hassanamin/tensorflow-mnist-gpu-tutorial\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Devices including GPU's with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Load MNiST Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape train inputs:  \", x_train.shape)\n",
    "print(\"Shape train outputs: \", y_train.shape)\n",
    "print(\"Shape test  inputs:  \", x_test.shape)\n",
    "print(\"Shape test  outputs: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type train inputs:  \", x_train.dtype)\n",
    "print(\"Type train outputs: \", y_train.dtype)\n",
    "print(\"Type test  inputs:  \", x_test.dtype)\n",
    "print(\"Type test  outputs: \", y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Preprocess the Training and Test Datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the type from integer to floating point. This will reduce our memory requirements by forcing the precision of the pixel values to be 32 bit, the default precision used by Keras anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training and test datasets are structured as a 3-dimensional array of instance, image width and image height. \n",
    "- For a multi-layer perceptron model we must reduce the images down into a vector of pixels. In this case the 28×28 sized images will be 784 pixel input values.\n",
    "- We can do this transform easily using the `reshape()` function on the NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert class vectors to binary class matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Create Sequential Model Using Tensorflow Keras</font>\n",
    "\n",
    "Architecture of the Network is:\n",
    "\n",
    "1. Input layer for 28x28=784 images in MNiST dataset\n",
    "2. Dense layer with 128 neurons and ReLU activation function\n",
    "2. Dense layer with 128 neurons and ReLU activation function\n",
    "3. Output layer with 10 neurons for classification of input images as one of ten digits(0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = tf.keras.models.Sequential()\n",
    "mnist_model.add(tf.keras.layers.Dense(128, activation='relu', \n",
    "                                input_shape=(784,)))\n",
    "#mnist_model.add(tf.keras.layers.Dropout(0.2))\n",
    "#mnist_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "mnist_model.add(tf.keras.layers.Dropout(0.2))\n",
    "mnist_model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model Designed Earlier\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    "- Loss function This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "- Optimizer This is how the model is updated based on the data it sees and its loss function.\n",
    "- Metrics Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Training and Validation</font>\n",
    "\n",
    "The `mnist_model.fit` method adjusts the model parameters to minimize the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mnist_model.fit(x_train, y_train,\n",
    "                          batch_size = batch_size,\n",
    "                          epochs = num_epochs,\n",
    "                          verbose = 1,\n",
    "                          validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Plot the Deceasing Loss over Epochs</font>\n",
    "\n",
    "Use Pandas to plot a graph showing the decrease in mean squared error (mse) as training improves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(history.history)\n",
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Evaluate the Model</font>\n",
    "\n",
    "The `mnist_model.evaluate` method checks the models performance, usually on a \"Validation-set\" or \"Test-set\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mnist_model.evaluate(x_test,  y_test, verbose=0)\n",
    "print('Test loss:     {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Visualize Predictions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = mnist_model.predict(x_test, steps=1)\n",
    "predicted_labels = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_digits(X, y):\n",
    "    \"\"\"\n",
    "      Given an array of images of digits X and \n",
    "      the corresponding values of the digit y,\n",
    "      this function plots the first 96 images and their values.\n",
    "    \"\"\"\n",
    "    # Figure size (width, height) in inches\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Adjust the subplots \n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(96):\n",
    "        # Initialize the subplots: \n",
    "        #    Add a subplot in the grid of 8 by 12, at the i+1-th position\n",
    "        ax = fig.add_subplot(8, 12, i + 1, xticks=[], yticks=[])\n",
    "        \n",
    "        # Display an image at the i-th position\n",
    "        ax.imshow(X[i].reshape(28, 28), cmap=plt.cm.binary, interpolation='nearest')\n",
    "       \n",
    "        # label the image with the target value\n",
    "        ax.text(0, 7, str(y[i]))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits(x_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Save the Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.save('my_MNIST_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to reload the model later, we can use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('my_MNIST_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "# https://liufuyang.github.io/2017/04/01/just-another-tensorflow-beginner-guide-3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
