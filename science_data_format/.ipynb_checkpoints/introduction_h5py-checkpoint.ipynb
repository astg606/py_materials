{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    " \n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">An Introduction to h5py </H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Useful References </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <A HREF=\"https://buildmedia.readthedocs.org/media/pdf/h5py/latest/h5py.pdf\">h5py Documentation</A>\n",
    "* <A HREF=\"https://www.christopherlovell.co.uk/blog/2016/04/27/h5py-intro.html\">h5py: reading and writing HDF5 files in Python</A>\n",
    "* <A HREF=\"https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/\">How to use HDF5 files in Python</A>\n",
    "* <a href=\"https://confluence.slac.stanford.edu/pages/viewpage.action?pageId=99485805\">How to access HDF5 data from Python</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> What we will cover </font>\n",
    "* Opening a file\n",
    "* Dimension\n",
    "* Variables\n",
    "* Attributes\n",
    "* Writing data\n",
    "* Creating groups\n",
    "* Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> What is HDF5?</font>\n",
    "* HDF5 is a file format and library for storing scientific data. \n",
    "* It was designed to meet growing and ever-changing scientific data-storage and data-handling needs, to take advantage of the power and features of today's computing systems. \n",
    "* It supports files larger than 2 GB and  parallel I/O. \n",
    "* An HDF5 file is a container for two kinds of objects: \n",
    "   1. **Datasets**:, Array-like collections of data.\n",
    "   2. **Groups**: Folder-like containers that hold datasets and other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> What is h5py?</font>\n",
    "\n",
    "* h5py is the Python interface to the HDF5.\n",
    "* Provide easy-to-use high level interface, which allows you to store huge amounts of numerical data.\n",
    "* Easily manipulate that data from NumPy. \n",
    "* Use straightforward NumPy and Python metaphors, like dictionary and NumPy array syntax. \n",
    "* Within h5py, HDF5 groups work like dictionaries, and datasets work like NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import six\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'> Opening a netCDF File</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdFileName = 'sample_hdf5.h5'\n",
    "modeType   = 'w'\n",
    "hdfid = h5py.File(hdFileName, modeType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `modeType`: 'w', 'r+', 'r', or 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdfid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'> Data Compression </font>\n",
    "* When saving data, you may opt for compressing it using different algorithms. \n",
    "* h5py supports a few compression filters such as GZIP, LZF, and SZIP. \n",
    "* When using one of the compression filters, the data will be processed on its way to the disk and it will be decompressed when reading it. \n",
    "* When readding data, the underlying HDF5 library automatically extracts the data from the compressed datasets with the appropriate algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gzip compression flag\n",
    "comp = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Specify the data type to optimize space\n",
    "* Set data compression if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'> Creating Dimensions in a HDF5 File</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlats = 46\n",
    "nlons = 90\n",
    "nlevs = 20\n",
    "\n",
    "# Latitude    \n",
    "lat = np.linspace(-90.0, 90.0, nlats)\n",
    "dset = hdfid.require_dataset('lat', shape=lat.shape, dtype=np.float32, \n",
    "                             compression=\"gzip\", compression_opts=comp)\n",
    "dset[...] = lat\n",
    "dset.attrs['name'] = 'latitude'\n",
    "dset.attrs['units'] = 'degrees north'\n",
    "\n",
    "# Longitude\n",
    "lon = np.arange(-180.0, 180.0, nlons)\n",
    "dset = hdfid.require_dataset('lon', shape=lon.shape, dtype=np.float32, \n",
    "                             compression=\"gzip\", compression_opts=comp)\n",
    "dset[...] = lon\n",
    "dset.attrs['name'] = 'longitude'\n",
    "dset.attrs['units'] = 'degrees east'\n",
    "\n",
    "# Vertical levels\n",
    "lev = np.arange(0, nlevs, 1)\n",
    "dset = hdfid.require_dataset('lev', shape=lev.shape, dtype=np.int, \n",
    "                             compression=\"gzip\", compression_opts=comp)\n",
    "dset[...] = lev\n",
    "dset.attrs.update({'name': 'vertical levels',\n",
    "                   'units': 'hPa'})\n",
    "\n",
    "# Time (note the unlimited dimension)\n",
    "time = np.arange(0,1,1)\n",
    "dset = hdfid.require_dataset('time', shape=time.shape, maxshape=(None), \n",
    "                             dtype=np.float32, compression=comp)\n",
    "dset[...] = time\n",
    "dset.attrs['name'] = 'time'\n",
    "dset.attrs['units'] = 'hours since 2013-01-01 00:00:00.0'\n",
    "dset.attrs['calendar'] = 'gregorian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Create Variables</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecs = 5\n",
    "arr = np.zeros((nrecs, nlevs, nlats, nlons))\n",
    "arr[0:nrecs,:,:,:] = 300*np.random.uniform(size=(nrecs,nlevs,nlats,nlons))\n",
    "dset = hdfid.require_dataset('temp', shape=arr.shape, \n",
    "                             dtype=np.float32, compression=comp)\n",
    "dset[...] = arr\n",
    "dset.attrs['name'] = 'temperature'\n",
    "dset.attrs['units'] = 'K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `create_dataset` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.zeros((lat.size,lon.size))\n",
    "arr2[:,:] = np.random.random(size=(lat.size,lon.size))\n",
    "landfrac = hdfid.create_dataset('landfrac', data=arr2, dtype=np.float32)\n",
    "landfrac.attrs['name'] = 'Fraction of land'\n",
    "landfrac.attrs['units'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Adding Global Attributes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfid.attrs['Description'] = 'Sample HDF5 file'\n",
    "hdfid.attrs['History']     = 'Created for the h5py Tutorial'\n",
    "hdfid.attrs['Source']      = 'NASA GSFC'\n",
    "hdfid.attrs['HDF5_Version'] = six.u(h5py.version.hdf5_version)\n",
    "hdfid.attrs['h5py_version'] = six.u(h5py.version.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_attr = {'Date': dt.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"), \n",
    "            'User': 'ASTG',}\n",
    "hdfid.attrs.update(glob_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the formulation based on `json` to write metadata in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {'Note': 'Another way to add metadata in file', \n",
    "            'OS': os.name,}\n",
    "m = hdfid.create_dataset('metadata', data=json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Printing File Attributes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in hdfid.attrs.keys():\n",
    "    print('{} => {}'.format(k, hdfid.attrs[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_read = json.loads(hdfid['metadata'][()])\n",
    "for k in metadata_read:\n",
    "    print('{} => {}'.format(k, metadata_read[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>List all Variables</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in hdfid:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Create Groups</font>\n",
    "* We can organize data in hierarchical groups, which are analogous to directories in a filesystem. \n",
    "* Groups serve as containers for variables, dimensions and attributes, as well as other groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpData2D = hdfid.create_group('2D_Data')\n",
    "gpData2D.attrs['Description'] = \"Group for 2D variables\"\n",
    "gpData2D.attrs['Sub groups'] = \"Land and Sea\"\n",
    "\n",
    "sgpLand  = gpData2D.create_group('2D_Land')\n",
    "sgpSea   = gpData2D.create_group('2D_Sea')\n",
    "\n",
    "gpData3D = hdfid.create_group('3D_Data')\n",
    "gpData3D.attrs['Description'] = \"Group for 2D variables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all variables and top groups\n",
    "for item in hdfid:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parent of \"2D_Land\" is {}'.format(sgpLand.parent))\n",
    "print('Parent of \"3D_Data\" is {}'.format(gpData3D.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in gpData2D.attrs.keys():\n",
    "    print('{} => {}'.format(j, gpData2D.attrs[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in gpData2D:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all top variables and top groups in a file\n",
    "for key in hdfid.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `visit()` method allows to list all the nested groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(name):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfid.visit(get_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `visititems()` method allows you to view all items in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "def get_all_objs(name, obj):\n",
    "    attr = list(obj.attrs.items())\n",
    "    print(name, obj, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfid.visititems(get_all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add variable to a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.zeros((nrecs, nlats, nlons))\n",
    "arr2[0:nrecs,:,:] = 300*np.random.uniform(\n",
    "                                size=(nrecs, nlats, nlons))\n",
    "surf_pres = sgpLand.create_dataset('sp', data=arr2)\n",
    "surf_pres.attrs['name'] = 'Surface Pressure'\n",
    "surf_pres.attrs['units'] = 'Pa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gpData3D.create_dataset('temp', data=arr)\n",
    "temp.attrs['name'] = 'temperature (3D)'\n",
    "temp.attrs['units'] = 'K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdfid[\"3D_Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdfid[\"2D_Data/2D_Land\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in hdfid.attrs.keys():\n",
    "    print('{} => {}'.format(j, hdfid.attrs[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in gpData2D.attrs.keys():\n",
    "    print('{} => {}'.format(j, gpData2D.attrs[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Close the file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Reading a HDF5 File</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
    "     print(hdfid.keys())\n",
    "\n",
    "     lev  = hdfid['lev'][()]\n",
    "     lat  = hdfid['lat'][()]\n",
    "     lon  = hdfid['lon'][()]\n",
    "     time = hdfid['time'][()]\n",
    "\n",
    "     temp1 = hdfid['temp'][()]\n",
    "     print(temp1[0,0,0,0], temp1[4,6,7,15])\n",
    "\n",
    "     temp2 = hdfid['3D_Data']['temp'][()]\n",
    "     print(temp2[0,0,0,0], temp2[4,6,7,15])     \n",
    "        \n",
    "     surf_pres = hdfid['2D_Data/2D_Land']['sp'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp1.shape)\n",
    "print(np.min(temp1), np.max(temp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp2.shape)\n",
    "print(np.min(temp2), np.max(temp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(surf_pres.shape)\n",
    "print(np.min(surf_pres), np.max(surf_pres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the names of datasets:** Use `visit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdFileName, 'r') as hf:\n",
    "     hf.visit(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the names of the datasets and the corresponding objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(name):\n",
    "    print(name, hf[name])\n",
    "\n",
    "with h5py.File(hdFileName, 'r') as hf:\n",
    "     hf.visit(my_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the datasets and their attributes:** Use `visititems`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printall(name, obj):\n",
    "    print(name, dict(obj.attrs))\n",
    "\n",
    "with h5py.File(hdFileName, 'r') as hf:\n",
    "     hf.visititems(printall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printall(name, obj):\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        print(name, \" is a Group\")\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        print(name, \" is a Dataset\")\n",
    "    else:\n",
    "        print(name, \" is of an unknown type\")\n",
    "\n",
    "with h5py.File(hdFileName, 'r') as hf:\n",
    "     hf.visititems(printall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get information about HDF5 item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For all HDF5 items:**\n",
    "\n",
    "```python\n",
    "item.id     \n",
    "item.ref     \n",
    "item.parent  \n",
    "item.file   \n",
    "item.name    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
    "     mylist = list(hdfid.keys())\n",
    "     for var in mylist:\n",
    "         obj = hdfid[var]\n",
    "         #if isinstance(obj, h5py.Dataset):\n",
    "         print(obj.name, obj.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Dataset**\n",
    "\n",
    "```python\n",
    "ds.dtype     \n",
    "ds.shape     \n",
    "ds.value     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
    "     mylist = list(hdfid.keys())\n",
    "     for var in mylist:\n",
    "         obj = hdfid[var]\n",
    "         if isinstance(obj, h5py.Dataset):\n",
    "             print(var, obj.dtype, obj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get item attributes for File or Group (if attributes available)**\n",
    "\n",
    "```python\n",
    "item.attrs          # for example: <Attributes of HDF5 object at 230141696>\n",
    "item.attrs.keys()   # for example: ['start.seconds', 'start.nanoseconds']\n",
    "item.attrs.values() # for example: [1297608424L, 627075857L]\n",
    "len(item.attrs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
    "     mylist = list(hdfid.keys())\n",
    "     for var in mylist:\n",
    "         obj = hdfid[var]\n",
    "         print(obj.attrs.keys(), len(obj.attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
    "     for param_key, param in hdfid.items():\n",
    "         msg = '{}:'.format(param_key)\n",
    "         if isinstance(param, h5py.Dataset):\n",
    "             msg += ' {}'.format(param.shape)\n",
    "         print(msg)\n",
    "         if isinstance(param, h5py.Group):\n",
    "             for child_key, child in param.items():\n",
    "                 if isinstance(child, h5py.Dataset):\n",
    "                     print('     {}: {}'.format(child_key, child.shape))\n",
    "                 else:\n",
    "                     print('     {}: {}'.format(child_key, child.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List everything in the file:** `h5ls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attrs(name, obj):\n",
    "    shift = name.count('/') * '    '\n",
    "    print(shift + name)\n",
    "    for key, val in obj.attrs.items():\n",
    "        print(shift + '    ' + f\"{key}: {val}\")\n",
    "        \n",
    "with h5py.File('sample_hdf5.h5', mode='r') as hdfid:\n",
    "     hdfid.visititems(print_attrs)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the list of all the datasets and their values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = dict()\n",
    "def get_data(name, obj, dct=dct):\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        _name = name if name.startswith('/') else '/'+name\n",
    "        dct[_name] = obj[()]\n",
    "\n",
    "with h5py.File('sample_hdf5.h5', mode='r') as hdfid:         \n",
    "     hdfid.visititems(get_data)\n",
    "    \n",
    "\n",
    "for key in dct:\n",
    "    print(\"{}:\".format(key, dct[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Updating a Variable in an Existing HDF5 File</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdFileName, mode='a') as hdfid:\n",
    "     tempO = hdfid['temp']  # Read the object associated with the dataset temp\n",
    "     data = tempO[:]        # Extract the values\n",
    "     print(np.min(data), np.max(data))\n",
    "     data = 1.1*data + 100.0\n",
    "     tempO[:] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(np.min(data), np.max(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdFileName, mode='r') as hdfid:\n",
    "     tempV = hdfid['temp'][()]\n",
    "     print(np.min(tempV), np.max(tempV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Resizing Datasets</font>\n",
    "* HDF5 allows resizing datasets on the fly and with little computational cost. \n",
    "* Datasets can be resized once created up to a maximum size. \n",
    "* You need to specify this maximum size when creating the dataset, via the keyword `maxshape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "- Create a dataset to store `100` values and set a maximum size of up to `500` (`maxshape=(500, )`) values.\n",
    "- Populate the first `100` entries of the dataset.\n",
    "- Expand (new size of `200`) the dataset to store `100` more entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('resize_dataset.h5', 'w') as hdfid:\n",
    "    d = hdfid.create_dataset('dataset', (100, ),  maxshape=(500, ))\n",
    "    d[:100] = np.random.uniform(0.0,99.0, (100,))\n",
    "    d.resize((200,))\n",
    "    d[100:200] = np.random.uniform(100,199.0, (100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open the file and read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('resize_dataset.h5', 'r') as hdfid:\n",
    "    dset = hdfid['dataset']\n",
    "    print(dset[99])\n",
    "    print(dset[199])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also resize the dataset at a later stage, don't need to do it in the same session when you created the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('resize_dataset.h5', 'a') as hdfid:\n",
    "    dset = hdfid['dataset']\n",
    "    dset.resize((300,))\n",
    "    #dset[:200] = 0\n",
    "    dset[200:300] = np.random.uniform(200, 299.0, (100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('resize_dataset.h5', 'r') as hdfid:\n",
    "    dset = hdfid['dataset']\n",
    "    print(dset[99])\n",
    "    print(dset[199])\n",
    "    print(dset[299])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
