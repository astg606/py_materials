{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "Accelerating (Numba) and Scaling (Dask) Python Codes on CPUs\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Reference Documents</font>\n",
    "- <a href=\"http://numba.pydata.org/\">Numba: A High Performance Python Compiler</a>\n",
    "- <a href=\"https://examples.dask.org/applications/stencils-with-numba.html\">Stencil Computations with Numba</a>\n",
    "- <a href=\"http://deepdata.com.pl/numba.html\">Python on steroids - speeding up calculations with numba</a>\n",
    "- <a href=\"https://thedatafrog.com/en/articles/make-python-fast-numba/\">Make python fast with numba</a>\n",
    "- <a href=\"https://www.deeplearningwizard.com/deep_learning/production_pytorch/speed_optimization_basics_numba/\">Speed Optimization Basics: Numba</a>\n",
    "- <a href=\"https://flothesof.github.io/optimizing-python-code-numpy-cython-pythran-numba.html\">Optimizing your code with NumPy, Cython, pythran and numba </a>\n",
    "\n",
    "- <a href=\"https://docs.dask.org/en/latest/why.html\">Why Dask?</a>\n",
    "- <a href=\"https://github.com/dask/dask-tutorial\">dask-tutorial</a>\n",
    "- <a href=\"https://www.manning.com/books/data-science-with-python-and-dask\">Data Science with Python and Dask</a>\n",
    "- <a href=\"https://carpentries-incubator.github.io/lesson-parallel-python/aio/index.html\">Parallel Programming in Python</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=uGy5gT2vLdI&feature=youtu.be\"> Working with the Python DASK library (video)</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=t_GRK4L-bnw&feature=youtu.be\">Who uses Dask (video)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>What will be Covered?</font>\n",
    "\n",
    "* Numba\n",
    "   * What is Numba?\n",
    "   * How Does Numba Work?\n",
    "   * How to Use Numba?\n",
    "   * Parallelization with Numba\n",
    "* Dask\n",
    "   * What is Dask?\n",
    "   * Parallelize Code with `delayed`\n",
    "   * Dask Array\n",
    "   * Dask DataFrame\n",
    "   * Dask Schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking how many cores are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print(\"Number of available cores: \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Numba</font>\n",
    "\n",
    "![fig_numba](https://thedatafrog.com/static/blog/images/2019/07/python_fast.0d88afcb4f8a.png)\n",
    "Image Source: Lison Bernet 2019\n",
    "\n",
    "## <font color='red'>What is Numba?</font>\n",
    "\n",
    "> Numba is an open-source JIT compiler that translates a subset of Python and NumPy into fast machine code using `LLVM` (low-level virtual machine), via the llvmlite Python package. It offers a range of options for parallelising Python code for CPUs and GPUs, often with only minor code changes. \n",
    ">\n",
    ">Wikipedia\n",
    "\n",
    "- Numba is a Python open source package that was originally developed by Continuum Analytics.\n",
    "- The core application area are math-heavy and array-oriented functions, which are in native Python pretty slow.\n",
    "- From a function, Numba can generate native code for that function as well as the wrapper code needed to call it directly from Python. This compilation is done on-the-fly and in-memory.\n",
    "- It accelerates Python code (numerical functions) for both CPU and GPU:\n",
    "   - **Function Compiler**: Numba compiles Python functions, not whole applications or parts of it. It is a Python module meant to improve the performance of functions with the goal of achieving a speed comparable to `C`.\n",
    "   - **Just-in-time**: (Dynamic translation) Numba translates the bytecode (intermediate code more abstract than the machine code) to machine code immediately before its execution to improve the execution speed.\n",
    "   - **Numerically-focused**: Numba is focused on numerical data, such as int, float, complex. \n",
    "   \n",
    "## <font color='red'>How Does Numba Work?</font>\n",
    "\n",
    "- Assume that you have a function `do_math` that is decorated with the Numba `@jit` decorator. \n",
    "- Compilation will be deferred until the first function execution. \n",
    "- Numba will infer the argument types at call time, and generate optimized code based on this information. \n",
    "- Numba will also be able to compile separate specializations depending on the input types. \n",
    "- The diagram below, shows all the steps carried out by Numba to execute `do_math`. \n",
    "\n",
    "![fig_numba](https://miro.medium.com/max/1400/1*S0S4QUjR-BsdTICtT9797Q.png)\n",
    "Image Source: Continuum Analytics\n",
    "\n",
    "- **IR**: Intermediate Representations\n",
    "- **Bytecode Analysis**: Intermediate code more abstract than machine code\n",
    "- **LLVM**: Low Level Virtual Machine, infrastructure to develop compilers\n",
    "- **NVVM**: It is an IR compiler based on LLVM, it is designed to represent GPU kernels\n",
    "\n",
    "## <font color='red'>Numpy and Numba</font>\n",
    "- One objective of Numba is having a seamless integration with NumPy. \n",
    "- Numba excels at generating code that executes on top of NumPy arrays.\n",
    "- NumPy support in Numba comes in many forms:\n",
    "    1. Numba understands calls to NumPy ufuncs (universal functions: there are over 60 of them) and is able to generate equivalent native code for many of them.\n",
    "    2. NumPy arrays are directly supported in Numba.\n",
    "    3. Numba is able to generate ufuncs and gufuncs (generalized universal functions). This means that it is possible to implement ufuncs and gufuncs within Python, getting speeds comparable to that of ufuncs/gufuncs implemented in C extension modules using the NumPy C API.\n",
    "    \n",
    "## <font color='red'>Usage</font>\n",
    "- Numba provides several utilities for code generation.\n",
    "- Its central feature is the `numba.jit()` decorator. \n",
    "- Using this decorator, you can mark a function for optimization by Numba’s JIT compiler. - - - Various invocation modes trigger differing compilation options and behaviours.\n",
    "\n",
    "\n",
    "Consider using Numba if:\n",
    "\n",
    "- Is numerically orientated.\n",
    "- Uses Numpy\n",
    "- Relies on loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "from numba import njit\n",
    "from numba import prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking your System**\n",
    "\n",
    "The `numba -s` or `numba --sysinfo` command prints a lot of information about your system and your Numba installation and relevant dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Consider the function that multiplies two `nxn` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tmp = 0.\n",
    "            for k in range(n):\n",
    "                tmp  += A[i, k]*B[k, j]\n",
    "            C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "A = np.random.rand(N, N)\n",
    "B = np.random.rand(N, N)\n",
    "C = np.zeros_like(A)\n",
    "D = np.random.rand(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRegMat = %timeit -o matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to use `Numba`:\n",
    "\n",
    "- Method 1: As a function calling the function we want to speed\n",
    "- Method 2: As a decorator of the function we want to speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_matrix_multiplication = jit(matrix_multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat0 = %timeit -o numba_matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 0: {}\".format(tRegMat.best/tNumMat0.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def matrix_multiplication_numba(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tmp = 0.\n",
    "            for k in range(n):\n",
    "                tmp  += A[i, k]*B[k, j]\n",
    "            C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit matrix_multiplication_numba(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measuring the Performance of Numba**\n",
    "\n",
    "- Once the compilation has taken place, Numba runs the machine code version of your function. \n",
    "- If it is called again with same argument types, it can reuse the cached version instead of having to compile again.\n",
    "- A common mistake when measuring performance is not accounting for the above behaviour and to time code once with a simple timer that includes the time taken to compile your function in the execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def matrix_multiplication_numba(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tmp = 0.\n",
    "            for k in range(n):\n",
    "                tmp  += A[i, k]*B[k, j]\n",
    "            C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_1 = time.time()\n",
    "matrix_multiplication_numba(A, B, C)\n",
    "end_1 = time.time()\n",
    "print(\"Elapsed (with compilation) = {}\".format(end_1 - start_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW THE FUNCTION IS COMPILED, RE-TIME IT EXECUTING FROM CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_2 = time.time()\n",
    "matrix_multiplication_numba(A, B, C)\n",
    "end_2 = time.time()\n",
    "print(\"Elapsed (after compilation) = {}\".format(end_2 - start_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gain of time = {}\".format((end_1 - start_1)/(end_2 - start_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Signature\n",
    "\n",
    "- You can specify the signature of the Numba function by describing the types of the arguments and the return type of the function. \n",
    "- This can produce slightly faster code as the compiler does not need to infer the types. \n",
    "- The drawback is that the function can no longer accept other types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_numbers(x, y):\n",
    "    return (x + y)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_average_numbers = jit(nb.float64(nb.int32, nb.int32))(average_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nb.float64(nb.int32, nb.int32))\n",
    "def average_numbers_numba(x, y):\n",
    "    return (x + y)/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nb.float64(nb.int32, nb.int32)` is the function’s signature specifying a function that takes two 32-bit integer arguments and returns a double precision float.\n",
    "- You can also use the abbreviated notation: `nb.f8(nb.i4, nb.i4)`\n",
    "- If you only pass `(nb.i4, nb.i4)` instead of `nb.f8(nb.i4, nb.i4)`, Numba will try to infer the type of the return value.\n",
    "- Array signatures are specified by subscripting a base type according to the number of dimensions. \n",
    "     - A 1-dimension single-precision array would be written `nb.f4[:]`.\n",
    "     - A 3-dimension array of the same underlying type would be `nb.f4[:,:,:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_matrix_multiplication = jit((nb.f8[:,:], nb.f8[:,:], nb.f8[:,:]))(matrix_multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat1 = %timeit -o numba_matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 1: {}\".format(tRegMat.best/tNumMat1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Example: Finding the Closet Two Points**\n",
    "\n",
    "- Find the two closest points in an array of points in 2D. \n",
    "- Returns the two points, and the distance between them.\n",
    "- If we have $N$ points, we would have to test NxN pairs of points. \n",
    "- This algorithm has a complexity of order $N \\times N$, denoted $O(N \\times N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def python_closest(points):\n",
    "    min_distance2 = 999999.\n",
    "    mdp1, mdp2 = None, None\n",
    "    for i in range(len(points)):\n",
    "        for j in range(i+1, len(points)):\n",
    "            distance2 = (points[i][0]-points[j][0])**2 + \\\n",
    "                        (points[i][1]-points[j][1])**2\n",
    "            if distance2 < min_distance2:\n",
    "                min_distance2 = distance2\n",
    "                mdp1, mdp2 = points[i], points[j]\n",
    "    return mdp1, mdp2, math.sqrt(min_distance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.uniform((-1,-1), (1,1), (8100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcloset = %timeit -o python_closest(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Numba to speedup the calculations. We can explicitly pass the types of the arguments to have a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@jit('Tuple((float64[:], float64[:], float64))(float64[:,:])', nopython=True)\n",
    "def numba_closest(points):\n",
    "    min_distance2 = 999999.\n",
    "    mdp1, mdp2 = None, None\n",
    "    for i in prange(len(points)):\n",
    "        for j in prange(i+1, len(points)):\n",
    "            distance2 = (points[i][0]-points[j][0])**2 + \\\n",
    "                        (points[i][1]-points[j][1])**2\n",
    "            if distance2 < min_distance2:\n",
    "                min_distance2 = distance2\n",
    "                mdp1, mdp2 = points[i], points[j]\n",
    "    return mdp1, mdp2, math.sqrt(min_distance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcloset_numba = %timeit -o numba_closest(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Closest Points: {}\".format(tcloset.best/tcloset_numba.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation Options\n",
    "A number of keyword-only arguments can be passed to the `@jit` decorator:\n",
    "1. `nopython`: Numba has two compilation modes:\n",
    "     - **nopython mode** (`nopython=True`): Compile the decorated function so that it will run entirely without the involvement of the Python interpreter. This mode produces the highest performance code, but requires that the native types of all values in the function can be inferred. Note that <font color=\"red\">**`@njit`**</font> is an alias for <font color=\"red\">**`@jit(nopython=True)`**</font>.\n",
    "     - **object mode**: In this mode Numba will identify loops that it can compile and compile those into functions that run in machine code, and it will run the rest of the code in the interpreter. For best performance avoid using this mode!\n",
    "     - By default Numba will automatically use **object mode** if **nopython mode** cannot be used for some reason. \n",
    "     - When you are in **nopython mode**, types that cannot be inferred by the compiler will generate an error.\n",
    "2. `nogil`: \n",
    "     - Whenever Numba optimizes Python code to native code that only works on native types and variables (rather than Python objects), it is not necessary anymore to hold Python’s global interpreter lock (GIL). \n",
    "     - Numba will release the GIL when entering such a compiled function if you passed `nogil=True`.\n",
    "     - When using `nogil=True`, you need to be wary of the usual pitfalls of multi-threaded programming (consistency, synchronization, race conditions, etc.).\n",
    "3. `cache`:\n",
    "     - To avoid compilation times each time you invoke a Python program, you can instruct Numba to write the result of function compilation into a file-based cache. \n",
    "     - This is done by passing `cache=True`.\n",
    "4. `parallel`: \n",
    "     - Enables automatic parallelization (and related optimizations) for operations in the function known to have parallel semantics.\n",
    "     - This feature is enabled by passing `parallel=True` and must be used in conjunction with `nopython=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use Numba (and Dask) to speed up the code below (calculations of `pi`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "def approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples\n",
    "\n",
    "def mean(*args):\n",
    "    return sum(args) / len(args)\n",
    "\n",
    "num_samples = 10**6\n",
    "num_experiments = 10\n",
    "\n",
    "pi_approx = mean(*[approximate_pi(num_samples) for i in range(num_experiments)])\n",
    "\n",
    "print(\"Approximation of Pi: {}\".format(pi_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Click here to access the solution</b></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "@nb.jit(nopython=True, nogil=True)\n",
    "def nb_approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples\n",
    "\n",
    "def mean(*args):\n",
    "    return sum(args) / len(args)\n",
    "\n",
    "num_samples = 10**6\n",
    "num_experiments = 10\n",
    "\n",
    "pi_approx = mean(*[np_approximate_pi(num_samples) for i in range(num_experiments)])\n",
    "\n",
    "print(\"Approximation of Pi: {}\".format(pi_approx))\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastmath\n",
    "- In certain classes of applications strict IEEE 754 compliance is less important. \n",
    "- It is possible to relax some numerical rigour with view of gaining additional performance. \n",
    "- The way to achieve this behaviour in Numba is through the use of the `fastmath` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=False)\n",
    "def do_sum(A):\n",
    "    acc = 0.\n",
    "    # without fastmath, this loop must accumulate in strict order\n",
    "    for x in A:\n",
    "        acc += np.sqrt(x)\n",
    "    return acc\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def do_sum_fast(A):\n",
    "    acc = 0.\n",
    "    # with fastmath, the reduction can be vectorized as floating point\n",
    "    # reassociation is permitted.\n",
    "    for x in A:\n",
    "        acc += np.sqrt(x)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_do_sum = %timeit -o acc1 = do_sum(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_do_sum_fast = %timeit  -o acc2 = do_sum_fast(D)\n",
    "print(time_do_sum.best / time_do_sum_fast.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Parallelization </font>\n",
    "\n",
    "- The setting `parallel=True` in `jit()` enables a Numba transformation pass that attempts to automatically parallelize and perform other optimizations on (part of) a function.\n",
    "- A user program may contain operations (for instance adding a scalar value to an array) that are known to have parallel semantics.\n",
    "- Each operation could be parallelized individually but that might light to poor performance due to poor cache behavior.\n",
    "- Numba uses instead auto-parallelization where it identifies all operations with parallel semantics and fuses adjacent ones together, to form one or more kernels that are automatically run in parallel.\n",
    "- The process is fully automated without modifications to the user program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Parallel Loops\n",
    "\n",
    "- Numba parallel execution also has support for explicit parallel loop declaration similar to that in OpenMP. \n",
    "- To indicate that a loop should be executed in parallel the `numba.prange` function should be used.\n",
    "- This function behaves like Python `range` and if `parallel=True` is not set it acts simply as an alias of `range`. \n",
    "- Loops induced with `prange` can be used for embarrassingly parallel computation and also reductions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def matrix_multiplication_numba2(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in prange(n):\n",
    "        for j in prange(n):\n",
    "            tmp = 0.\n",
    "            for k in prange(n):\n",
    "                tmp += A[i, k]*B[k, j]\n",
    "            C[i,j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat2 = %timeit -o matrix_multiplication_numba2(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 2: {}\".format(tRegMat.best/tNumMat2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_functions(n):\n",
    "    \"\"\"\n",
    "        Evaluate the trigononmetric functions for n values evenly\n",
    "        spaced over the interval [-1500.00, 1500.00]\n",
    "    \"\"\"\n",
    "    vector1 = np.linspace(-1500.00, 1500.0, n)\n",
    "    iterations = 10000\n",
    "    for i in range(iterations):\n",
    "        vector2 = np.sin(vector1)\n",
    "        vector1 = np.arcsin(vector2)\n",
    "        vector2 = np.cos(vector1)\n",
    "        vector1 = np.arccos(vector2)\n",
    "        vector2 = np.tan(vector1)\n",
    "        vector1 = np.arctan(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def evaluate_functions_numba(n):\n",
    "    \"\"\"\n",
    "        Evaluate the trigononmetric functions for n values evenly\n",
    "        spaced over the interval [-1500.00, 1500.00]\n",
    "    \"\"\"\n",
    "    vector1 = np.linspace(-1500.00, 1500.0, n)\n",
    "    iterations = 10000\n",
    "    for i in prange(iterations):\n",
    "        vector2 = np.sin(vector1)\n",
    "        vector1 = np.arcsin(vector2)\n",
    "        vector2 = np.cos(vector1)\n",
    "        vector1 = np.arccos(vector2)\n",
    "        vector2 = np.tan(vector1)\n",
    "        vector1 = np.arctan(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRegFun = %timeit -o evaluate_functions(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumFun = %timeit -o evaluate_functions_numba(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup: {}\".format(tRegFun.best/tNumFun.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics\n",
    "- We can produce diagnostic information about the transforms undertaken in automatically parallelizing the decorated code. \n",
    "- This information can be accessed in two ways:\n",
    "     1. Setting the environment variable: `NUMBA_PARALLEL_DIAGNOSTICS`\n",
    "     2. Calling the function `parallel_diagnostics()`\n",
    "- The level of verbosity in the diagnostic information is controlled by an integer argument of value between 1 and 4 inclusive, 1 being the least verbose and 4 the most.\n",
    "\n",
    "For additional information, consult the webpage: <a href=\"http://numba.pydata.org/numba-doc/latest/user/parallel.html\"> http://numba.pydata.org/numba-doc/latest/user/parallel.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_functions_numba.parallel_diagnostics(level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Calling Other Functions</font>\n",
    "\n",
    "- Numba functions can call other Numba functions. \n",
    "- Both functions must have the `@jit` decorator, otherwise the code will be much slower.\n",
    "\n",
    "```python\n",
    "@jit\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    return math.sqrt(square(x) + square(y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Numba and Pandas</font>\n",
    "\n",
    "- Pandas is built on top of Numpy.\n",
    "- Pandas offers flexibility in manipulating data but not necessary speed.\n",
    "- This flexibility allows the creation of built-in function.\n",
    "- Crude looping (over DataFrame rows for instance) in Pandas does not take advantage of any built-in optimizations, making it extremely inefficient.\n",
    "- Using vectorized Pandas built-in functions (acting on Pandas Series) is almost always preferable to accomplishing similar ends with custom-written looping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "- We have a Pandas DataFrame and we want to add a new column by multiplying an exiting column by a constant.\n",
    "- We use three methods methods for the multiplication operations: `apply` method, Pandas and vectorization with Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def multiply(x):\n",
    "    return x * 5\n",
    "    \n",
    "@nb.vectorize\n",
    "def multiply_numba(x):\n",
    "    return x * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table of 100,000 rows and 4 columns filled with random numbers from 0 to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(0,100,size=(100000, 4)),columns=['a', 'b', 'c', 'd'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_apply = %timeit -o df['new_col1'] = df['a'].apply(multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pandas = %timeit -o df['new_col2'] = df['a'] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_numba1 = %timeit -o df['new_col3'] = multiply_numba(df['a'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multiply Apply:  {}\".format(time_apply.best/time_apply.best))\n",
    "print(\"Multiply Pandas: {}\".format(time_apply.best/time_pandas.best))\n",
    "print(\"Multiply Numba:  {}\".format(time_apply.best/time_numba1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "- Square the values of each row and take their mean to create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_mean(row):\n",
    "    row = np.power(row, 2)\n",
    "    return np.mean(row)\n",
    "\n",
    "@njit\n",
    "def square_mean_numba(arr):\n",
    "    res = np.empty(arr.shape[0])\n",
    "    arr = np.power(arr, 2)\n",
    "    for i in prange(arr.shape[0]):\n",
    "        res[i] = np.mean(arr[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_list = [10, 100, 1000, 10000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_times = list()\n",
    "for nrows in nrows_list:\n",
    "    df = pd.DataFrame(np.random.randint(0,100,size=(nrows, 2)),columns=['a', 'b'])\n",
    "    tp = %timeit -o df['new_col'] = df.apply(square_mean, axis=1)\n",
    "    pandas_times.append(tp.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_times = list()\n",
    "for nrows in nrows_list:\n",
    "    df = pd.DataFrame(np.random.randint(0,100,size=(nrows, 2)),columns=['a', 'b'])\n",
    "    tn = %timeit -o df['new_col'] = square_mean_numba(df.to_numpy())\n",
    "    numba_times.append(tn.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_times)\n",
    "print(numba_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(nrows_list))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "width = 0.25\n",
    "axes.bar(x, pandas_times, width, label='Pandas apply')\n",
    "axes.bar(x + width, numba_times, width, label='Numba function')\n",
    "axes.set_xticks(x)\n",
    "axes.set_xticklabels(nrows_list)\n",
    "axes.legend(prop={'size': 10})\n",
    "axes.set_yscale('log')\n",
    "axes.set_xlabel('Number of rows')\n",
    "axes.set_ylabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Could we claim that Numpy/Numba is faster than Pandas?**\n",
    "\n",
    "- Not necessarily!\n",
    "- Over time, Pandas relies more on  Cython operations.\n",
    "- In Pandas 1.0 (and newer versions) Pandas’ `apply()` method (applies a function along a specific axis of a DataFrame) can make use of Numba (if installed) instead of cython and be faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Things to Consider when Using Numba</font>\n",
    "\n",
    "- Numba allows its behaviour to be changed through the use of <a href=\"http://numba.pydata.org/numba-doc/latest/reference/envvars.html\">environment variables</a>. Unless otherwise mentioned, those variables have integer values and default to zero.\n",
    "- Not all the <a href=\"http://numba.pydata.org/numba-doc/latest/reference/pysupported.html\">Python features</a> are supported by Numba.\n",
    "- While Python has arbitrary-sized integers, integers in Numba-compiled functions get a fixed size through type inference (usually, the size of a machine integer). This means that arithmetic operations can wrapround or produce undefined results or overflow.\n",
    "- When Numba compiles machine code for functions, it treats global variables as constants to ensure type stability.\n",
    "- Numba may or may not copy global variables referenced inside a compiled function. Small global arrays are copied for potential compiler optimization with immutability assumption. However, large global arrays are not copied to conserve memory. The definition of “small” and “large” may change.\n",
    "- Numba does not work with recusive function.\n",
    "- For some operations, Numba may use a different algorithm than Python or Numpy. The results may not be bit-by-bit compatible. The difference should generally be small and within reasonable expectations. However, small accumulated differences might produce large differences at the end, especially if a divergent function is involved.\n",
    "\n",
    "\n",
    "## <font color=\"red\">[Steps for Getting the Best out of Numba](https://techdecoded.intel.io/resources/parallelism-in-python-using-numba/#gs.m9ly7s)</font>\n",
    "\n",
    "Achieving parallelism in Python with Numba is about knowing a few fundamentals and modifying your workflow to take these methods into account while you’re actively coding in Python. Here are the steps in the process:\n",
    "\n",
    "1. **Ensure the abstraction of your core kernels is appropriate**. Numba requires the optimization target to be in a function. Unnecessarily complex code can cause the Numba compilation to fall back to object code.\n",
    "2. **Look for places in your code where you see processing data in some form of a loop with a known data type**. Examples would be a for-loop iterating over a list of integers, or an arithmetic computation that processes an array in pure Python.\n",
    "3. **If you’re using NumPy and SciPy, look at computations that can be stacked in a single statement and that are not BLAS or LAPACK functions**. These are prime candidates for using the ufunc optimization capabilities of Numba.\n",
    "4. **Experiment with Numba’s compilation options**.\n",
    "5. **Determine the intended datatype signature of the function and core code**. If it’s known (such as int8 or int32), then inform Numba about which input datatype parameters it should expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Dask</font>\n",
    "\n",
    "![fig_dask](https://miro.medium.com/max/1000/1*D6mSsdWECFLn6wJne4VTjg.png)\n",
    "\n",
    "\n",
    "## <font color=\"red\"> What is Dask?</font>\n",
    "\n",
    "- A flexible library for parallel computing in Python that makes it easy to build intuitive workflows for ingesting and analyzing large, distributed datasets. \n",
    "- A native parallel analytics tool designed to integrate seamlessly with Numpy, Pandas, and Scikit-Learn. \n",
    "- An out-of-core (data is read into memory from disk on an as-needed basis) parallelization library that seamlessly integrates with existing NumPy and Pandas data structures to address the following:\n",
    "     * **The available dataset does not fit in memory of a single machine.**\n",
    "     * **The data processing task is time consuming and needs to be sped up.**\n",
    "- Orchestrates parallel threads or processes for us and help speed up processing times.\n",
    "\n",
    "Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler, low-level APIs, and high-level APIs.\n",
    "\n",
    "- Dask provides a few high-level constructs called Dask Bags, Dask DataFrames, and Dask Arrays. They provide an easy-to-use interface to parallelize many of the typical data transformations in ML workflows. \n",
    "- Dask allows the creation of highly customized job execution graphs by using their extensive Python API (e.g., `dask.delayed`) and integration with existing data structures.\n",
    "\n",
    "\n",
    "![fig_layers](http://bicortex.com/bicortex/wp-content/post_content//2019/06/Dask_APIs_Architecture.png)\n",
    "Image Source: bicortex.com\n",
    "\n",
    "\n",
    "The diagram below describes the steps Dask takes to manipulate data.\n",
    "\n",
    "- The operation is broken down into a sequence of operations on smaller partitions of our data (without having to read the whole dataset into memory).\n",
    "- Dask reads each partition as it is needed and computes the intermediate results. \n",
    "- The intermediate results are aggregated into the final result.\n",
    "- Dask handles all of that sequencing internally for us. \n",
    "- On a single machine, Dask can use threads or processors to parallelize these operations. \n",
    "\n",
    "![fig_proc](https://www.manifold.ai/hs-fs/hubfs/Blog%20Post%20Illos/ML%20pipelines%20-%20dask%20single%20machine.jpeg?width=600&name=ML%20pipelines%20-%20dask%20single%20machine.jpeg)\n",
    "Image Source: www.manifold.ai\n",
    "\n",
    "\n",
    "**Advantages of Using Dask**\n",
    "\n",
    "- Fully implemented in Python and natively scales NumPy, Pandas, and scikit-learn.\n",
    "- Can be used effectively to work with both medium datasets on a single machine and large datasets on a cluster.\n",
    "- Can be used as a general framework for parallelizing most Python objects.\n",
    "- Has a very low configuration and maintenance overhead.\n",
    "\n",
    "\n",
    "\n",
    ">Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don’t fit into main memory. Dask’s high-level collections are alternatives to NumPy and Pandas for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install dask[dataframe] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "import memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Parallelize Code with `dask.delayed`</font>\n",
    "\n",
    "- A simple way to parallelize the code.\n",
    "- Allows users to delay function calls into a task graph with dependencies.\n",
    "- Systems like `dask.dataframe` are built with `dask.delayed`.\n",
    "\n",
    "**Simple Example**\n",
    "\n",
    "Consider the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def increment(x):\n",
    "    time.sleep(1.0)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(1.0)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(1.0)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = increment(1)\n",
    "y = increment(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the `dask.delayed` decorator to parallelize the functions `increment` and `add`.\n",
    "- By decorating the functions, we record what we want to compute as tasks into graphs that will be run later on parallel hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = dask.delayed(increment)(1)\n",
    "yd = dask.delayed(increment)(2)\n",
    "zd = dask.delayed(add)(xd, yd)\n",
    "zd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we call the delayed version by passing the arguments, exactly as before, but the original function isn't actually called yet.\n",
    "- A delayed object is made, which keeps track of the function to call and the arguments to pass to it.\n",
    "- We use the `visualize` method (relies on the `graphviz` package) that provide a visual representation of the operations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zd.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we have not physically calculated **total** yet.\n",
    "- We need to apply the `compute` method to get the answer. \n",
    "- <font color=\"red\">It is only here that the data are loaded into memory for calculations</font>.\n",
    "- The calculations are done through using a local thread pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(zd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `delayed` in Loops**\n",
    "\n",
    "Consider the sequential code with two for-loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 7\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = increment(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parallelize the above using the `delayed` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 7\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)\n",
    "\n",
    "totald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the visual representation through a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totald.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(totald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "Use the `delayed` decorator to parallelize the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_odd(x):\n",
    "    return x%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "results = list()\n",
    "\n",
    "for x in data:\n",
    "    if is_odd(x):\n",
    "        y = double(x)\n",
    "    else:\n",
    "        y = increment(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Click here to access the solution</b></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "results = list()\n",
    "\n",
    "for x in data:\n",
    "    if is_odd(x):\n",
    "        y = dask.delayed(double)(x)\n",
    "    else:\n",
    "        y = dask.delayed(increment)(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)\n",
    "print(total.compute())\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Important Lessons</font>**\n",
    "\n",
    "- The `delayed` decorator adds overhead.\n",
    "- It is good not to use it when a task requires a little amount of time.\n",
    "- Call `delayed` on the function not the result.\n",
    "- Break up computations into many pieces. You achieve parallelism by having many delayed calls, not by using only a single one: Dask will not look inside a function decorated with `delayed` and parallelize that code internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Dask Array</font>\n",
    "\n",
    "- Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid. \n",
    "    - _Parallel_: Uses all of the cores on your computer\n",
    "    - _Larger-than-memory_: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "    - _Blocked Algorithms_: Perform large computations by performing many smaller computations\n",
    "- They support a large subset of the Numpy API.\n",
    "\n",
    "![fig_array](https://miro.medium.com/max/1388/1*JfQnXJ5_R104bPyE8_XhwQ.png)\n",
    "\n",
    "**Create a Dask Array**\n",
    "\n",
    "- Create a 20000x20000 array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). \n",
    "- There are 400 (20x20) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((20000, 20000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \",  x.shape)\n",
    "print(\"Size:  \",  x.size)\n",
    "print(\"Chunks: \", x.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Numpy syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x.T\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x.mean(axis=0)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **`compute()`** function if you want your result as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = z.compute()\n",
    "print(type(w), w.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Persit Data in Memory**\n",
    "\n",
    "- If you have the available RAM for your dataset then you can persist data in memory.\n",
    "- This allows future computations to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy against Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_numpy():\n",
    "    x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "    y = x.mean(axis=0)[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "f_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dask():\n",
    "    x = da.random.normal(10, 0.1, size=(20000, 20000), \n",
    "                         chunks=(1000, 1000))\n",
    "    y = x.mean(axis=0)[::100].compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "f_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshapping the chunk size might provide a better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dask2():\n",
    "    x = da.random.normal(10, 0.1, size=(20000, 20000), \n",
    "                         chunks=(10000, 100))\n",
    "    y = x.mean(axis=0)[::100].compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_dask2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask finished faster, but used more total CPU time because Dask was able to transparently parallelize the computation because of the chunk size.**\n",
    "\n",
    "**<font color=\"red\">Things to Consider</font>**\n",
    "\n",
    "- If your data fits in RAM and you are not performance bound, then using NumPy might be the right choice. Dask adds another layer of complexity which may get in the way.\n",
    "- If you are just looking for speedups rather than scalability then you may want to consider using Numba for manipulating Numpy arrays.\n",
    "- How to select the chunk size?\n",
    "     - Too small: huge overheads.\n",
    "     - Poorly aligned with data: inefficient reading.\n",
    "     - Recommended to have a chuck size of at least 100 Mb.\n",
    "     - Choose a chunk size that is large in order to reduce the number of chunks that Dask has to think about (which affects overhead) but also small enough so that many of them can fit in memory at once. Dask will often have as many chunks in memory as twice the number of active threads.\n",
    "   \n",
    "\n",
    "**Avoid Oversubscribing Threads**\n",
    "     \n",
    "- By default Dask will run as many concurrent tasks as you have logical cores. \n",
    "- It assumes that each task will consume about one core.\n",
    "- Many array-computing libraries (used in Dask) are themselves multi-threaded, which can cause contention and low performance.\n",
    "- For better performance, we need to explicitly specify the use of one thread:\n",
    "\n",
    "```python\n",
    "   export OMP_NUM_THREADS=1\n",
    "   export MKL_NUM_THREADS=1\n",
    "   export OPENBLAS_NUM_THREADS=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Dask DataFrames</font>\n",
    "\n",
    "- Pandas is great for tabular datasets that fit in memory. \n",
    "- Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. \n",
    "- Dask DataFrames:\n",
    "     - Coordinate many Pandas DataFrames, partitioned along an index. \n",
    "     - Support a large subset of the Pandas API.\n",
    "- One operation on a Dask DataFrame triggers many Pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints.\n",
    "- Some of the operations that are really fast if you use Dask Dataframes:\n",
    "     - Arithmetic operations (multiplying or adding to a Series)\n",
    "     - Common aggregations (`mean`, `min`, `max`, `sum`, etc.)\n",
    "     - Calling `apply`\n",
    "     - Calling `value_counts()`, `drop_duplicates()` or `corr()`\n",
    "     - Filtering with `loc`, `isin`, and row-wise selection\n",
    "\n",
    "![fig_df](https://pythondata.com/wp-content/uploads/2016/11/Screen-Shot-2016-11-24-at-6.52.24-PM-168x300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> NYC Flights Dataset</font>\n",
    "\n",
    "Data is specific to flights (in 1990's) out of the three airports in the New York City area.\n",
    "\n",
    "\n",
    "Download the remote data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "print(\"\\t Downloading NYC dataset...\", end=\"\\n\", flush=True)\n",
    "\n",
    "url = \"https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\"\n",
    "filename, header = urllib.request.urlretrieve(url, \"nycflights.tar.gz\")\n",
    "\n",
    "print(\"\\t Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `.csv` files from the tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(filename, mode=\"r:gz\") as flights:\n",
    "     flights.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt data/nycflights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real all the files at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The representation of the dataframe object contains no data. \n",
    "- `pandas.read_csv` reads in the entire file before inferring datatypes.\n",
    "- `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file). These inferred datatypes are then enforced when reading all partitions.\n",
    "\n",
    "We can display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the last few rows, we have a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an issue with the data types of few columns.\n",
    "- The datatypes inferred in the sample are incorrect.\n",
    "- We can fix it by reading the files again and specify the appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]},\n",
    "                dtype={'TailNum': str,\n",
    "                       'CRSElapsedTime': float,\n",
    "                       'Cancelled': bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Perform Operations as with `Pandas DataFrames`</font>\n",
    "\n",
    "**Maximum value of a column**:\n",
    "\n",
    "- We now want to compute the maximum of the `DepDelay` column.\n",
    "- With `Pandas`, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\n",
    "- `dask.dataframe` allows us to write pandas-like code that operates on large than memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do the same thing in `Pandas`, we will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import glob\n",
    "\n",
    "list_files = glob.glob(\"data/nycflights/*csv\")\n",
    "   \n",
    "maxes = list()\n",
    "for file_name in list_files:\n",
    "    pddf = pd.read_csv(file_name)\n",
    "    maxes.append(pddf.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "\n",
    "print(\"Final Maximum: \", max(maxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Dest == 'PIT'].compute().plot(kind='scatter', \n",
    "                                    x=\"DayOfWeek\", \n",
    "                                    y=\"DepDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Operations\n",
    "\n",
    "Number of non-cancelled flights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights were taken from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average departure delay from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"DayOfWeek\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by destinations and count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\").count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\")[\"ArrDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ArrDelay+df.DepDelay>30.0].groupby(\"Dest\").Dest.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharing Intermediate Results**\n",
    "\n",
    "- We sometimes do the same operation more than once. \n",
    "- For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass both to a single `compute` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = da.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "- Consider the code below that computes the mean departure delay per airport. \n",
    "- Parallelize the code using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "sum_delays = list()\n",
    "count_delays = list()\n",
    "\n",
    "for file_name in list_files:\n",
    "    pddf = pd.read_csv(file_name)\n",
    "    by_origin = pddf.groupby('Origin')\n",
    "    loc_total = by_origin.DepDelay.sum()\n",
    "    loc_count = by_origin.DepDelay.count()\n",
    "    sum_delays.append(loc_total)\n",
    "    count_delays.append(loc_count)\n",
    "\n",
    "total_delays = sum(sum_delays)\n",
    "n_flights = sum(count_delays)\n",
    "mean_delays = total_delays / n_flights\n",
    "print(\"Mean delays: {}\".format(mean_delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Schedulers</font>\n",
    "\n",
    "- After Dask generates the task graphs, it needs to execute them on parallel hardware. \n",
    "- It is the role of a task scheduler. \n",
    "- There are different task schedulers. Each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "\n",
    "![schedulers](https://docs.dask.org/en/latest/_images/dask-overview.svg)\n",
    "\n",
    "Image Source: [https://docs.dask.org/en/latest/](https://docs.dask.org/en/latest/)\n",
    "\n",
    "To execute the task graphs there are two types of schedulers:\n",
    "* **Single machine**: Provides basic features on a local process or thread pool. It is simple and cheap to use, although it can only be used on a single machine and does not scale\n",
    "* **Distributed**: Offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster.\n",
    "\n",
    "### <font color=\"green\"> Single Machine Scheduler</font>\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 7\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)\n",
    "\n",
    "totald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print(\"Number of available processors: \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single thread**\n",
    "\n",
    "- The single-threaded synchronous scheduler executes all computations in the local thread with no parallelism at all.\n",
    "- It is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local threads**\n",
    "\n",
    "- Uses `multiprocessing.pool.ThreadPool`\n",
    "\n",
    "To use all the available processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local processes**\n",
    "\n",
    "- The multiprocessing scheduler executes computations with a local `multiprocessing.Pool`.\n",
    "- Every task and all of its dependencies are shipped to a local process, executed, and then their result is shipped back to the main process. \n",
    "- Moving data to remote processes and back can introduce performance penalties, particularly when the data being transferred between processes is large. \n",
    "- The multiprocessing scheduler is an excellent choice when workflows are relatively linear, and so does not involve significant inter-task data transfer as well as when inputs and outputs are both small, like filenames and counts.\n",
    "\n",
    "To use all the available processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Distributed Scheduler</font>\n",
    "\n",
    "- The Dask distributed scheduler can either be setup on a cluster or run locally on a personal machine. \n",
    "- It is a centrally managed, distributed, dynamic task scheduler. \n",
    "     - The central dask-scheduler process coordinates the actions of several dask-worker processes spread across multiple machines and the concurrent requests of several clients.\n",
    "     - The scheduler is asynchronous and event-driven, simultaneously responding to requests for computation from multiple clients and tracking the progress of multiple workers.\n",
    "     - The event-driven and asynchronous nature makes it flexible to concurrently handle a variety of workloads coming from multiple users at the same time while also handling a fluid worker population with failures and additions. \n",
    "     - Workers communicate amongst each other for bulk data transfer over TCP.\n",
    "- To set up `dask.distributed`, we need to create client instance by calling `Client` class from `dask.distributed`. \n",
    "- It will internally create a dask scheduler and dask workers. \n",
    "- We will get the **link of the dashboard** where we can analyze tasks running in parallel. \n",
    "- We can pass a number of workers (using the `n_workers` argument) and threads to use per worker process (using the `threads_per_worker` argument).\n",
    "- As soon as you create a client, Dask will automatically start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "#client = Client(n_workers=2, threads_per_worker=4)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren’t in jupyterlab and using the `dask-labextension`, you can  click the `Dashboard` link to open up the diagnostics dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_slow_add(x, y):\n",
    "    time.sleep(random.randrange(3,10))\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y = dask.delayed(random_slow_add)(x, 1)\n",
    "    results.append(y)\n",
    "    \n",
    "total = dask.delayed(sum)(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Things to Consider</font>**\n",
    "\n",
    "- Each Dask task has overhead (about 1 ms). If you have a lot tasks this overhead can add up. It is a good idea to give each task more than a few seconds of work.\n",
    "- To better understand how your program is performing, check the [Dask Performance Diagnostics](https://distributed.dask.org/en/latest/diagnosing-performance.html) documentation. You can also view the [video](https://docs.dask.org/en/stable/diagnostics-distributed.html) to find out how to group your work into fewer, more substantial tasks. This might mean that you call lazy operations at once instead of individually. This might also repartitioning your dataframe(s).\n",
    "- A good rule of thumb for choosing number of threads per Dask worker is to choose the square root of the number of cores per node. \n",
    "     - In general more threads per worker are good for a program that spends most of its time in NumPy, SciPy, Numba, etc., and fewer threads per worker are better for simpler programs that spend most of their time in the Python interpreter.\n",
    "- The Dask scheduler runs on a single thread, so assigning it its own node is a waste.\n",
    "- There is no hard limit on Dask scaling. The task overhead though will eventually start to swamp your calculation depending on how long each task takes to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Parallelize using Dask `Map_Partition`</font>\n",
    "\n",
    "We construct a Dask DataFrame from pandas dataframe using `from_pandas` function and specify the number of partitions (`nparitions`) to break this dataframe into.\n",
    "\n",
    "```python\n",
    "   dd = ddf.from_pandas(df, npartitions=N)\n",
    "```\n",
    "\n",
    "`ddf` is the name you imported Dask Dataframes with, and `npartitions` is an argument telling the Dataframe how you want to partition it.\n",
    "\n",
    "Each partition will run on a different thread, and communication between them will become too costly if there are too many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a Pandas DataFrame with 100000 rows and two columns with values selected randomly between 1 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100000\n",
    "df = pd.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                   'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break the DataFrame into 4 partitions (or the number of available cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply `add_squares` method on each of these partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['add_squares'] = df.apply(add_squares, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf['add_squares'] = ddf.map_partitions(add_squares, \n",
    "                               meta=(None, 'int64')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Numba and Dask</font>\n",
    "\n",
    "- Numba allows for run-time compilations of functions to optimize single-machine code.\n",
    "    - If you intend to call a function multiple times, you can decrease your compute time significantly by compliling the function on the first call. \n",
    "    - Numba is useful for speeding up individual tasks.\n",
    "- Dask is a parallel computing library for out-of-memory and distributed computations. \n",
    "    - At the heart of dask are a series of task schedulers — algorithms for determining when and how to run various user-defined computational “tasks”; consequently, dask can automatically identify which tasks can be run in parallel, or not run at all. \n",
    "    - Employing dask’s schedulers allows us to scale out to a network of many interrelated tasks and efficiently compute only those outputs we need, even on a single machine.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Example:</font> Manipulating Arrays\n",
    "\n",
    "- We use Numpy or Dask arrays to find the appromation of Pi.\n",
    "- We add Numba to find out if we can accelerate the calcultions.\n",
    "\n",
    "![pi](https://miro.medium.com/max/2400/0*Hc8vWedSy1mTOuw6)\n",
    "Image Source: [https://blog.esciencecenter.nl/parallel-programming-in-python-7fd62c90217d](https://blog.esciencecenter.nl/parallel-programming-in-python-7fd62c90217d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def py_approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_approximate_pi(num_samples):\n",
    "    pts = np.random.uniform(-1, 1, (2, num_samples))\n",
    "    \n",
    "    # Count number of impacts inside the circle\n",
    "    num_points_circ = np.count_nonzero((pts**2).sum(axis=0) < 1)\n",
    "\n",
    "    return 4 * num_points_circ / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, nogil=True)\n",
    "def nb_approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba and Dask Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "@nb.jit(nopython=True, nogil=True)\n",
    "def nbdk_approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to repeat the calculations several times and then take the verage value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_mean(*args):\n",
    "    return sum(args) / len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def dk_mean(*args):\n",
    "    return sum(args) / len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = 10**6\n",
    "number_experiments = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_py = %timeit -o reg_mean(*[py_approximate_pi(number_samples) for i in range(number_experiments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_np = %timeit -o reg_mean(*[np_approximate_pi(number_samples) for i in range(number_experiments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_nb = %timeit -o reg_mean(*[nb_approximate_pi(number_samples) for i in range(number_experiments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_nbdk = %timeit -o dk_mean(*[nbdk_approximate_pi(number_samples) for i in range(number_experiments)]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:         {}\".format(time_py.best/time_py.best))\n",
    "print(\"Numpy:          {}\".format(time_py.best/time_np.best))\n",
    "print(\"Numba:          {}\".format(time_py.best/time_nb.best))\n",
    "print(\"Numba and Dask: {}\".format(time_py.best/time_nbdk.best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Example:</font> Manipulating DataFrames\n",
    "\n",
    "We implment five tests to manipullate DataFrames in both Pandas and Dask:\n",
    "\n",
    "1. Pandas and `apply` method\n",
    "2. Pandas and Numba\n",
    "3. Dask and `map_partitions` & `apply` methods\n",
    "4. Dask, `map_partitions` & `apply` methods and Numba\n",
    "5. Dask and `assign` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dist(col1, col2, col3):\n",
    "    \"\"\"\n",
    "       Check if the distance is less that a value.\n",
    "    \"\"\"\n",
    "    dist = np.sqrt(col1**2 + col2**2)\n",
    "    return dist < col3\n",
    "\n",
    "@jit(nopython=True)\n",
    "def check_dist_fast(col1, col2, col3):\n",
    "    dist = np.sqrt(col1**2+col2**2)\n",
    "    return dist < col3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1000000\n",
    "num_cols = 3\n",
    "df = pd.DataFrame(np.random.random(size=(num_rows,num_cols))*100, \n",
    "                  columns=['col1','col2','col3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the corresponding Dask DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas and `apply` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "df['col4'] = df.apply(lambda x: check_dist(x.col1, x.col2, x.col3), \n",
    "                      axis=1)\n",
    "time_py = time.time()-t0\n",
    "print(\"Regular pandas took\", time_py)\n",
    "print('Number of True', df['col4'].sum())\n",
    "df = df.drop('col4',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas and Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "df['col4'] = check_dist_fast(df.col1.to_numpy(),\n",
    "                             df.col2.to_numpy(),df.col3.to_numpy())\n",
    "time_nbpd = time.time()-t0\n",
    "print(\"Numba pandas took\", time_nbpd)\n",
    "print('Number of True', df['col4'].sum())\n",
    "df = df.drop('col4',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask and `map_partitions` & `apply` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ddf['col4'] = ddf.map_partitions(lambda d: d.apply(\n",
    "    lambda x: check_dist(x.col1,x.col2,x.col3),axis=1)\n",
    "                                ).compute()\n",
    "\n",
    "time_dkpd = time.time()-t0\n",
    "print(\"Dask pandas took\", time_dkpd)\n",
    "print('Number of True', ddf['col4'].sum().compute())\n",
    "ddf = ddf.drop('col4',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask, `map_partitions` & `apply` methods and Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ddf['col4'] = ddf.map_partitions(lambda d: d.apply(lambda x:\n",
    "                    check_dist_fast(x.col1,x.col2,x.col3),axis=1)).compute()\n",
    "time_dkmap = time.time()-t0\n",
    "print(\"Numba Dask pandas took\", time_dkmap)\n",
    "print('Number of True', ddf['col4'].sum().compute())\n",
    "ddf = ddf.drop('col4',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask and  `assign` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ddf = ddf.assign(col4=lambda x: check_dist(x.col1, x.col2, x.col3))\n",
    "ddf.compute()\n",
    "time_dkassign = time.time()-t0\n",
    "print(\"Dask using Assign took\", time_dkassign)\n",
    "print('Number of True', ddf['col4'].sum().compute())\n",
    "ddf = ddf.drop('col4',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pandas/apply:               {time_py/time_py}\")\n",
    "print(f\"Pandas + Numba:             {time_py/time_nbpd}\")\n",
    "print(f\"Dask/map_partitions/apply:  {time_py/time_dkpd}\")\n",
    "print(f\"Dask/map_partition + Numba: {time_py/time_dkmap}\")\n",
    "print(f\"Dask/assign:                {time_py/time_dkassign}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
