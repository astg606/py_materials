{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "RAPIDS cuPy, cuDF and cuML\n",
    "</H1>\n",
    "</CENTER>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "- [Introduction to GPUs](https://nyu-cds.github.io/python-gpu/01-introduction/)\n",
    "- [RAPIDS: GPU DATA SCIENCE](https://rapids.ai/)\n",
    "- [RAPIDS Notebooks](https://github.com/rapidsai/notebooks)\n",
    "- [CuPy: A NumPy-compatible array library accelerated by CUDA](https://cupy.dev/)\n",
    "- [cuML - GPU Machine Learning Algorithms](https://github.com/rapidsai/cuml)\n",
    "- [Introduction to RAPIDS and GPU Data Science: CUDF/Dask vs. Pandas](https://code-love.com/2020/12/06/rapids-introduction/)\n",
    "- [Introduction to RAPIDS](https://github.com/rapidsai-community/notebooks-contrib/blob/branch-0.14/getting_started_notebooks/intro_tutorials/01_Introduction_to_RAPIDS.ipynb) by Paul Hendricks\n",
    "- [GPU Hackathons](https://www.gpuhackathons.org/technical-resources)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What will be Covered?</font>\n",
    "\n",
    "* Introduction to GPUs\n",
    "* What is RAPIDS?\n",
    "* CuPy\n",
    "* CuDF\n",
    "* CuML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">GPUs</font>\n",
    "\n",
    "\n",
    "- Graphics Processing Units (GPUs) are custom designed to be very efficient at handling computer graphics and image processing.\n",
    "- GPUs are designed to handle billions of repetitive low level tasks, like arithmetic operations.\n",
    "- Central Processing Units (CPUs) handle computations serially, meaning the logic in handled in one stream: the next task will complete when the subsequent task has finished. CPUs can execute tasks in parallel across cores. For example, most computer CPUs tend to have either two, four or six cores.\n",
    "- In comparison, GPUs have hundreds of 'cores'. This massively parallel architecture is what gives the GPU its high compute performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![CPU_GPU](https://nyu-cds.github.io/python-gpu/fig/01-cpugpuarch.png)\n",
    "Image Source: [nyu-cds.github.io](https://nyu-cds.github.io/python-gpu/fig/01-cpugpuarch.png)\n",
    "\n",
    "- Compared to the CPU, the GPU is specialized for compute-intensive, highly parallel computation - exactly what graphics rendering is about - and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control.\n",
    "- The GPU is especially well-suited to address problems that can be expressed as data-parallel computations - the same program is executed on many data elements in parallel - with a high ratio of arithmetic operations to memory operations.\n",
    "- Because the same program is executed for each data element, there is a lower requirement for sophisticated flow control, and because it is executed on many data elements and has high arithmetic intensity, the memory access latency can be hidden with calculations instead of big data caches.\n",
    "\n",
    "### Programming with GPUs\n",
    "\n",
    "- GPU programming is the use of a GPU together with a CPU to accelerate computation in applications traditionally handled only by the CPU.\n",
    "- Parallel computing platforms allow us to write codes to be executed on GPUs:\n",
    "   - **Nvidia’s CUDA** (Compute Unified Device Architecture): can be called within  C, C++, Fortran, or Python codes without any skills in graphics programming. \n",
    "   - **OpenCL**: The most popular open, royalty-free standard for cross-platform, parallel programming. OpenCL defines a C-like language for writing programs, but third-party APIs exist for other programming languages and platforms such as Python or Java.\n",
    "   - **OpenACC**: Designed for scientists and engineers interested in porting their codes to a wide-variety of heterogeneous HPC hardware platforms and architectures. It used by annotating C, C++, and Fortran source code to tell the GPU which areas that should be accelerated.\n",
    "- A GPU program comprises two parts: \n",
    "   - a host part that runs on the CPU, and \n",
    "   - one or more kernels that run on the GPU. \n",
    "- Typically, the CPU portion of the program is used to set up the parameters and data for the computation, while the kernel portion performs the actual computation. \n",
    "- In some cases the CPU portion may comprise a parallel program that performs message passing operations using MPI.\n",
    "\n",
    "![GPUs](http://www.nvidia.com/docs/IO/143716/how-gpu-acceleration-works.png)\n",
    "Image Source: NVIDIA\n",
    "\n",
    "\n",
    "**Useful Terminology**\n",
    "\n",
    "| Term | Meaning |\n",
    "| ---  | --- |\n",
    "| `host` | the CPU |\n",
    "| `device` | the GPU |\n",
    "| `host memory` | the system main memory |\n",
    "| `device memory` | onboard memory on a GPU card |\n",
    "| `kernels` | a GPU function launched by the host and executed on the device |\n",
    "| `device function` | a GPU function executed on the device which can only be called from the device  |\n",
    "\n",
    "## Using a GPU\n",
    "\n",
    "1. You must retarget code for the GPU\n",
    "2. The working set must fit in GPU RAM\n",
    "3. You must copy data to/from GPU RAM\n",
    "4. Data accesses should be streaming, or use scratchpad as user-managed cache\n",
    "5. Lots of parallelism preferred (throughput, not latency)\n",
    "6. SIMD-style parallelism best suited\n",
    "7. High arithmetic intensity (FLOPs/byte) preferred\n",
    "\n",
    "# <font color=\"red\"> What is RAPIDS?</font>\n",
    "\n",
    "> RAPIDS utilizes NVIDIA CUDA® primitives for low-level compute optimization, and exposes GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.\n",
    "\n",
    "- A suite of open-source software libraries for executing end-to-end data science and analytics pipelines entirely on GPUs. \n",
    "- Is designed to look and feel like Python. All RAPIDS libraries are based on Python and are designed to have Pandas and Sklearn like interfaces to facilitate adoption.\n",
    "- Accelerates data science pipelines to create more productive workflows.\n",
    "- Works with different machine learning algorithms to provide a faster processing speed without serialization costs. \n",
    "- The main componanets are:\n",
    "    - **cuDF**: used to perform data processing tasks (Pandas like).\n",
    "    - **cuML**: used to create Machine Learning models (Sklearn like).\n",
    "    - **cuGraph**: used to perform graphing tasks (Graph Theory).\n",
    "\n",
    "![rapids](https://pbs.twimg.com/media/D2CeyaYVAAAe3kM.jpg)\n",
    "Image Source: NVIDIA\n",
    "\n",
    "\n",
    "![scientisr](https://qph.fs.quoracdn.net/main-qimg-040086613ce3f56014c109a918ee8a4f)\n",
    "Image Source: NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the GPU on Google Colab\n",
    "\n",
    "In order to access GPUs for free:\n",
    "\n",
    "1. Go to the `Runtime` menu,\n",
    "2. Click on `Change runtime type`, and \n",
    "3. In the pop-up box, under `Hardware accelerator`, select `GPU` and click on `SAVE`.\n",
    "\n",
    "## Environment Sanity Check ##\n",
    "\n",
    "- <font color='red'>Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.</font>\n",
    "- Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that you were allocated the GPU compatible with RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "gpu_name = pynvml.nvmlDeviceGetName(handle).decode('UTF-8')\n",
    "\n",
    "if('K80' not in gpu_name):\n",
    "   print('***********************************************************************')\n",
    "   print('Woo! Your instance has the right kind of GPU, a '+ str(gpu_name)+'!')\n",
    "   print('***********************************************************************')\n",
    "   print()\n",
    "else:\n",
    "   raise Exception(\"\"\"\n",
    "                  Unfortunately Colab didn't give you a RAPIDS compatible GPU (P4, P100, T4, or V100), but gave you a \"\"\"+ gpu_name +\"\"\".\n",
    "                  Make sure you've configured Colab to request a GPU Instance Type.                \n",
    "                  If you get an incompatible GPU (i.e., a K80), use 'Runtime -> Factory Reset Runtimes...' to try again\"\"\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install RAPIDS\n",
    "\n",
    "- Install most recent Miniconda release compatible with Google Colab's Python install (3.7.10)\n",
    "- Removes incompatible files\n",
    "- Install RAPIDS' current stable version of its libraries, including:\n",
    "   - cuDF\n",
    "   - cuML\n",
    "   - cuGraph\n",
    "   - cuSpatial\n",
    "   - cuSignal\n",
    "   - xgboost\n",
    "- Set necessary environment variables\n",
    "- Copy RAPIDS .so files into current working directory, a workaround for conda/colab interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!bash rapidsai-csp-utils/colab/rapids-colab.sh stable\n",
    "\n",
    "import sys, os\n",
    "\n",
    "dist_package_index = sys.path.index('/usr/local/lib/python3.7/dist-packages')\n",
    "sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.7/site-packages'] + sys.path[dist_package_index:]\n",
    "sys.path\n",
    "exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">CuPy</font>\n",
    "\n",
    "- An open-source array library accelerated with NVIDIA CUDA. \n",
    "- Provides GPU accelerated computing with Python.\n",
    "- Is an implementation of NumPy-compatible multi-dimensional array on CUDA. In general, it can be used as a drop-in replacement. All you need to do is just replace Numpy with CuPy in your Python code.\n",
    "- Is supports [methods](https://docs.cupy.dev/en/stable/reference/comparison.html) such as indexing, data types, broadcasting, array manipulation routines (`reshape`, `concatenate`, etc.), array creation routines (`empty`, `ones`, `ones_like`, etc.).\n",
    "- Includes the following features for performance:\n",
    "     - User-defined elementwise CUDA kernels\n",
    "     - User-defined reduction CUDA kernels\n",
    "     - Fusing CUDA kernels to optimize user-defined calculation\n",
    "     - Customizable memory allocator and memory pool\n",
    "     - cuDNN utilities\n",
    "- Uses on-the-fly kernel synthesis:\n",
    "     - When a kernel call is required, it compiles a kernel code optimized for the shapes and dtypes of given arguments, sends it to the GPU device, and executes the kernel.\n",
    "     - The compiled code is cached to `$(HOME)/.cupy/kernel_cache` directory can be overwritten by setting the `CUPY_CACHE_DIR` environment variable). \n",
    "     - It may make things slower at the first kernel call, though this slow down will be resolved at the second execution. \n",
    "     - CuPy also caches the kernel code sent to GPU device within the process, which reduces the kernel transfer time on further calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `cupy.ndarray`\n",
    "\n",
    "- The `cupy.ndarray` class is in its core, which is a compatible GPU alternative of `numpy.ndarray`.\n",
    "- The main difference of `cupy.ndarray` from `numpy.ndarray` is\n",
    "that the content is allocated on the device memory.\n",
    "- The `cupy.ndarray` data is allocated on the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the array manipulations are also done in the way similar to NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3, 4, 5, 6])\n",
    "L2_cpu = np.linalg.norm(x_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate it on GPU with CuPy in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array([1, 2, 3, 4, 5, 6])\n",
    "L2_gpu = cp.linalg.norm(x_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CuPy implements many functions on `cupy.ndarray` objects.\n",
    "- Understanding NumPy might help utilizing most features of CuPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Device\n",
    "\n",
    "- CuPy has a concept of the current device, which is the default device on which the allocation, manipulation, calculation etc. of arrays are taken place. \n",
    "\n",
    "Suppose the ID of current device is 0. The following code allocates array contents on GPU 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_on_gpu0 = cp.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current device can be changed by `cupy.cuda.Device.use()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_on_gpu0 = cp.array([1, 2, 3, 4, 5, 6])\n",
    "cp.cuda.Device(1).use()\n",
    "x_on_gpu1 = cp.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you switch the current GPU temporarily, `with` statement comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(1):\n",
    "     x_on_gpu1 = cp.array([1, 2, 3, 4, 5, 6])\n",
    "x_on_gpu0 = cp.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most operations of CuPy is done on the current device. Be careful that if processing of an array on a non-current device will cause an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(0):\n",
    "     x_on_gpu0 = cp.array([1, 2, 3, 4, 5, 6])\n",
    "with cp.cuda.Device(1):\n",
    "     x_on_gpu0 * 2 # raises error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cupy.ndarray.device` attribute indicates the device on which the array is allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(1):\n",
    "     x = cp.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the environment has only one device, such explicit device switching is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transfer\n",
    "\n",
    "#### Move Data to a Device\n",
    "`cupy.asarray()` can be used to move a numpy.ndarray, a list, or any object that can be passed to `numpy.array()` to the current device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3, 4, 5, 6])\n",
    "x_gpu = cp.asarray(x_cpu) # move the data to the current device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cupy.asarray()` can accept `cupy.ndarray`, which means we can transfer the array between devices with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(0):\n",
    "     x_gpu_0 = cp.ndarray([1, 2, 3]) # create an array in GPU 0\n",
    "\n",
    "with cp.cuda.Device(1):\n",
    "     x_gpu_1 = cp.asarray(x_gpu_0) # move the array to GPU 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `cupy.asarray()` does not copy the input array if possible. So, if you put an array of the current device, it returns the input object itself.\n",
    "- If we do copy the array in this situation, you can use `cupy.array()` with `copy=True`. \n",
    "- Actually `cupy.asarray()` is equivalent to `cupy.array(arr, dtype, copy=False)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move array from a device to the host\n",
    "\n",
    "Moving a device array to the host can be done by `cupy.asnumpy()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array([1, 2, 3, 4, 5, 6]) # create an array in the current device\n",
    "x_cpu = cp.asnumpy(x_gpu) # move the array to the host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `cupy.ndarray.get()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = x_gpu.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to write CPU/GPU agnostic code\n",
    "\n",
    "- The compatibility of CuPy with NumPy enables us to write CPU/GPU generic code. \n",
    "- It can be made easy by the `cupy.get_array_module()` function. \n",
    "- This function returns the numpy or cupy module based on arguments.\n",
    "\n",
    "A CPU/GPU generic function is defined using it like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable implementation of log(1 + exp(x))\n",
    "def softplus(x):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.maximum(0, x) + xp.log1p(xp.exp(-abs(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes, an explicit conversion to a host or device array may be required. \n",
    "- `cupy.asarray()` and `cupy.asnumpy()` can be used in agnostic implementations to get host or device arrays from either CuPy or NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cpu = np.array([7, 8, 9, 10, 11, 12])\n",
    "x_cpu + y_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu + y_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.asnumpy(x_gpu) + y_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.asnumpy(x_gpu) + cp.asnumpy(y_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu + cp.asarray(y_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.asarray(x_gpu) + cp.asarray(y_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [cuDF](https://docs.rapids.ai/api/cudf/stable/10min.html)\n",
    "\n",
    "- CuDF is a library that aims to bring Pandas functionality to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `cudf` to create a dataframe and perform operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100000\n",
    "df = cudf.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                     'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['add_squares'] = add_squares(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Series Data**\n",
    "\n",
    "`DataFrames` supports `datetime` typed columns, which allow users to interact with and filter data based on specific timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = cudf.DataFrame()\n",
    "date_df['date'] = pd.date_range('01/05/1980', periods=15000, freq='D')\n",
    "date_df['value'] = cp.random.sample(len(date_df))\n",
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_date1 = dt.datetime.strptime('2001-09-11', '%Y-%m-%d')\n",
    "search_date2 = dt.datetime.strptime('2019-11-23', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_df.query('date >= @search_date1 and date <= @search_date2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "date_ddf = dask_cudf.from_cudf(date_df, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_ddf.query('date >= @search_date1 and date <= @search_date2', \n",
    "               local_dict={'search_date1':search_date1, \n",
    "                           'search_date2':search_date2}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed for Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://data.ny.gov/api/views/xe9x-a24f/rows.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading AERONET Observations at Goddard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/astg606/py_materials/master/aeronet/\"\n",
    "filename = url+\"19930101_20210102_GSFC.lev20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparse = lambda x: dt.datetime.strptime(x, '%d:%m:%Y %H:%M:%S')\n",
    "df = pd.read_csv(filename, skiprows=6, na_values=-999,\n",
    "                   parse_dates={'datetime': [0, 1]}, \n",
    "                   date_parser=dateparse, index_col=0, \n",
    "                   squeeze=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = cudf.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renaming Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_cols = ['Day_of_Year', 'AOD_675nm', 'AOD_440nm', \n",
    "            '440-675_Angstrom_Exponent']\n",
    "\n",
    "new_cols = ['DoY', 'A675', 'A440', 'Alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngdf = gdf[old_cols]\n",
    "ngdf.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngdf['A550'] = ngdf['A675']*(675.0/550.0)**ngdf['Alpha']\n",
    "ngdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>CuML</font>\n",
    "\n",
    "- The mathematical operations underlying many machine learning algorithms are often matrix multiplications. \n",
    "- These types of operations are highly parallelizable and can be greatly accelerated using a GPU. \n",
    "- cuML makes it easy to build machine learning models in an accelerated fashion while still using an interface nearly identical to Scikit-Learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "from cuml import make_regression, train_test_split\n",
    "from cuml.linear_model import LinearRegression as LinearRegression_GPU\n",
    "from cuml.metrics.regression import r2_score\n",
    "from sklearn.linear_model import LinearRegression as skLinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Example\n",
    "\n",
    "#### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the relationship: y = 2.0 * x + 1.0\n",
    "n_rows = 40000  # let's use 100 thousand data points\n",
    "w = 2.0\n",
    "x = np.random.normal(loc=0, scale=1, size=(n_rows,))\n",
    "b = 1.0\n",
    "y = w * x + b\n",
    "\n",
    "# add a bit of noise\n",
    "noise = np.random.normal(loc=0, scale=2, size=(n_rows,))\n",
    "y_noisy = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y_noisy, label='empirical data points')\n",
    "plt.plot(x, y, color='black', label='true relationship')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and fit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = skLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_regression.fit(np.expand_dims(x, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new data and perform inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.linspace(start=-5, stop=5, num=1000)\n",
    "outputs = linear_regression.predict(np.expand_dims(inputs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y_noisy, label='empirical data points')\n",
    "plt.plot(x, y, color='black', label='true relationship')\n",
    "plt.plot(inputs, outputs, color='red', \n",
    "         label='predicted relationship (cpu)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CuML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a cuDF DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.DataFrame({'x': x, 'y': y_noisy})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the GPU accelerated LinearRegression class from cuML, instantiate it, and fit it to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_gpu = LinearRegression_GPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_regression_gpu.fit(df[['x']], df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new data and perform inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_df = cudf.DataFrame({'inputs': inputs})\n",
    "outputs_gpu = linear_regression_gpu.predict(new_data_df[['inputs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y_noisy, label='empirical data points')\n",
    "plt.plot(x, y, color='black', label='true relationship')\n",
    "plt.plot(inputs, outputs, color='red', \n",
    "         label='predicted relationship (cpu)')\n",
    "plt.plot(inputs, outputs_gpu.to_array(), color='green', \n",
    "         label='predicted relationship (gpu)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters:\n",
    "+ If you are running on a GPU with less than 16GB RAM, please change to 2**19 or you could run out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2**20\n",
    "n_features = 399\n",
    "n_info = 70\n",
    "random_state = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data:\n",
    "+ Use the `make_regression` function (generating) a random regression problem) to create the dataset:\n",
    "   + `n_samples`: The number of samples (default=100)\n",
    "   + `n_features`: The number of features (default=2).\n",
    "   + `n_info`: The number of informative features, i.e., the number of features used to build the linear model used to generate the output (default=2).\n",
    "   + `random_state`: Seed for the random number generator for dataset creation.\n",
    "+ The function `make_regression` returns:\n",
    "   + The input samples: data array of shape `[n_samples, n_features]`\n",
    "   + The output values: data array of shape `[n_samples, 1]`\n",
    "+ Split the dat into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X, y = make_regression(n_samples=n_samples, \n",
    "                       n_features=n_features, \n",
    "                       n_informative = n_info,\n",
    "                       random_state=random_state)\n",
    "\n",
    "X = cudf.DataFrame(X)\n",
    "y = cudf.DataFrame(y)[0]\n",
    "\n",
    "X_cudf, X_cudf_test, y_cudf, y_cudf_test = train_test_split(X, y, \n",
    "                                                            test_size = 0.2, \n",
    "                                                            random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy dataset from GPU memory to host memory. This is done to later compare CPU and GPU results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_cudf.to_pandas()\n",
    "X_test = X_cudf_test.to_pandas()\n",
    "y_train = y_cudf.to_pandas()\n",
    "y_test = y_cudf_test.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ols_sk = skLinearRegression(fit_intercept=True,\n",
    "                            normalize=True,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "ols_sk.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predict_sk = ols_sk.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "r2_score_sk = r2_score(y_cudf_test, predict_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ols_cuml = LinearRegression_GPU(fit_intercept=True,\n",
    "                                normalize=True,\n",
    "                                algorithm='eig')\n",
    "\n",
    "ols_cuml.fit(X_cudf, y_cudf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predict_cuml = ols_cuml.predict(X_cudf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "r2_score_cuml = r2_score(y_cudf_test, predict_cuml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 score (SKL):  {r2_score_sk}\")\n",
    "print(f\"R^2 score (cuML): {r2_score_cuml}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
