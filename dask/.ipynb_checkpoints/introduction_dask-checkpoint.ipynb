{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P90R-v_Xr6Wd"
   },
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://github.com/astg606/py_materials/blob/master/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1> <font color=\"red\">Introduction to Dask</font></h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "  <head> </head>\n",
    "  <body>\n",
    "<script src=\"https://bot.voiceatlas.mysmce.com/v1/chatlas.js\"></script>\n",
    "<app-chatlas\n",
    "\tatlas-id=\"f759a188-f8bb-46bb-9046-3b1b961bd6aa\"\n",
    "\twidget-background-color=\"#3f51b5ff\"\n",
    "\twidget-text-color=\"#ffffffff\"\n",
    "\twidget-title=\"Chatlas\">\n",
    "</app-chatlas>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Reference Document</font>\n",
    "\n",
    "- <a href=\"https://docs.dask.org/en/latest/why.html\">Why Dask?</a>\n",
    "- <a href=\"https://github.com/dask/dask-tutorial\">dask-tutorial</a>\n",
    "- <a href=\"https://www.manning.com/books/data-science-with-python-and-dask\">Data Science with Python and Dask</a>\n",
    "- <a href=\"https://www.manifold.ai/dask-and-machine-learning-preprocessing-tutorial\">Dask and Machine Learning: Preprocessing Tutorial</a>\n",
    "- <a href=\"https://carpentries-incubator.github.io/lesson-parallel-python/aio/index.html\">Parallel Programming in Python</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=uGy5gT2vLdI&feature=youtu.be\"> Working with the Python DASK library (video)</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=t_GRK4L-bnw&feature=youtu.be\">Who uses Dask (video)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_dask](https://miro.medium.com/max/1000/1*D6mSsdWECFLn6wJne4VTjg.png)\n",
    "\n",
    "\n",
    "# <font color=\"red\"> What is Dask?</font>\n",
    "\n",
    "- A flexible library for parallel computing in Python that makes it easy to build intuitive workflows for ingesting and analyzing large, distributed datasets. \n",
    "- A native parallel analytics tool designed to integrate seamlessly with Numpy, Pandas, and Scikit-Learn. \n",
    "- An out-of-core (data is read into memory from disk on an as-needed basis) parallelization library that seamlessly integrates with existing NumPy and Pandas data structures to address the following:\n",
    "     * **The available dataset does not fit in memory of a single machine.**\n",
    "     * **The data processing task is time consuming and needs to be scaled and sped up.**\n",
    "- Orchestrates parallel threads or processes for us and help speed up processing times.\n",
    "   - Works by distributing larger computations and breaking them down into smaller computations through a task scheduler and task workers.\n",
    "\n",
    "Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler, low-level APIs, and high-level APIs.\n",
    "\n",
    "- Dask provides a few high-level constructs called Dask Bags, Dask DataFrames, and Dask Arrays. They provide an easy-to-use interface to parallelize many of the typical data transformations in Machine Learning (ML) workflows. \n",
    "- Dask allows the creation of highly customized job execution graphs by using their extensive Python API (e.g., `dask.delayed`) and integration with existing data structures.\n",
    "\n",
    "\n",
    "![fig_layers](http://bicortex.com/bicortex/wp-content/post_content//2019/06/Dask_APIs_Architecture.png)\n",
    "Image Source: bicortex.com\n",
    "\n",
    "\n",
    "The diagram below describes the steps Dask takes to manipulate data.\n",
    "\n",
    "- The operation is broken down into a sequence of operations on smaller partitions of our data (without having to read the whole dataset into memory).\n",
    "- Dask reads each partition as it is needed and computes the intermediate results. \n",
    "- The intermediate results are aggregated into the final result.\n",
    "- Dask handles all of that sequencing internally for us. \n",
    "- On a single machine, Dask can use threads or processors to parallelize these operations. \n",
    "\n",
    "![fig_proc](https://www.manifold.ai/hs-fs/hubfs/Blog%20Post%20Illos/ML%20pipelines%20-%20dask%20single%20machine.jpeg?width=600&name=ML%20pipelines%20-%20dask%20single%20machine.jpeg)\n",
    "Image Source: www.manifold.ai\n",
    "\n",
    "\n",
    "**Advantages of Using Dask**\n",
    "\n",
    "- Fully implemented in Python and natively scales NumPy, Pandas, and scikit-learn.\n",
    "- Can be used effectively to work with both medium datasets on a single machine and large datasets on a cluster.\n",
    "- Can be used as a general framework for parallelizing most Python objects.\n",
    "- Has a very low configuration and maintenance overhead.\n",
    "\n",
    "\n",
    "\n",
    ">Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don’t fit into main memory. Dask’s high-level collections are alternatives to NumPy and Pandas for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall on Processes and Threads**\n",
    "\n",
    "- A process is an execution of a program. \n",
    "- A thread is a single execution sequence within the process.\n",
    "- A process can contain multiple threads.\n",
    "- Threads are used for small tasks, whereas processes are used for more ‘heavyweight’ tasks. \n",
    "\n",
    "![threads](https://pediaa.com/wp-content/uploads/2018/07/Difference-Between-Process-and-Thread-Comparison-Summary-684x1024.jpg)\n",
    "Image Source: pediaa.com\n",
    "\n",
    "\n",
    "**The regular Python can only run one thread at the time.**\n",
    "\n",
    ">Dask offers an easy and consistent way to parallelize computations that scales from a single laptop to clusters with thousands of cores. It's based on a task scheduler that distributes Python function calls across multiple threads, processes or cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules\n",
    "\n",
    "Uncomment the next two cells if in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install dask[dataframe] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlWFzeaGvPmc"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uktcOPYqt8Cn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy version:  {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Dask   version: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "import memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We may want to first determine the system information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def convert_size(size):\n",
    "    \"\"\"\n",
    "      Convert from KB to another unit.\n",
    "    \"\"\"\n",
    "    if (size == 0):\n",
    "       return '0B'\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size,1024)))\n",
    "    p = math.pow(1024,i)\n",
    "    s = round(size/p,2)\n",
    "    return \" \".join([str(s),size_name[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(\"=\"*20, \"System Information\", \"=\"*20)\n",
    "uname = platform.uname()\n",
    "print(f\"           System: {uname.system}\")\n",
    "print(f\"        Node Name: {uname.node}\")\n",
    "print(f\"          Release: {uname.release}\")\n",
    "print(f\"          Version: {uname.version}\")\n",
    "print(f\"          Machine: {uname.machine}\")\n",
    "print(f\"        Processor: {uname.processor}\")\n",
    "print(\"=\"*20, \"CPU Information\", \"=\"*20)\n",
    "cpufreq = psutil.cpu_freq()\n",
    "print(\"# logical cores = # physical cores times # threads \")\n",
    "print(\"                    that can run on each physical core.\")\n",
    "print(f\"   Physical cores: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"    Logical cores: {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"Current frequency: {psutil.cpu_freq().current}\")\n",
    "print(f\"    Min frequency: {psutil.cpu_freq().min}\")\n",
    "print(f\"    Max frequency: {psutil.cpu_freq().max}\")\n",
    "print(\"=\"*20, \"Memory Information\", \"=\"*20)\n",
    "svmem = psutil.virtual_memory()\n",
    "print(f\"     Total memory: {convert_size(svmem.total)}\")\n",
    "print(f\" Available memory: {convert_size(svmem.available)}\")\n",
    "svmem = psutil.virtual_memory()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Progress Bar\n",
    "\n",
    "- You can use Dask’s built-in Progress Bar he progress on any `get()` or `compute()` calls. \n",
    "- Here we will use the global registration where the Progress Bar will be displayed for all computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Parallelize Code with `dask.delayed`</font>\n",
    "\n",
    "- A simple way to parallelize the code.\n",
    "- Allows users to delay function calls into a task graph with dependencies.\n",
    "- Systems like `dask.dataframe` are built with `dask.delayed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Example**\n",
    "\n",
    "Consider the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def increment(x):\n",
    "    time.sleep(1.0)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(1.0)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(1.0)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = increment(1)\n",
    "y = increment(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the `dask.delayed` decorator to parallelize the functions `increment` and `add`.\n",
    "- By decorating the functions, we record what we want to compute as tasks into graphs that will be run later on parallel hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = dask.delayed(increment)(1)\n",
    "yd = dask.delayed(increment)(2)\n",
    "zd = dask.delayed(add)(xd, yd)\n",
    "zd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we call the delayed version by passing the arguments, exactly as before, but the original function isn't actually called yet.\n",
    "- A delayed object is made, which keeps track of the function to call and the arguments to pass to it.\n",
    "- We use the `visualize` method (relies on the `graphviz` package) that provide a visual representation of the operations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zd.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we have not physically calculated **total** yet.\n",
    "- We need to apply the `compute` method to get the answer. \n",
    "- <font color=\"red\">It is only here that the data are loaded into memory for calculations</font>.\n",
    "- The calculations are done through using a local thread pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(zd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `delayed` in Loops**\n",
    "\n",
    "Consider the sequential code with two for-loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = list()\n",
    "for x in data:\n",
    "    y = increment(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parallelize the above using the `delayed` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = list()\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)\n",
    "\n",
    "totald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the visual representation through a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totald.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(totald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use the `delayed` decorator to parallelize the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_odd(x):\n",
    "    return x%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "results = list()\n",
    "\n",
    "for x in data:\n",
    "    if is_odd(x):\n",
    "        y = double(x)\n",
    "    else:\n",
    "        y = increment(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<p>\n",
    "\n",
    "<details><summary><b>Click here to access the solution</b></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "results = list()\n",
    "\n",
    "for x in data:\n",
    "    if is_odd(x):\n",
    "        y = dask.delayed(double)(x)\n",
    "    else:\n",
    "        y = dask.delayed(increment)(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = dask.delayed(sum)(results)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Palindromic Words\n",
    "\n",
    "- A palindromic word is a word which characters read the same backward as forward. \n",
    "- Some examples of palindromes are `redivider`, `deified`, `civic`, `radar`, `level`, `rotor`, `kayak`, `reviver`, `racecar`, `madam`, and `refer`.\n",
    "\n",
    "We want to find the number of palindromes from a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_palindrome(s):\n",
    "    return s.upper() == s.upper()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = [\n",
    "    'complete', 'abstraction', 'from', 'compass', 'sights', 'sounds', \n",
    "    'Human', 'shapes', 'interferences', 'troubles', 'joys', 'were', \n",
    "    'they', 'were', 'there', \"man\", 'seemed', 'shaded', 'hemisphere', \n",
    "    'globe', 'sentient', 'being', 'save', 'himself', \"rather\", \n",
    "    \"Abba\", \"Aibohphobia\", \"Bib\", \"Bob\", \"Civic\", \"Deified\", \n",
    "    \"Detartrated\", \"Dewed\", \"Eve\", \"Hannah\", \"Kayak\", \"Level\", \n",
    "    \"Madam\", \"Malayalam\", \"Minim\", \"Mom\", \"Murdrum\", \"Noon\", \"Nun\", \n",
    "    \"Otto\", \"Peep\", \"Pop\", \"Racecar\", \"Radar\", \"Redder\", \"Refer\",\n",
    "    \"Repaper\", \"Rotator\", \"Rotavator\", \"Rotor\", \"Sagas\", \n",
    "    \"Sis\", \"Solo\", \"Stats\", \"Tattarrattat\", \"Tenet\",\n",
    "    'redivider', 'deified', 'civic', 'radar', 'level',\n",
    "    'Being', 'not', 'without', 'frequent', 'consciousness',\n",
    "    'that', 'there', 'was', 'some', 'charm', 'this', 'life', 'stood',\n",
    "    'still', 'after', 'looking', 'sky', 'useful', 'instrument',\n",
    "    'regarded', 'appreciative', 'spirit', 'work', 'art',\n",
    "    'superlatively', 'beautiful', 'moment', 'seemed',\n",
    "    'impressed', 'with', 'speaking', 'loneliness', 'scene',\n",
    "    \"brother\", \"system\", \"SISteR\", \"TEXT\", \"paREnts\", \"python\",\n",
    "    \"Numpy\", \"Dask\", \"PanDaS\"\n",
    "] \n",
    "\n",
    "len(list_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Regular Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "palindromes_py = [is_palindrome(s) for s in list_words]\n",
    "total_py = sum(palindromes_py)\n",
    "total_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palindromes_da = [dask.delayed(is_palindrome)(s) for s in list_words]\n",
    "total_da = dask.delayed(sum)(palindromes_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_da.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = total_da.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use Dask Bag, we will do the same computations faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bag = db.from_sequence(list_words)\n",
    "bag.map(is_palindrome).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "result= sum(bag.map(is_palindrome).compute())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Important Lessons</font>**\n",
    "\n",
    "- The `delayed` decorator adds overhead.\n",
    "- It is good not to use it when a task requires a little amount of time.\n",
    "- Call `delayed` on the function not the result.\n",
    "- Break up computations into many pieces. You achieve parallelism by having many delayed calls, not by using only a single one: Dask will not look inside a function decorated with `delayed` and parallelize that code internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use Dask to parallelize the code below (calculations of `pi`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "def approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples\n",
    "\n",
    "def mean(*args):\n",
    "    return sum(args) / len(args)\n",
    "\n",
    "number_samples = 10**6\n",
    "number_experiments = 10\n",
    "\n",
    "pi_approx = mean(*[approximate_pi(number_samples) for i in range(number_experiments)])\n",
    "\n",
    "print(\"Approximation of Pi: {}\".format(pi_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<p>\n",
    "<details><summary><b>Click here to access the solution</b></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "number_samples = 10**6\n",
    "number_experiments = 10\n",
    "\n",
    "pi_approx = dask.delayed(mean)(*[dask.delayed(approximate_pi)(number_samples) for i in range(number_experiments)])\n",
    "\n",
    "print(\"Approximation of Pi: {}\".format(pi_approx.compute()))\n",
    "```\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Dask Array</font>\n",
    "\n",
    "- Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid. \n",
    "    - _Parallel_: Uses all of the cores on your computer\n",
    "    - _Larger-than-memory_: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "    - _Blocked Algorithms_: Perform large computations by performing many smaller computations\n",
    "- They support a large subset of the Numpy API.\n",
    "\n",
    "![fig_array](https://miro.medium.com/max/1388/1*JfQnXJ5_R104bPyE8_XhwQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Dask Array**\n",
    "\n",
    "- Create a 20000x20000 array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). \n",
    "- There are 400 (20x20) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((10000, 40000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array:\n",
    "- Has 2.98 Gb\n",
    "- Is organized in 400 chunks of `1000x1000` Numpy arrays.\n",
    "- Each chunk has 7.64 Mb\n",
    "\n",
    "Similar information can be obtained from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"     Type: {type(x)}\")\n",
    "print(f\"    Shape: {x.shape}\")\n",
    "print(f\"     Size: {x.size}\")\n",
    "print(f\"Num bytes: {x.nbytes} B or {convert_size(x.nbytes)}\")\n",
    "print(f\"   Chunks: {x.chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Numpy syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2.0 + x.T\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x.mean(axis=0)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **`compute()`** function if you want your result as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = z.compute()\n",
    "print(type(w), w.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Persit Data in Memory**\n",
    "\n",
    "- If you have the available RAM for your dataset then you can persist data in memory.\n",
    "- This allows future computations to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy against Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_numpy():\n",
    "    x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "    y = x.mean(axis=0)[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%%memit` \n",
    "\n",
    "- Measures the memory use of a single statement.\n",
    "- Provides the peak memory and incremental memory growth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "f_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dask():\n",
    "    x = da.random.normal(10, 0.1, size=(20000, 20000), \n",
    "                         chunks=(1000, 1000))\n",
    "    y = x.mean(axis=0)[::100].compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "f_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshapping the chunk size might provide a better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dask2():\n",
    "    x = da.random.normal(10, 0.1, size=(20000, 20000), \n",
    "                         chunks=(2000, 500))\n",
    "    y = x.mean(axis=0)[::100].compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_dask2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask finished faster, but used more total CPU time because Dask was able to transparently parallelize the computation because of the chunk size.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Things to Consider</font>**\n",
    "\n",
    "- If your data fits in RAM and you are not performance bound, then using NumPy might be the right choice. Dask adds another layer of complexity which may get in the way.\n",
    "- **If you are just looking for speedups rather than scalability then you may want to consider using Numba for manipulating Numpy arrays.**\n",
    "- How to select the chunk size?\n",
    "     - Too small: huge overheads.\n",
    "     - Poorly aligned with data: inefficient reading.\n",
    "     - Recommended to have a chuck size of at least 100 Mb.\n",
    "     - Choose a chunk size that is large in order to reduce the number of chunks that Dask has to think about (which affects overhead) but also small enough so that many of them can fit in memory at once. Dask will often have as many chunks in memory as twice the number of active threads.\n",
    "   \n",
    "\n",
    "**Avoid Oversubscribing Threads**\n",
    "     \n",
    "- By default Dask will run as many concurrent tasks as you have logical cores. \n",
    "- It assumes that each task will consume about one core.\n",
    "- Many array-computing libraries (used in Dask) are themselves multi-threaded, which can cause contention and low performance.\n",
    "- For better performance, we need to explicitly specify the use of one thread:\n",
    "\n",
    "```bash\n",
    "   export OMP_NUM_THREADS=1\n",
    "   export MKL_NUM_THREADS=1\n",
    "   export OPENBLAS_NUM_THREADS=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Memory Profiling</font>\n",
    "\n",
    "- We use the `memory_profiler` package to track memory usage.\n",
    "- It's written totally in python and monitors process which is running python code as well as line by line memory usage by code. \n",
    "- We use the `memory_usage()` and pass the parameter `interval` for the frequency of measuring the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_with_numpy():\n",
    "    # Serial implementation\n",
    "    np.arange(10**8).sum()\n",
    "\n",
    "def sum_with_dask():\n",
    "    # Parallel implementation\n",
    "    work = da.arange(10**8).sum()\n",
    "    work.compute()\n",
    "\n",
    "memory_numpy = memory_usage(sum_with_numpy, interval=0.01)\n",
    "memory_dask = memory_usage(sum_with_dask, interval=0.01)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(memory_numpy, label='numpy')\n",
    "plt.plot(memory_dask, label='dask')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Memory / MB')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also use Dask profiling options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import Profiler, ResourceProfiler\n",
    "work = da.arange(10**8).sum()\n",
    "with Profiler() as prof, ResourceProfiler(dt=0.001) as rprof:\n",
    "    result2 = work.compute()\n",
    "\n",
    "from bokeh.plotting import output_notebook\n",
    "from dask.diagnostics import visualize\n",
    "visualize([prof,rprof], output_notebook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ResourceProfiler(dt=0.001) as rprof2:\n",
    "    result = np.arange(10**8).sum()\n",
    "visualize([rprof2], output_notebook())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Dask DataFrames</font>\n",
    "\n",
    "- Pandas is great for tabular datasets that fit in memory. \n",
    "- Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. \n",
    "- Dask DataFrames:\n",
    "     - Coordinate many Pandas DataFrames, partitioned along an index. \n",
    "     - Support a large subset of the Pandas API.\n",
    "- One operation on a Dask DataFrame triggers many Pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints.\n",
    "- Some of the operations that are really fast if you use Dask Dataframes:\n",
    "     - Arithmetic operations (multiplying or adding to a Series)\n",
    "     - Common aggregations (`mean`, `min`, `max`, `sum`, etc.)\n",
    "     - Calling `apply`\n",
    "     - Calling `value_counts()`, `drop_duplicates()` or `corr()`\n",
    "     - Filtering with `loc`, `isin`, and row-wise selection\n",
    "\n",
    "![fig_df](https://pythondata.com/wp-content/uploads/2016/11/Screen-Shot-2016-11-24-at-6.52.24-PM-168x300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> NYC Flights Dataset</font>\n",
    "\n",
    "Data is specific to flights (in 1990's) out of the three airports in the New York City area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the remote data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "print(\"\\t Downloading NYC dataset...\", end=\"\\n\", flush=True)\n",
    "\n",
    "url = \"https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\"\n",
    "filename, header = urllib.request.urlretrieve(url, \"nycflights.tar.gz\")\n",
    "\n",
    "print(\"\\t Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `.csv` files from the tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(filename, mode=\"r:gz\") as flights:\n",
    "     flights.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt data/nycflights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the files at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The representation of the dataframe object contains no data. \n",
    "- `pandas.read_csv` reads in the entire file before inferring datatypes.\n",
    "- `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file). These inferred datatypes are then enforced when reading all partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the last few rows, we have a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an issue with the data types of few columns.\n",
    "- The datatypes inferred in the sample are incorrect.\n",
    "- We can fix it by reading the files again and specify the appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]},\n",
    "                dtype={'TailNum': str,\n",
    "                       'CRSElapsedTime': float,\n",
    "                       'Cancelled': bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Perform Operations as with `Pandas DataFrames`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum value of a column**:\n",
    "\n",
    "- We now want to compute the maximum of the `DepDelay` column.\n",
    "- With `Pandas`, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\n",
    "- `dask.dataframe` allows us to write pandas-like code that operates on large than memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do the same thing in `Pandas`, we will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import glob\n",
    "\n",
    "list_files = glob.glob(\"data/nycflights/*csv\")\n",
    "   \n",
    "maxes = list()\n",
    "for file_name in list_files:\n",
    "    pddf = pd.read_csv(file_name)\n",
    "    maxes.append(pddf.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "\n",
    "print(\"Final Maximum: \", max(maxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Dest == 'PIT'].compute().plot(kind='scatter', \n",
    "                                    x=\"DayOfWeek\", \n",
    "                                    y=\"DepDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights were taken from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average departure delay from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"DayOfWeek\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by destinations and count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\").count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\")[\"ArrDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ArrDelay+df.DepDelay>30.0].groupby(\"Dest\").Dest.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharing Intermediate Results**\n",
    "\n",
    "- We sometimes do the same operation more than once. \n",
    "- For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass both to a single `compute` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = da.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "- Consider the code below that computes the mean departure delay per airport. \n",
    "- Parallelize the code using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "sum_delays = list()\n",
    "count_delays = list()\n",
    "\n",
    "for file_name in list_files:\n",
    "    pddf = pd.read_csv(file_name)\n",
    "    by_origin = pddf.groupby('Origin')\n",
    "    loc_total = by_origin.DepDelay.sum()\n",
    "    loc_count = by_origin.DepDelay.count()\n",
    "    sum_delays.append(loc_total)\n",
    "    count_delays.append(loc_count)\n",
    "\n",
    "total_delays = sum(sum_delays)\n",
    "n_flights = sum(count_delays)\n",
    "mean_delays = total_delays / n_flights\n",
    "print(\"Mean delays: {}\".format(mean_delays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<p>\n",
    "\n",
    "<details><summary><b>Click here to access the solution</b></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "%%time \n",
    "\n",
    "df.groupby(\"Origin\")[\"DepDelay\"].mean().compute()\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Example of Machine Learning with Dask</font>\n",
    "\n",
    "Grab columns from the Dask DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[[\"CRSDepTime\", \"CRSArrTime\", \"Cancelled\"]]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can query the shape (note delayed # of sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = len(df_train.columns)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic EDA**\n",
    "\n",
    "We can get descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform searches and operations on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20, \n",
    "                       input_dim=num_cols, \n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1,  activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**\n",
    "\n",
    "Generate batches of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(df, fraction=0.01):\n",
    "    while True:\n",
    "          batch = df.sample(frac=fraction)\n",
    "          X = batch.iloc[:, :-1]\n",
    "          y = batch.iloc[:, -1]\n",
    "          yield (X.compute(), y.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We never run of memory while doing the training:\n",
    "\n",
    "```\n",
    "   steps_per_epoch * batch_size = number_of_rows_in_train_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=batch_data_generator(df_train),\n",
    "                    steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Task Schedulers</font>\n",
    "\n",
    "- After Dask generates the task graphs, it needs to execute them on parallel hardware. \n",
    "- It is the role of a task scheduler. \n",
    "- There are different task schedulers. Each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "\n",
    "![schedulers](https://docs.dask.org/en/latest/_images/dask-overview.svg)\n",
    "\n",
    "Image Source: [https://docs.dask.org/en/latest/](https://docs.dask.org/en/latest/)\n",
    "\n",
    "\n",
    "Dask networks are composed of three pieces:\n",
    "- **Centralized scheduler**: Manages workers and assigns the tasks that need to be completed by them.\n",
    "- **Workers**: Are threads, processes, or separate machines in a cluster. They execute the computations from the computation graph: do the calculations, hold onto results, and communicate results to each other.\n",
    "- **One or multiple clients**: interact (Jupyter noteboooks or scripts) with users and submit work to the scheduler for execution on the workers.\n",
    "\n",
    "\n",
    "![networks](https://miro.medium.com/max/700/0*9JHQAjTVoKbm2f4X.png)\n",
    "Image Source: [Steven Gon](https://gongster.medium.com/dask-an-introduction-and-tutorial-b42f901bcff5)\n",
    "\n",
    "To execute the task graphs there are two types of schedulers:\n",
    "* **Single machine**: Provides basic features on a local process or thread pool. It is simple and cheap to use, although it can only be used on a single machine and does not scale\n",
    "* **Distributed**: Offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Single Machine Scheduler</font>\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = list()\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single thread**\n",
    "\n",
    "- The single-threaded synchronous scheduler executes all computations in the local thread with no parallelism at all.\n",
    "- It is useful for debugging or profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local threads**\n",
    "\n",
    "Uses `multiprocessing.pool.ThreadPool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to use a single thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='single-threaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local processes**\n",
    "\n",
    "- The multiprocessing scheduler executes computations with a local `multiprocessing.Pool`.\n",
    "- Every task and all of its dependencies are shipped to a local process, executed, and then their result is shipped back to the main process. \n",
    "- Moving data to remote processes and back can introduce performance penalties, particularly when the data being transferred between processes is large. \n",
    "- The multiprocessing scheduler is an excellent choice when workflows are relatively linear, and so does not involve significant inter-task data transfer as well as when inputs and outputs are both small, like filenames and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print (multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes', num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = totald.compute(scheduler='processes', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads or Processes?\n",
    "\n",
    "- **Use the threaded scheduler** if your computation is dominated by non-Python code, as is primarily the case when operating on numeric data in NumPy arrays, Pandas DataFrames, or using any of the other C/C++/Cython based projects in the ecosystem.\n",
    "   - It is lightweight.\n",
    "   - Little overhead.\n",
    "   - Tranferring data between tasks not expensives because everything happens in the same process.\n",
    "- **Use the multipeocessing scheduler** if your computation is dominated by processing pure Python objects like strings, dictionaries, or lists.\n",
    "   - It is lightweight.\n",
    "   - Every task and all of its dependencies are shipped to a local process, executed, and then their result is shipped back to the main process.\n",
    "   - Moving data to remote processes and back can introduce performance penalties, particularly when the data being transferred between processes is large. \n",
    "   - Is an excellent choice when workflows are relatively linear, and so does not involve significant inter-task data transfer as well as when inputs and outputs are both small, like filenames and counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Distributed Scheduler</font>\n",
    "\n",
    "- The Dask distributed scheduler can either be setup on a cluster or run locally on a personal machine. \n",
    "- It is a centrally managed, distributed, dynamic task scheduler. \n",
    "     - The central dask-scheduler process coordinates the actions of several dask-worker processes spread across multiple machines and the concurrent requests of several clients.\n",
    "     - The scheduler is asynchronous and event-driven, simultaneously responding to requests for computation from multiple clients and tracking the progress of multiple workers.\n",
    "     - The event-driven and asynchronous nature makes it flexible to concurrently handle a variety of workloads coming from multiple users at the same time while also handling a fluid worker population with failures and additions. \n",
    "     - Workers communicate amongst each other for bulk data transfer over TCP.\n",
    "- To set up `dask.distributed`, we need to create a client instance by calling `Client` class from `dask.distributed`. \n",
    "- It will internally create a dask scheduler and dask workers. \n",
    "- We will get the **link of the dashboard** where we can analyze tasks running in parallel. \n",
    "- We can pass a number of workers (using the `n_workers` argument) and threads to use per worker process (using the `threads_per_worker` argument).\n",
    "- As soon as you create a client, Dask will automatically start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client = Client(n_workers=3, threads_per_worker=4)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren’t in jupyterlab and using the `dask-labextension`, you can  click the `Dashboard` link to open up the diagnostics dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_slow_add(x, y):\n",
    "    time.sleep(random.randrange(3,10))\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "\n",
    "for x in data:\n",
    "    y = dask.delayed(random_slow_add)(x, 1)\n",
    "    results.append(y)\n",
    "    \n",
    "total = dask.delayed(sum)(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Things to Consider</font>**\n",
    "\n",
    "- Each Dask task has overhead (about 1 ms). If you have a lot tasks this overhead can add up. It is a good idea to give each task more than a few seconds of work.\n",
    "- To better understand how your program is performing, check the [Dask Performance Diagnostics](https://distributed.dask.org/en/latest/diagnosing-performance.html) documentation. You can also view the [video](https://docs.dask.org/en/stable/diagnostics-distributed.html) to find out how to group your work into fewer, more substantial tasks. This might mean that you call lazy operations at once instead of individually. This might also repartitioning your dataframe(s).\n",
    "- A good rule of thumb for choosing number of threads per Dask worker is to choose the square root of the number of cores per node. \n",
    "     - In general more threads per worker are good for a program that spends most of its time in NumPy, SciPy, Numba, etc., and fewer threads per worker are better for simpler programs that spend most of their time in the Python interpreter.\n",
    "- The Dask scheduler runs on a single thread, so assigning it its own node is a waste.\n",
    "- There is no hard limit on Dask scaling. The task overhead though will eventually start to swamp your calculation depending on how long each task takes to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Example with DataFrame</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Pandas DataFrame with 100000 rows and two columns with values selected randomly between 1 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100000\n",
    "df = pd.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                   'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that computes the sum of square for each column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time it takes to call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df['add_squares'] = df.apply(add_squares,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Parallelize using Dask `Map_Partition`</font>\n",
    "\n",
    "We construct a Dask DataFrame from pandas dataframe using `from_pandas` function and specify the number of partitions (`nparitions`) to break this dataframe into.\n",
    "\n",
    "```python\n",
    "   dd = ddf.from_pandas(df, npartitions=N)\n",
    "```\n",
    "\n",
    "`ddf` is the name you imported Dask Dataframes with, and `npartitions` is an argument telling the Dataframe how you want to partition it.\n",
    "\n",
    "Each partition will run on a different thread, and communication between them will become too costly if there are too many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will break into 4 partitions (number of available cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply `add_squares` method on each of these partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf['z'] = ddf.map_partitions(add_squares, \n",
    "                               meta=(None, 'int64')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x, y):\n",
    "    return y * (x**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df1 = df.apply(lambda row: myfunc(row.X, row.Y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "ddf = dd.from_pandas(df, npartitions=4*multiprocessing.cpu_count())\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddfz = ddf.map_partitions(lambda data: \n",
    "                              data.apply(lambda row: myfunc(row.X, row.Y), axis=1)).compute(scheduler='processes')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adv_viz.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
