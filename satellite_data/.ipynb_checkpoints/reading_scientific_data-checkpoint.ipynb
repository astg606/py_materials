{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://github.com/astg606/py_materials/blob/master/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1><font color=\"red\" size=\"+3\">Reading Scientific Data Format Files</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Useful References </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <A HREF=\"http://pyhogs.github.io/intro_netcdf4.html\">Create and read netCDF files</A>\n",
    "* <A HREF=\"https://unidata.github.io/netcdf4-python/\">netCDF4 module</A>\n",
    "* <a href=\"https://annefou.github.io/metos_python/07-LargeFiles/\">Handling very large files in Python</a>\n",
    "* <A HREF=\"https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/\">How to use HDF5 files in Python</A>\n",
    "* <a href=\"https://confluence.slac.stanford.edu/pages/viewpage.action?pageId=99485805\">How to access HDF5 data from Python</a>\n",
    "* [Welcome to pyhdfâ€™s documentation!](http://fhs.github.io/pyhdf/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Scientific Data</font>\n",
    "\n",
    "- Store a variety of data types that include singlepoint observations, time series, regularly spaced grids, and satellite or radar images.\n",
    "- Include metadata.\n",
    "- Measurements at specific time, location, condition\n",
    "   - Physics: temperature, pressure\n",
    "   - Chemistry: reaction speed\n",
    "   - Biology: type (species, cell types, nucleotides)\n",
    "   - Economics: price\n",
    "   - Algorithmics: program time and space\n",
    "   - Networking: network activity\n",
    "   - Robotics: movements\n",
    "     \n",
    "### Requirements\n",
    "\n",
    "+ **Compact storage**: compression\n",
    "+ **Fast I/O**: parallel, partial, random access\n",
    "+ **Portability**: transporting data between computers\n",
    "+ **Tools for manipulating data**: reorganizing, aggregating, subsetting, converting,visualizing\n",
    "+ **Easy API in many languages**: C, C++, Fortran, Java, Matlab, Perl, Python, R, ...\n",
    "\n",
    "We need to use four guiding principles (known as the [FAIR Principles](https://www.nature.com/articles/sdata201618)) for the proper creation, storage and manipulation of scientific data:\n",
    "\n",
    "1. Data must be **F**indable\n",
    "2. Data must be **A**ccessible\n",
    "3. Data must be **I**nteroperable\n",
    "4. Data must be **R**eusable.\n",
    "\n",
    "\n",
    "> In order for the scientific community to get the most value out of the available data, it is vital that storage formats are optimal for sharing, archiving and reuse. Adequate description of the data (stored in the form of metadata) is also key for turning data into information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Data Formats of Interest </font>\n",
    "\n",
    "+ **Network Common Data Format** (netCDF)\n",
    "+ **Hierarchical Data Format** (HDF)\n",
    "  - HDF4\n",
    "  - HDF5\n",
    "  \n",
    "We will learn how to access data in files using the above formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> netCDF</font>\n",
    "### <font color='blue'> What is netCDF?</font>\n",
    "\n",
    "#### Overview\n",
    "\n",
    "* NetCDF, is an interface to a library of data access functions for storing and retrieving data in the form of arrays.\n",
    "* NetCDF is an abstraction that supports a view of data as a collection of self-describing, portable objects that can be accessed through a simple interface.\n",
    "* All operations to access and manipulate data in a netCDF dataset must use only the set of functions provided by the interface.\n",
    "* Array values may be accessed directly, without knowing details of how the data are stored.\n",
    "* NetCDF supports efficient access to small subsets of large datasets.\n",
    "* Stores data in an array-oriented dataset which contains dimensions, variables, and attributes.\n",
    "* The dataset file is divided into two parts: \n",
    "   - The file header contains all information (metadata) about dimensions, attributes, and variables except for the variable data itself.\n",
    "   - The array data section contains arrays of variable values (raw data).\n",
    "\n",
    "#### Features\n",
    "\n",
    "- Self-contained, platform independent, binary\n",
    "- Dimensions\n",
    "   - Contain name and size\n",
    "   - Only one size unlimited (dataset dimension)\n",
    "   - Measurands e.g. time, latitude, longitude, etc.\n",
    "- Variables\n",
    "   - Array of values with same type\n",
    "   - Contain name, datatype, shape\n",
    "   - Coordinate variable: one dimensional variable with same name as dimension\n",
    "- Attributes\n",
    "   - Metadata\n",
    "   - Used for variables and file (global attributes)\n",
    "- Conventions\n",
    "   - Standards for specific use case\n",
    "      - * The names of variables and dimensions should be meaningful and conform to any relevant conventions.\n",
    "      - Attribute settings need to follow relevant conventions.\n",
    "   - Compare files from different sources.\n",
    "   - e.g. Climate and Forecast (CF), Cooperative Ocean/Atmosphere Research Data Service (COARDS)\n",
    "\n",
    "\n",
    "#### Portability\n",
    "\n",
    "* The netCDF library is supported for various Linux/UNIX operating systems as well as MS Windows.\n",
    "* APIs written for Fortran 77/90, C, C++, Java, etc.\n",
    "\n",
    "\n",
    "### <font color='blue'> What is netCDF4 Python?</font>\n",
    "\n",
    "* Python interface to the netCDF version 4 library.\n",
    "* **Can read and write files in both the new netCDF 4 and the netCDF 3 formats**.\n",
    "* Can create files that are readable by HDF5 utilities.\n",
    "* Relies on NumPy arrays.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"/Users/jkouatch/myTasks/PythonTraining/ASTG606/Materials/sat_data/VIIRS_Data/\"\n",
    "data_dir = \"/tljh-data/sat_data/VIIRS_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = os.path.join(data_dir, \"VNP14IMG_NRT.A2018064.1200.001.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfid = nc4.Dataset(nc_file,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick overview of the file content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Content of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the variable information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncfid.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the dimension information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in ncfid.dimensions.values():\n",
    "     print(dim, dim.isunlimited())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of dimension name and retrieve info for each dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ncfid.dimensions.keys():\n",
    "    try:\n",
    "        dim = ncfid.variables[name]\n",
    "        print(name, dim.dtype, dim.size)\n",
    "    except:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dim in ncfid.dimensions.items():\n",
    "    print(name, dim.size, dim.isunlimited())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Printing File Attributes</font>\n",
    "\n",
    "Get the global file attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for att in ncfid.ncattrs():\n",
    "    print(f\"{att}: \\n\\t {ncfid.getncattr(att)}\")\n",
    "    #print(\"{:>15}: {}\".format(att, ncfid.getncattr(att)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for att in ncfid.ncattrs():\n",
    "    print(f\"{att:>15}: {getattr(ncfid, att)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global attributes as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncfid.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Printing Variable Information</font>\n",
    "\n",
    "List variable information but exclude dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ncfid.variables.keys():\n",
    "    if (name not in ncfid.dimensions.keys()):\n",
    "        data = ncfid.variables[name]\n",
    "        try:\n",
    "            print(f\"{name}: \\n\\t Unit: {data.units} \\n\\t shape: {data.shape} \\n\\t Type: {data.dtype} \\n\\t Dim: {data.dimensions}\")\n",
    "        except:\n",
    "            print(f\"{name}: \\n\\t Type: {data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, var in ncfid.variables.items():\n",
    "    if (name not in ncfid.dimensions.keys()):\n",
    "        try:\n",
    "            print(name, var.units, var.shape, var.dtype, var.dimensions)\n",
    "        except:\n",
    "            print(name, var.dtype)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write a function to print variable attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ncattr(fid, key):\n",
    "    \"\"\"\n",
    "        Prints the NetCDF file attributes for a given key\n",
    "\n",
    "        Parameters: \n",
    "            * fid:  netCDF file identifier\n",
    "            * key:  unicode (a valid netCDF4.Dataset.variables key)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('{}  -->'.format(key))\n",
    "        print(\"\\t {:>15}: {}\".format(\"type\", fid.variables[key].dtype))\n",
    "        for attr in fid.variables[key].ncattrs():\n",
    "            print('\\t {:>15}: {}'.format(attr, fid.variables[key].getncattr(attr)))\n",
    "    except KeyError:\n",
    "        print(\"\\t WARNING: {} does not contain variable attributes\".format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_ncattr.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ncfid.variables.keys():\n",
    "    print_ncattr(ncfid, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ncattr(ncfid, \"FP_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, var in ncfid.variables.items():\n",
    "    print('{}  -->'.format(name))\n",
    "    print(\"\\t {:>15}: {}\".format(\"type\", var.dtype))\n",
    "    for attr in var.ncattrs():\n",
    "        print('\\t {:>15}: {}'.format(attr, var.getncattr(attr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the groups (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncfid.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_group_tree(top):\n",
    "    \"\"\"\n",
    "       Python generator that is used to walk the directory tree.\n",
    "    \"\"\"\n",
    "    values = top.groups.values()\n",
    "    yield values\n",
    "    for value in top.groups.values():\n",
    "        for children in walk_group_tree(value):\n",
    "            yield children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the created groups in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for children in walk_group_tree(ncfid):\n",
    "    for child in children:\n",
    "        print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Close the file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"blue\"> Important Notes</font>\n",
    "\n",
    "The command:\n",
    "\n",
    "```python\n",
    "   field = ncfid.variables[var_name]\n",
    "```\n",
    "extract from the netCDF file all the infomation (attribute, dimensions, values, etc.) related to `var_name` and `field` may point (memory) to the dataset in the netCDF file. Any modification of `field` may result in changes in the netCDF file.\n",
    "\n",
    "To access to the values, we recommend the command:\n",
    "\n",
    "```python\n",
    "   vals = ncfid.variables[var_name][:]\n",
    "``` \n",
    "However, it loads into memory all the values of the variable `var_name`. To potentially avoid crashing your computer, it is better to load small parts at a time by replacing `[:]` with slices, for example, `[0:2]`, `[2:4]`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"blue\"> APPLICATION:</font> Plot of I05 brightness temperature of fire pixel@1\n",
    "\n",
    "\n",
    "[Source code](https://hdfeos.org/zoo/LAADS/VNP14IMG_NRT.A2018064.1200.001.nc.py)\n",
    "\n",
    "[Plot](https://hdfeos.org/zoo/LAADS/VNP14IMG_NRT.A2018064.1200.001.nc.py.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nc4.Dataset(nc_file,'r') as ncfid:\n",
    "    lons = ncfid.variables['FP_longitude'][:] # longitude grid points\n",
    "    lats = ncfid.variables['FP_latitude'][:]  # latitude grid points\n",
    "    var = ncfid.variables['FP_T5']\n",
    "    data = var[:]\n",
    "    units = var.units\n",
    "    long_name = var.long_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of lons: {np.shape(lons)}\")\n",
    "print(f\"Shape of lats: {np.shape(lats)}\")\n",
    "print(f\"Shape of data: {np.shape(data)}\")\n",
    "print(f\"Units:         {units}\")\n",
    "print(f\"Long name:     {long_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 5))\n",
    "\n",
    "# Get the central latitude and longitude\n",
    "lat_m = lats[lats.shape[0]//2]\n",
    "lon_m = lons[lons.shape[0]//2]\n",
    "\n",
    "# Orthographic projection.\n",
    "map_projection = ccrs.Orthographic(central_longitude=lon_m,\n",
    "                                   central_latitude=lat_m,\n",
    "                                   globe=None)\n",
    "\n",
    "data_transform = ccrs.PlateCarree()\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1, projection=map_projection)\n",
    "\n",
    "# Remove the following to see zoom-in view.\n",
    "ax.set_global()\n",
    "\n",
    "# Plot on map.\n",
    "p = plt.scatter(lons, lats, c=data, s=1, cmap=plt.cm.jet,\n",
    "                transform=data_transform)\n",
    "# Put grids.\n",
    "gl = ax.gridlines()\n",
    "\n",
    "# Put coast lines.\n",
    "ax.coastlines()\n",
    "\n",
    "# Adjust colorbar size and location using fraction and pad.\n",
    "cb = plt.colorbar(p, fraction=0.022, pad=0.01)\n",
    "cb.set_label(units, fontsize=8)\n",
    "\n",
    "# Put title.\n",
    "basename = os.path.basename(nc_file)\n",
    "plt.title('{0}\\n{1}'.format(basename, long_name), fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> HDF4</font>\n",
    "\n",
    "### <font color=\"blue\"> What is HDF4? </font>\n",
    "\n",
    "- HDF4 is an older hierarchical data format as compared to HDF5.\n",
    "- HDF4 files are self-describing.\n",
    "- It supports annotated multidimensional arrays (also called scientific data sets), raster images, tables, etc.\n",
    "-  One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. \n",
    "- Users can create their own grouping structures called \"vgroups.\"\n",
    "- Does not support file larger than 2Gb.\n",
    "- HDF4 is still the primary data format that is adapted for MODIS data products published by NASA.\n",
    "\n",
    "### <font color=\"blue\"> What is pyhdf? </font>\n",
    "\n",
    "- Python interface to HDF4.\n",
    "- Implements the SD (scientific dataset), VS (Vdata) and V (Vgroup) APIs.\n",
    "- SD datasets are read/written through numpy arrays. \n",
    "- It supports both Python 2 and Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhdf.SD import SD\n",
    "from pyhdf.SD import SDS\n",
    "from pyhdf.SD import SDC\n",
    "from pyhdf.SD import SDim\n",
    "from pyhdf.SD import SDAttr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyHDF Import Context\n",
    "\n",
    "- The `SD` (Scientific Data) class is used for file and top-level info access and implements the HDF SD interface.\n",
    "- The `SDS` (Scientific Dataset) class is used for dataset objects.\n",
    "- The `SDC` (Scientific Data Constants) class holds the constants that define file opening modes and data types.\n",
    "- The `SDim` (Scientific Data Dimensions) class is used for dimension objects.\n",
    "- The `SDAttr` (Scientific Data Attributes) class is used for attribute objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf4_file_name = os.path.join(data_dir, \"VNP09_NRT.A2018064.0524.001.hdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = SD(hdf4_file_name, SDC.READ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information on the file\n",
    "\n",
    "- The first number indicates the number of datasets in the file  while the second number indicates the number of attributes attached to the global file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the file attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attrs = hdf.attributes()\n",
    "pprint.pprint(global_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(global_attrs)):\n",
    "    print(hdf.attr(i).info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all available SDS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(hdf.datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = hdf.datasets()\n",
    "\n",
    "for index, name in enumerate(datasets_dict.keys()):\n",
    "    print(f\"{index}: --> {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extraction\n",
    "\n",
    "- The `select()` method from the `SD` class allows us to extract a dataset (object) given it's name or index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = \"375m Surface Reflectance Band I1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_ds = hdf.select(field_name) # selects a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get information on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the data as a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_data = field_ds.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_data_slice = field_data[:,0].astype(np.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the data directly from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_data = field_ds[:,:].astype(np.double)\n",
    "field_data_slice = field_ds[:,0].astype(np.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `SDS` class, we can also access the dimension names and sizes using the `dimensions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_ds.dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_attrs = field_ds.attributes(full=1)\n",
    "field_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_attrs[\"FILL_VALUES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = set()\n",
    "for a in field_attrs[\"FILL_VALUES\"][0].split(\",\"):\n",
    "    b = a.split(\"=\")[1]\n",
    "    fill_values.add(float(b))\n",
    "    \n",
    "fill_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_name = field_name\n",
    "data = field_data\n",
    "\n",
    "scale_factor = field_attrs[\"Scale\"][0]        \n",
    "add_offset = field_attrs[\"Offset\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of data:  {np.shape(data)}\")\n",
    "print(f\"Min/Max Values: {np.nanmin(data)}/{np.nanmax(data)}\")\n",
    "print(f\"Long Name:      {long_name}\")\n",
    "print(f\"Scale:          {scale_factor}\")\n",
    "print(f\"Offset:         {add_offset}\")\n",
    "print(f\"Units:          {units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _FillValue in fill_values:\n",
    "    data[data == _FillValue] = np.nan\n",
    "\n",
    "data -= add_offset\n",
    "data *= scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min/Max Values: {np.nanmin(data)}/{np.nanmax(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(data, interpolation='nearest')\n",
    "ax.set_title(long_name)\n",
    "plt.colorbar(im);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>HDF5</font>\n",
    "\n",
    "### <font color='blue'> What is HDF5?</font>\n",
    "\n",
    "* HDF5 is a file format and library for storing scientific data.  \n",
    "* It supports files larger than 2 GB and  parallel I/O. \n",
    "* It uses a \"file directory\" like structure that allows you to organize data within the file in many different structured ways, as you might do with files on your computer. \n",
    "* The HDF5 format allows for embedding of metadata making it self-describing.\n",
    "* An HDF5 file is a container for two kinds of objects: \n",
    "   1. **Datasets**: Array-like collections of data.\n",
    "   2. **Groups**: Folder-like containers that hold datasets and other groups.\n",
    "* Each group or dataset can have an associated attribute list to provide extra information related to the object.\n",
    "   \n",
    "![hdf5](https://miro.medium.com/max/1400/0*_vh8GIkBQNOg42uv.jpg)\n",
    "Image Source: [https://www.neonscience.org/about-hdf5](https://www.neonscience.org/about-hdf5)\n",
    "   \n",
    "- HDF5 is conceptually related to HDF4 but incompatible; it cannot directly read or work with HDF4 files or the HDF4 library\n",
    "\n",
    "### <font color='blue'> What is h5py?</font>\n",
    "\n",
    "* h5py is the Python interface to the HDF5.\n",
    "* Provide easy-to-use high level interface, which allows you to store huge amounts of numerical data.\n",
    "* Easily manipulate that data from NumPy. \n",
    "* Use straightforward NumPy and Python metaphors, like dictionary and NumPy array syntax. \n",
    "* Within h5py, HDF5 groups work like dictionaries, and datasets work like NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = os.path.join(data_dir, \"VNP46A1.A2020302.h07v07.001.2020303075447.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "    for var in hdfid.keys():\n",
    "        print(f\"{var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have `Dataset` or `Group`:\n",
    "\n",
    "```python\n",
    "   isGroup   = isinstance(item, h5py.Group)\n",
    "   isDataset = isinstance(item, h5py.Dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "    for var in hdfid.keys():\n",
    "        obj = hdfid[var]\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(f\"{var:>25}: --> Group\")\n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            print(f\"{var:>25}: --> Dataset\")\n",
    "        else:\n",
    "            print(f\"{var:25}: --> unknown type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specific information about an item\n",
    "\n",
    "```python\n",
    "  item.id     \n",
    "  item.ref     \n",
    "  item.parent  \n",
    "  item.file   \n",
    "  item.name \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "     for var in hdfid.keys():\n",
    "         obj = hdfid[var]\n",
    "         print(f\"{obj.name:>20} --> {obj.parent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the datasets\n",
    "\n",
    "```python\n",
    "  isinstance(obj, h5py.Dataset)\n",
    "\n",
    "  ds.dtype     \n",
    "  ds.shape     \n",
    "  ds.value \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "    for var in hdfid.keys():\n",
    "        obj = hdfid[var]\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"{var}: \\n\\t Type: {obj.dtype} \\t Shape: {obj.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get item attributes for File or Group (if attributes available)**\n",
    "\n",
    "```python\n",
    "item.attrs  # for example: <Attributes of HDF5 object at 230141696>\n",
    "item.attrs.keys() # for example: ['start.seconds', 'start.nanoseconds']\n",
    "item.attrs.values() # for example: [1297608424L, 627075857L]\n",
    "len(item.attrs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "    mylist = list(hdfid.keys())\n",
    "    for var in mylist:\n",
    "        obj = hdfid[var]\n",
    "        print(obj.attrs.keys(), len(obj.attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the names of datasets:** Use `visit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "     hdfid.visit(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the names of the datasets and the corresponding objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(name):\n",
    "    print(f\"{name}: \\n\\t {hdfid[name]}\")\n",
    "\n",
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "    hdfid.visit(my_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the datasets and their attributes:** Use `visititems`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all(name, obj):\n",
    "    print(f\"{name}: \\n\\t {dict(obj.attrs)}\")\n",
    "\n",
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "     hdfid.visititems(print_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List each item and determine if it is a group or a dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_2(name, obj):\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        print(f\"{name:>25}: --> Group\")\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{name:>25}: --> Dataset\")\n",
    "    else:\n",
    "        print(f\"{name:25}: --> unknown type\")\n",
    "\n",
    "with h5py.File(hdf5_file_name, 'r') as hdfid:\n",
    "     hdfid.visititems(print_all_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"blue\">APPLICATION:</font>  Read and restore the values of the   `Brightness Temperature of band M12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GRID_NAME = 'VNP_Grid_DNB'\n",
    "DATAFIELD_NAME = 'BrightnessTemperature_M12'\n",
    "\n",
    "with h5py.File(hdf5_file_name, mode='r') as f:        \n",
    "    name = f'/HDFEOS/GRIDS/{GRID_NAME}/Data Fields/{DATAFIELD_NAME}'\n",
    "    data = f[name][:].astype(np.float64)\n",
    "    # Read attributes.\n",
    "    scale_factor = f[name].attrs['scale_factor'][0]\n",
    "    add_offset = f[name].attrs['add_offset'][0]\n",
    "    units = f[name].attrs['units'].decode()\n",
    "    _FillValue = f[name].attrs['_FillValue'][0]\n",
    "    long_name = f[name].attrs['long_name'].decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of data: {np.shape(data)}\")\n",
    "print(f\"Scale:         {scale_factor}\")\n",
    "print(f\"Offset:        {add_offset}\")\n",
    "print(f\"Fill Value:    {_FillValue}\")\n",
    "print(f\"Units:         {units}\")\n",
    "print(f\"Long name:     {long_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum: {data.min()}\")\n",
    "print(f\"Maximum: {data.max()}\")\n",
    "print(f\"Mean:    {data.mean()}\")\n",
    "print(f\"STD:     {data.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data == _FillValue] = np.nan\n",
    "data -= add_offset\n",
    "data *= scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum: {data.min()}\")\n",
    "print(f\"Maximum: {data.max()}\")\n",
    "print(f\"Mean:    {data.mean()}\")\n",
    "print(f\"STD:     {data.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(data, interpolation='nearest',cmap=cm.hot)\n",
    "ax.set_title(long_name)\n",
    "plt.colorbar(im);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
