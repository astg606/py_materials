{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://github.com/astg606/py_materials/blob/master/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1> <font color=\"red\">Reading LANDSAT hdf Files using pyhdf</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook shows an example of how to use the **pyhdf**, **Numpy**, **Xarray**, **Matplotlib**, **Cartopy**, and **hvplot** Python packages to work with a LANDSAT file in HDF4 format.  \n",
    "\n",
    "The main workflow steps are:\n",
    "- Open a LANDSAT HDF4 file\n",
    "- Read the global file metadata\n",
    "    - Recognize the file attribute\n",
    "    - Find names of variables and their attributes\n",
    "- Read dataset from file\n",
    "- Visualize satellite data on a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Primary References/Resources</font>\n",
    "\n",
    "- https://moonbooks.org/Articles/How-to-read-a-MODIS-HDF-file-using-python-/\n",
    "- http://fhs.github.io/pyhdf/modules/SD.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Import the Python Packages</font>\n",
    "\n",
    "Six Python packages (libraries)  used in this Notebook:\n",
    "- **pyhdf**: Read HDF4 files\n",
    "- **NumPy**: Perform array operations\n",
    "- **Xarray**: Work with labeled multi-dimensional arrays\n",
    "- **Matplotlib**: Make static plots (mainly two-dimensional)\n",
    "- **Cartopy**: Create maps\n",
    "- **hvplot**: Create interactive plots/maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.xarray\n",
    "from cartopy import crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhdf.SD import SD\n",
    "from pyhdf.SD import SDS\n",
    "from pyhdf.SD import SDC\n",
    "from pyhdf.SD import SDim\n",
    "from pyhdf.SD import SDAttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Version of Numpy:   {np.__version__}\")\n",
    "print(f\"Version of Xarray:  {xr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">LANDSAT Satellite Program</font>\n",
    "\n",
    "[The Landsat Program](https://www.usgs.gov/landsat-missions/landsat-satellite-missions) is a series of Earth-observing satellite missions jointly managed by NASA and the U.S. Geological Survey. Landsat satellites have the optimal ground resolution and spectral bands to efficiently track land use and to document land change due to climate change, urbanization, drought, wildfire, biomass changes (carbon assessments), and a host of other natural and human-caused changes. \n",
    "\n",
    "The Landsat Program’s continuous archive (1972-present) provides essential land change data and trending information not otherwise available. Landsat represents the world’s longest continuously acquired collection of space-based moderate-resolution land remote sensing data. Landsat is an essential capability that enables the U.S. Department of the Interior to wisely manage Federal lands. People around the world are using Landsat data for research, business, education, and other activities. \n",
    "\n",
    "#### LANDSAT Data File Name Convention\n",
    "\n",
    "Details on the naming convention can be found in:\n",
    "\n",
    "https://gisgeography.com/landsat-file-naming-convention/\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Accessing a Sample HDF4 Data Files</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Step 1: Identify the Location of the File</font>\n",
    "\n",
    "Directory where the LANDSAT file is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"/Users/jkouatch/myTasks/PythonTraining/ASTG606/Materials/sat_data/LANDSAT_Data/\"\n",
    "data_dir = \"/tljh-data/sat_data/LANDSAT_Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full path to the file name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(data_dir, \"LT50830152011198GLC00.hdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding the naming convention on `fname`:\n",
    "- `L`: Landsat\n",
    "- `T`: IRS sensor\n",
    "- `5`: Landsat-5\n",
    "- `083`: Worldwide Reference System (WRS) path\n",
    "- `015`: WRS row (the north-south row that sectionalizes WRS)\n",
    "- `2011`: year\n",
    "- `214`: Julian day of the year\n",
    "- `GLC`: ground station identifier\n",
    "- `00`: archive version number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Step 2: Open the File</font>\n",
    "\n",
    "Opening files for reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = SD(fname, SDC.READ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Step 3: Obtain the File Attributes</font>\n",
    "\n",
    "Basic information on the files:\n",
    "\n",
    "- First number: number of datasets in the file (not to be confused with Xarray datasets)\n",
    "- Second number: number of attributes attached to the global file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Attributes\n",
    "\n",
    "- We can access the file attributes which hold important global metadata.\n",
    "- Some notable ones are the data provider, satellite name and instrument, coordinate boundaries, and structural metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_attrs = fid.attributes()\n",
    "for index, name in enumerate(file_attrs.keys(), start=1):\n",
    "    print(f\"{index:>2}- {name}: --> {file_attrs[name]}\")\n",
    "\n",
    "#pprint.pprint(file_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the datasets' names and basic info such as shape and dimension labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dts = fid.datasets()\n",
    "for index, name in enumerate(file_dts.keys(), start=1):\n",
    "    print(f\"{index:>2}- {name} \\n\\t {file_dts[name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Step 4: Data Extraction</font>\n",
    "\n",
    "Let's assume that we want to extract data from the field `sr_band4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select()` method from the `SD` class allows us to extract a dataset (object) given it's name or index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'sr_band4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = fid.select(field_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get basic information from the dataset:\n",
    "\n",
    "- The `info()` function in the `SDS` class allows us to get the dataset name, rank (or level with file-leve being rank 1), dimension lengths, data type, and number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the dataset attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_attrs = sample_ds.attributes()\n",
    "ds_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Important information such as `_FillValue`, `scale_factor`, and `add_offset` are found in dataset attributes and will be important to fully restoring the data.\n",
    "- It is good practice to set default values if using different datasets/variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FillValue = None\n",
    "scale_factor = 1\n",
    "add_offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sample_ds.attributes().items():\n",
    "    if key == '_FillValue':\n",
    "        _FillValue = value  \n",
    "    if key == 'scale_factor':\n",
    "        scale_factor = value\n",
    "    if key == 'add_offset':\n",
    "        add_offset = value\n",
    "    \n",
    "print(f'Fill Value:   {_FillValue}')\n",
    "print(f'Scale Factor: {scale_factor}')\n",
    "print(f'Offset:       {add_offset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the data\n",
    "\n",
    "- We can retrieve and store the data itself as a NumPy array using the `get()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sample_ds.get() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms that the data has been stored as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Class Type: \", type(sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like any NumPy array, we can get the shape and dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Step 5: Get the Dimensions</font>\n",
    "\n",
    "- From the `SDS` class, we can access the dimension names and sizes using the `dimensions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dims = sample_ds.dimensions()\n",
    "ds_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While this is nice, the dictionary above does not allow us to access actual dimension objects and the other information that they may hold. \n",
    "- To access the objects, we can use the `dim()` function from the `SDS` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dims = list()   \n",
    "\n",
    "for i in range(len(ds_dims)):\n",
    "    sample_dims.append(sample_ds.dim(i))\n",
    "\n",
    "sample_dims   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can access not only the labels and size but also the units, scale data type, and number of attributes. \n",
    "- We can also access the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample_dims)):\n",
    "    print(f\"Dimension {i+1}\")\n",
    "    print(\"\\tInfo:\", sample_dims[i].info())\n",
    "    print(\"\\tLength:\", sample_dims[i].length())\n",
    "    print(\"\\tAttributes: \", sample_dims[i].attributes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Step 6: Coordinates</font>\n",
    "\n",
    "- In `pyhdf`, it is possible that coordinates (known as dimension scales) are actually stored as datasets. \n",
    "- The `SDS` class provides the `iscoordvar()` function to determine that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bool(sample_ds.iscoordvar()))\n",
    "\n",
    "# If there was a scale, it would be accessible via: dim1.getscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to traverse through all the datasets and see if one of them holds coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_sets = list() \n",
    "\n",
    "for i in range(len(fid.datasets())):\n",
    "    ds = fid.select(i)\n",
    "    if ds.iscoordvar():\n",
    "        coord_sets.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is also possible that some coordinate information is stored as a file attribute. \n",
    "- If we go back once more to our global attribute dictionary, we can see some keys such as corner and bounding coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pprint(fid.attributes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then traverse through it and extract and attributes that may be related to the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_attrs = dict()   # Will hold coordinate-related attributes\n",
    "for key, value in fid.attributes().items():\n",
    "    if 'coordinate' in key.lower() or 'latlong' in key.lower():\n",
    "        coord_attrs[key] = value\n",
    "\n",
    "coord_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these attributes, it would be much easier to work with our bounding coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now go back to our datasets, we can see that they all have the same shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dts = fid.datasets()\n",
    "for index, name in enumerate(file_dts.keys(), start=1):\n",
    "    print(f\"{index:>2}- {name} \\t {file_dts[name][0:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, vals in enumerate(fid.datasets().values(), start=1):\n",
    "    print(f\"Dataset {idx:>3} Shape:\", vals[0:2])\n",
    "    shape = vals[1]\n",
    "\n",
    "#vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the bounding coordinates and the shape, we can artificially create our full coordinates.\n",
    "- However, this is under the assumption that the datasets truly align with this artificial system. This means that we can't confirm its accuracy.\n",
    "\n",
    "The first step would be to extract our boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in coord_attrs.keys():\n",
    "    if 'north' in key.lower():\n",
    "        latN = coord_attrs[key]\n",
    "    if 'south' in key.lower():\n",
    "        latS = coord_attrs[key]\n",
    "    if 'east' in key.lower():\n",
    "        lonE = coord_attrs[key]\n",
    "    if 'west' in key.lower():\n",
    "        lonW = coord_attrs[key]\n",
    "\n",
    "print(\"North:\", latN)\n",
    "print(\"South:\", latS)\n",
    "print(\"East:\", lonE)\n",
    "print(\"West:\", lonW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would create our spacing using the dataset shapes and boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our coords. 'lats' can be substituted for 'y' and 'lons' for 'x'\n",
    "lat_space = (latN - latS) / shape[0]\n",
    "lon_space = (lonE - lonW) / shape[1]\n",
    "\n",
    "print(lat_space, \"|\", lon_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally create our coordinates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.linspace(latS, latN, shape[0])\n",
    "lons = np.linspace(lonW, lonE, shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Latitudes:\\n', lats)\n",
    "print('Longitudes:\\n', lons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now close our file reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Conversion to Xarray DataArrays and Datasets</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a set of functions that we help us to:\n",
    "- Read a LANDSAT data file\n",
    "- Read a field and determine each dimension and coordinate.\n",
    "- Read the field attribute and and restore its values.\n",
    "- Create a Xarray DataArray associated with the field.\n",
    "\n",
    "We can then combine all the DataArrays into a Xarray Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to open the file and get the file identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fid(file_name):\n",
    "    \"\"\"\n",
    "    Open a HDF4 file and return thfile identifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "         a file name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fid :\n",
    "         an SD object (file identifier)\n",
    "    \"\"\"\n",
    "    fid = SD(file_name, SDC.READ)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the value of an attribute of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_attribute_value(sample_ds, attr_name):\n",
    "    '''\n",
    "    Obtain the value of a specified attribute in a dataset.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    sample_ds : SDS object\n",
    "    attr_name : str\n",
    "         Attribute name    \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    value: float, int, str, list\n",
    "         Value of the attribute. If attribute not available, None.\n",
    "    '''\n",
    "    for key, value in sample_ds.attributes().items():\n",
    "        if key == attr_name:\n",
    "            return value \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to restore the data from the dataset attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_data(sample_ds):\n",
    "    '''\n",
    "    Restore the data given the dataset attribute\n",
    "    Parameter(s): SDS object\n",
    "    Return Type(s): NumPy array\n",
    "    Function: restores data of a given dataset object\n",
    "    '''\n",
    "    _FillValue = get_ds_attribute_value(sample_ds, '_FillValue')\n",
    "    scale_factor = get_ds_attribute_value(sample_ds, 'scale_factor')\n",
    "    add_offset = get_ds_attribute_value(sample_ds, 'add_offset')\n",
    "    valid_range = get_ds_attribute_value(sample_ds, 'valid_range')\n",
    "    \n",
    "    data = sample_ds.get().astype('float')\n",
    "    \n",
    "    if valid_range:\n",
    "        valid_min, valid_max = valid_range[0], valid_range[1]\n",
    "        invalid = np.logical_or(data > valid_max, data < valid_min)\n",
    "    \n",
    "    #data[invalid] = np.nan\n",
    "    #data = np.where(data != _FillValue, data, np.nan)\n",
    "    \n",
    "    if _FillValue:\n",
    "        if valid_range:\n",
    "            invalid = np.logical_or(invalid, data == _FillValue)\n",
    "            data[invalid] = np.nan\n",
    "        else:\n",
    "            data = np.where(data != _FillValue, data, np.nan)\n",
    "    \n",
    "    if add_offset:\n",
    "        data -= add_offset\n",
    "    if scale_factor:\n",
    "        data *= scale_factor\n",
    "    data = np.ma.masked_array(data, np.isnan(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to obtain the dimension object from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dims(sample_ds):\n",
    "    '''\n",
    "    Get the dimension objects of a given dataset object\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_ds : SDS object\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sample_dim : list\n",
    "          List of SDim objects\n",
    "    '''\n",
    "    sample_dims = list()\n",
    "    \n",
    "    for i in range(len(sample_ds.dimensions())):\n",
    "        sample_dims.append(sample_ds.dim(i))\n",
    "        \n",
    "    return sample_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to obtain the dimension attributes from a dimension object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dim_attrs(dim):\n",
    "    '''\n",
    "    Get the attributes of a given dimension object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : SDim object\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    attrs : dict\n",
    "         Dictionary of attributes.\n",
    "    '''\n",
    "    attrs = dict()\n",
    "    \n",
    "    attrs['Given Name'] = dim.info()[0]\n",
    "    attrs['dtype'] = dim.info()[2]\n",
    "    attrs.update(dim.attributes())   \n",
    "    \n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dims_attrs(ds):\n",
    "    dims_attrs = dict()\n",
    "    dims = get_dims(ds)\n",
    "    \n",
    "    for dim in dims:\n",
    "        if dim.info()[0] == 'YDim':\n",
    "            dims_attrs['lat'] = get_dim_attrs(dim)\n",
    "        elif dim.info()[0] == 'XDim':\n",
    "            dims_attrs['lon'] = get_dim_attrs(dim)\n",
    "            \n",
    "    return dims_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to obtain the coordinates from the file identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fid_coords(sample_fid):\n",
    "    '''\n",
    "    Returns False if there are no file-level coords, otherwise True.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_fid : SD object\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "      \n",
    "    '''\n",
    "    coord_sets = list()\n",
    "\n",
    "    for i in range(len(sample_fid.datasets())):\n",
    "        sample_ds = sample_fid.select(i)\n",
    "        if bool(sample_ds.iscoordvar()):\n",
    "            coord_sets.append(sample_ds)\n",
    "\n",
    "    if coord_sets:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to obtain the coordinate bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord_bounds(sample_fid):\n",
    "    '''\n",
    "    Obtain the latitude/longitude corners from file attributes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_fid : SD object\n",
    "    \n",
    "    Returns\n",
    "    coord_bounds : dict\n",
    "          Dictionary of String keys and numeric items\n",
    "    '''\n",
    "    coord_attrs = {}    \n",
    "    # Gets our coordinate-related attributes\n",
    "    for key, value in sample_fid.attributes().items():\n",
    "        if 'coordinate' in key.lower():\n",
    "            coord_attrs[key] = value \n",
    "        \n",
    "    coord_bounds = {}\n",
    "    # Gets our coordinate bounds\n",
    "    for key in coord_attrs.keys():\n",
    "        if 'north' in key.lower():\n",
    "            coord_bounds['latN'] = coord_attrs[key]\n",
    "        if 'south' in key.lower():\n",
    "            coord_bounds['latS'] = coord_attrs[key]\n",
    "        if 'east' in key.lower():\n",
    "            coord_bounds['lonE'] = coord_attrs[key]\n",
    "        if 'west' in key.lower():\n",
    "            coord_bounds['lonW'] = coord_attrs[key]    \n",
    "    \n",
    "    return coord_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute the coordinate (latitude/longitude gridpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_coords(sample_fid, sample_ds): \n",
    "    '''\n",
    "    Determine the latitude/longitude grid points of a dataset.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_fid : SD object\n",
    "    sample_ds : SDS object\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sample_coords : dict\n",
    "          Dictionary containing the latitude and longitudes coordinates.\n",
    "    '''\n",
    "    sample_bounds = get_coord_bounds(sample_fid)\n",
    "    \n",
    "    latN = sample_bounds['latN']\n",
    "    latS = sample_bounds['latS']\n",
    "    lonE = sample_bounds['lonE']\n",
    "    lonW = sample_bounds['lonW']\n",
    "    \n",
    "    lat_shape = sample_ds.dimensions()['YDim']\n",
    "    lon_shape = sample_ds.dimensions()['XDim']\n",
    "    \n",
    "    if isinstance(sample_bounds, dict):   # Have to configure dataset coords\n",
    "        lat_space = (latN - latS) / lat_shape\n",
    "        lon_space = (lonE - lonW) / lon_shape\n",
    "        \n",
    "        lats = np.linspace(latS, latN + lat_space, lat_shape)\n",
    "        lons = np.linspace(lonW, lonE + lon_space, lon_shape)\n",
    "        \n",
    "        sample_coords = {'lats': lats, 'lons': lons}\n",
    "        return sample_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create a Xarray Dataset from a LANDSAT file\n",
    "\n",
    "To simplify the file, we are not going to only consider the fields: `sr_band2`, `sr_band4`, and `toa_band6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xarray_dataset_from_file(file):\n",
    "    '''\n",
    "    Read a LANDSAT HDF4 file and returns an Xarray dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "         LANDSAT file name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr_ds : Xarray Dataset\n",
    "    '''\n",
    "    \n",
    "    list_attributes = ['long_name', 'units', 'app_version']\n",
    "    sample_fields = ['sr_band2', 'sr_band4', 'toa_band6']\n",
    "    \n",
    "    fid = get_fid(file)\n",
    "    \n",
    "    if get_fid_coords(fid):   # File-level coords exist\n",
    "        pass\n",
    "    else:\n",
    "        fid_coords = False    # File-level coords do not exist   \n",
    "    \n",
    "    xr_ds = xr.Dataset()\n",
    "    \n",
    "    for name in fid.datasets().keys():\n",
    "        \n",
    "        # We are only going to consider few fields\n",
    "        if name not in sample_fields:\n",
    "            continue\n",
    "            \n",
    "        ds = fid.select(name)\n",
    "        \n",
    "        coords_dict = get_ds_coords(fid, ds)\n",
    "        lats = coords_dict['lats']\n",
    "        lons = coords_dict['lons']\n",
    "        \n",
    "        xr_ds[name] = xr.DataArray(restore_data(ds), \n",
    "                                   coords = [lats, lons], \n",
    "                                   dims = ['lat', 'lon'])\n",
    "        for key in list_attributes:\n",
    "            xr_ds[name].attrs[key] = get_ds_attribute_value(ds, key)\n",
    "        #xr_ds[name].attrs = ds.attributes()\n",
    "        \n",
    "        dims = get_dims(ds)\n",
    "        for dim in dims:\n",
    "            if dim.info()[0] == 'YDim':\n",
    "                xr_ds[name].lat.attrs = get_dim_attrs(dim)\n",
    "            elif dim.info()[0] == 'XDim':\n",
    "                xr_ds[name].lon.attrs = get_dim_attrs(dim)\n",
    "    \n",
    "    xr_ds.attrs = fid.attributes()\n",
    "    \n",
    "    fid.end()\n",
    "    return xr_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Xarray Dataset from a LANDSAT Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_ds = create_xarray_dataset_from_file(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Plotting Our Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_GB = fname_ds.nbytes / (1024*1024*1024)\n",
    "\n",
    "fname_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_band6 = fname_ds['toa_band6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic image plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_band6.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Cartopy for a contour plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_m = toa_band6.lon.values.mean()\n",
    "lat_m = toa_band6.lat.values.mean()\n",
    "\n",
    "map_projection = ccrs.LambertAzimuthalEqualArea(\n",
    "                      central_longitude=lon_m, \n",
    "                      central_latitude=lat_m\n",
    "                )\n",
    "data_transform = ccrs.PlateCarree()\n",
    "\n",
    "subplot_kw = dict(projection=map_projection)\n",
    "fig, ax = plt.subplots(1, 1,\n",
    "                       figsize=(15, 9),\n",
    "                       subplot_kw=subplot_kw)\n",
    "\n",
    "units = toa_band6.attrs['units']\n",
    "cbar_kwargs = {'orientation':'horizontal', \n",
    "               'shrink':0.6, \"pad\" : .05, \n",
    "               'aspect':40, 'label': units}\n",
    "\n",
    "toa_band6.plot.pcolormesh(ax=ax, x='lon', y='lat',\n",
    "                          transform=data_transform,\n",
    "                          cbar_kwargs=cbar_kwargs,\n",
    "                          add_colorbar=True,\n",
    "                          cmap=\"jet\"\n",
    "                         )\n",
    "\n",
    "ax.coastlines()\n",
    "plt.title(field_name, fontsize=8)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic `hvPlot` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_band6.hvplot.image().opts(cmap = 'jet', height=650, width=1300)   # Doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intermediate `hvPlot` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_band6.hvplot.quadmesh('lon', 'lat', cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_band6.hvplot.quadmesh('lon', 'lat', \n",
    "                          xlim = (-172.5, -166.75), \n",
    "                          ylim = (63, 65.3), \n",
    "                          geo = True, project = True, \n",
    "                          rasterize = True, \n",
    "                          projection = ccrs.PlateCarree(), \n",
    "                          features = ['borders'], coastline = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the file:\n",
    "\n",
    "```\n",
    "new_fname = os.path.join(data_dir, \"LT50830152011214GLC00.hdf\")\n",
    "```\n",
    "\n",
    "To create an Xarray Dataset and plot any field of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"blue\">Click here for the solution to Exercise</font></b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "new_fname = os.path.join(data_dir, \"LT50830152011214GLC00.hdf\")\n",
    "\n",
    "new_fname_ds = create_xarray_dataset_from_file(new_fname)\n",
    "\n",
    "new_fname_ds['toa_band6'].plot()\n",
    "\n",
    "```\n",
    "    \n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
