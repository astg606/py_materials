{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "Accessing Web Resources with Python\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "\n",
    "* <a href=\"http://zetcode.com/python/requests/\">Python Requests Tutorial</a>\n",
    "* <a href=\"https://realpython.com/python-requests/\">Python’s Requests Library (Guide)</a>\n",
    "* <a href=\"https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/\">What is web scraping</a>\n",
    "* <a href=\"https://hackernoon.com/building-a-web-scraper-from-start-to-finish-bb6b95388184\">Building a Web Scraper from start to finish</a>\n",
    "* <a href=\"https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/\">Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup</a>\n",
    "* <a href=\"https://www.dataquest.io/blog/web-scraping-tutorial-python/\">Tutorial: Web Scraping with Python Using Beautiful Soup</a>\n",
    "* <a href=\"https://realpython.com/beautiful-soup-web-scraper-python/\">Beautiful Soup: Build a Web Scraper With Python</a>\n",
    "* <a href=\"https://stackabuse.com/download-files-with-python/\">Download Files with Python</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What will be Covered?</font>\n",
    "+ Accessing Web Pages with Requests\n",
    "+ Introduction to Json\n",
    "+ Web Scraping with Json\n",
    "+ Web Scraping with Beautiful Soup\n",
    "\n",
    "![fig_scrap](https://miro.medium.com/max/1400/1*4BnBQE9Bu-EQ-gGz25x8pg.png)\n",
    "Image Source: gurutechnolabs.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Python `requests` Module</font>\n",
    "\n",
    "* Requests is a simple and elegant Python HTTP (Hypertext Transfer Protocol) library. \n",
    "* It provides methods for accessing Web resources via HTTP. \n",
    "* The HTTP request returns a Response Object with all the response data (content, encoding, status, etc.).\n",
    "* Requests is a built-in Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import requests as reqs\n",
    "\n",
    "print(reqs.__version__)\n",
    "print(reqs.__copyright__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading a Web Page**\n",
    "- We use the function `get()` to grab the content of a web page into an object.\n",
    "- We extract from the object the HTML content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get all information from the `resp` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the module `re` to strip all the HTML markups from the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "\n",
    "content = resp.text\n",
    "\n",
    "stripped = re.sub('<[^<]+?>', '', content)\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you make a request, `Requests` makes educated guesses about the encoding of the response based on the HTTP headers. \n",
    "- The text encoding guessed by `Requests` is used when you access `resp.text`. \n",
    "- You can find out what encoding `Requests` is using, and change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change the encoding, `Requests` will use the new value of `resp.encoding` whenever you call `resp.text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HTTP Request**\n",
    "- An HTTP request is a message send from the client to the browser to retrieve some information or to make some action.\n",
    "- Request's request method creates a new request. \n",
    "- We use the `request` module methods: `get()`, `post()`, or `put()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `GET` request and send it to the web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.request(method='GET', url=\"http://www.webcode.me\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the Status of a Web Page**\n",
    "- We perform an HTTP request with the `get()` method and check for the returned status.\n",
    "- `200` is a standard response for a successful HTTP request and `404` tells that the requested resource could not be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me/news\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for checking if a website is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_website(url):\n",
    "    try:\n",
    "        resp = reqs.get(url,timeout=3)\n",
    "        resp.raise_for_status()\n",
    "    except reqs.exceptions.HTTPError as errh:\n",
    "        print(f\"Http Error: {errh}\")\n",
    "    except reqs.exceptions.ConnectionError as errc:\n",
    "        print(f\"Error Connecting: {errc}\")\n",
    "    except reqs.exceptions.Timeout as errt:\n",
    "        print(f\"Timeout Error: {errt}\")\n",
    "    except reqs.exceptions.RequestException as err:\n",
    "        print(f\"Something Else: {err}\")\n",
    "    else:\n",
    "        print(\"Successfully accessed the site!\")\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.Jules\"\n",
    "resp = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\"\n",
    "resp = access_website(url)\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "\n",
    "print(\"\\t URL:      {}\".format(resp.url))\n",
    "print(\"\\t Encoding: {}\".format(resp.encoding))\n",
    "print(\"\\t URL:      {}\".format(resp.url))\n",
    "print(\"\\t Time:     {}\".format(resp.elapsed))\n",
    "\n",
    "print(\"Server:         {}\".format(resp.headers['server']))\n",
    "print(\"CONNECTION:     {}\".format(resp.headers['CONNECTION']))\n",
    "print(\"Date:           {}\".format(resp.headers['Date']))\n",
    "print(\"Last modified:  {}\".format(resp.headers['last-modified']))\n",
    "print(\"Content type:   {}\".format(resp.headers['content-type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`requests` `head()` Method**\n",
    "- The `head()` method retrieves document headers. \n",
    "- The headers consist of fields, including date, server, content type, or last modification time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.head(\"http://www.webcode.me\")\n",
    "\n",
    "print(\"Server:         {}\".format(resp.headers['server']))\n",
    "print(\"CONNECTION:     {}\".format(resp.headers['CONNECTION']))\n",
    "print(\"Date:           {}\".format(resp.headers['Date']))\n",
    "print(\"Last modified:  {}\".format(resp.headers['last-modified']))\n",
    "print(\"Content type:   {}\".format(resp.headers['content-type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`requests` `get()` Method**\n",
    "- The `get()` method issues a GET request to the server. \n",
    "- The GET method requests a representation of the specified resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the following script sends a variable with a value to the `httpbin.org` server. The variable is specified directly in the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"https://httpbin.org/get?name=Peter\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `get()` method takes a params parameter where we can specify the query parameters.\n",
    "     - The beginning of the query parameters is denoted by a question mark (`?`).\n",
    "     - The pieces of information constituting one query parameter are encoded in key-value pairs, where related keys and values are joined together by an equals sign (`key=value`).\n",
    "     - Every URL can have multiple query parameters, which are separated from each other by an ampersand (`&`)\n",
    "\n",
    "```python\n",
    "resp = reqs.get(\"https://httpbin.org/get?name=Peter&age=23\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We send a `get()` request to the web site and pass the data, which is specified in the `params` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': 'Peter'}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`payload` is a dictionary of pairs of keys/values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': 'Peter', 'age': 23}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass a list of items as a value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': ['Peter', 'Johns'], 'age': 23}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Methods**\n",
    "\n",
    "```python\n",
    "requests.post('https://httpbin.org/post', data={'key':'value'})\n",
    "requests.put('https://httpbin.org/put', data={'key':'value'})\n",
    "requests.delete('https://httpbin.org/delete')\n",
    "requests.patch('https://httpbin.org/patch', data={'key':'value'})\n",
    "requests.options('https://httpbin.org/get')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of `requests` Methods \n",
    "\n",
    "| Method\t| Description |\n",
    "| :--- | :--- |\n",
    "| delete(url, args)\t| Sends a DELETE request to the specified url | \n",
    "| get(url, params, args)\t| Sends a GET request to the specified url | \n",
    "| head(url, args)\t| Sends a HEAD request to the specified url | \n",
    "| patch(url, data, args)\t| Sends a PATCH request to the specified url | \n",
    "| post(url, data, json, args)\t| Sends a POST request to the specified url | \n",
    "| put(url, data, args)\t| Sends a PUT request to the specified url | \n",
    "| request(method, url, args)\t| Sends a request of the specified method to the specified url| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Web Scraping</font>\n",
    "\n",
    "![fig_json](https://daveberesford.co.uk/wp-content/uploads/2019/02/data-scraping-960x594.png)\n",
    "Image Source: daveberesford.co.uk\n",
    "\n",
    "> Web scraping is a mechanism of collecting large amounts of data from the webpage and store the data into any required format which further helps us to perform analysis on the extracted data.\n",
    "\n",
    "\n",
    "- Web scraping is used to extract or “scrape” data from any web page on the Internet.\n",
    "- Web scraping is performed using a “**web scraper**” (or a “bot” or a “web spider” or “web crawler”). \n",
    "- A web-scraper is a program that goes to web pages, downloads the contents, extracts data out of the contents and then saves the data to a file or a database.\n",
    "\n",
    "\n",
    "\n",
    "Web scraping involves a three-step process:\n",
    "\n",
    "1. **Step 1**: Send an HTTP request to the webpage you want to scrape\n",
    "   - The server responds to the request by returning the HTML content of the target webpage.\n",
    "2. **Step 2**: Parse the HTML content\n",
    "   - A parser is needed to create a nested structure of the HTML data. \n",
    "3. **Step 3**: Pull data out of HTML\n",
    "   - We use Python packages such as Json and Beautiful Soup to pull out data and store them.\n",
    "   \n",
    "![fig_scap](https://prowebscraper.com/blog/wp-content/uploads/2017/11/how_does_web_scraping_work.png)\n",
    "Image Source: prowebscraper.com\n",
    "     \n",
    "\n",
    "Web Scrapers crawl websites, extracts data from it, transforms to a usable structured format and load it to a file or database for subsequent use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Web Scraping Rules</font>\n",
    "\n",
    "![fig_ethics](https://hackernoon.com/hn-images/0*MPt2rectMhwklT63.jpg)\n",
    "\n",
    "As reference, check: <a href=\"https://info.scrapinghub.com/web-scraping-guide/web-scraping-best-practices\">The Web Scraping Best Practices Guide</a> or watch the video <a href=\"https://www.youtube.com/watch?v=i7DEy-ZB_Lk\">Is Web Scraping Legal?</a>\n",
    "\n",
    "- Check a website’s Terms and Conditions before you scrape it.\n",
    "- Do not request data from the website too aggressively with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "- The layout of a website may change from time to time, so make sure to revisit the site and rewrite your code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Web Scraping with JSON</font>\n",
    "\n",
    "### <font color=\"green\"> What is JSON?</font>\n",
    "\n",
    "* JSON (JavaScript Object Notation) is a popular data format used for representing structured data. \n",
    "* It is a text format that is language independent and can be used in Python, Perl among other languages. \n",
    "* JSON format is used for data communications between servers and web applications.\n",
    "* It is built on two structures:\n",
    "\n",
    "     - A collection of name/value pairs. This is realized as an object, record, dictionary, hash table, keyed list, or associative array.\n",
    "     - An ordered list of values. This is realized as an array, vector, list, or sequence.\n",
    "     \n",
    "     \n",
    "The main functions of `JSON` are:\n",
    "\n",
    "* `dump()`: encoded string writing on file.\n",
    "* `load()`: Decode while JSON file read.\n",
    "* `dumps()`: encoding to JSON objects\n",
    "* `loads()`: Decode the JSON string.\n",
    "\n",
    "**Example of JSON Data**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"stations\": [\n",
    "        {\n",
    "            \"acronym\": “BLD”, \n",
    "            \"name\": \"Boulder Colorado\",\n",
    "            \"latitude”: 40.00,\n",
    "            \"longitude”: -105.25\n",
    "        }, \n",
    "        {\n",
    "            \"acronym”: “BHD”, \n",
    "            \"name\": \"Baring Head Wellington New Zealand\",\n",
    "            \"latitude\": -41.28,\n",
    "            \"longitude\": 174.87\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Another Example of JSON Data**\n",
    "\n",
    "We consider an online database, <a href=\"IP-API.com\">IP-API.com</a>, that returns GeoIP data in JSON format. Simply opening <a href=\"http://ip-api.com/json/54.148.84.95\">http://ip-api.com/json/54.148.84.95</a> will return the following JSON result:\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"as\": \"AS16509 Amazon.com, Inc.\",\n",
    "  \"city\": \"Boardman\",\n",
    "  \"country\": \"United States\",\n",
    "  \"countryCode\": \"US\",\n",
    "  \"isp\": \"Amazon\",\n",
    "  \"lat\": 45.8696,\n",
    "  \"lon\": -119.688,\n",
    "  \"org\": \"Amazon\",\n",
    "  \"query\": \"54.148.84.95\",\n",
    "  \"region\": \"OR\",\n",
    "  \"regionName\": \"Oregon\",\n",
    "  \"status\": \"success\",\n",
    "  \"timezone\": \"America\\/Los_Angeles\",\n",
    "  \"zip\": \"97818\"\n",
    "}\n",
    "```\n",
    "\n",
    "To see your own Geolocation data in JSON format, just open <a href=\"http://ip-api.com/json/\">http://ip-api.com/json/</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.request(method='GET', url=\"http://ip-api.com/json/\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Serialization and Deserialization</font>\n",
    "\n",
    "> … the process of translating data structures or object state into a format that can be stored … or transmitted … and reconstructed later (possibly in a different computer environment). (Wikipedia)\n",
    "\n",
    "* **Serialization** is a process of transforming objects or data structures into byte streams or strings. \n",
    "* These byte streams can then be stored or transferred easily. \n",
    "* This allows the developers to save, for example, configuration data or user's progress, and then store it (on disk or in a database) or send it to another location.\n",
    "* The reverse process of serialization is known as **deserialization**.\n",
    "\n",
    "### Why do we need serialization?\n",
    "\n",
    "We need Serialization for the following reasons:\n",
    "\n",
    "- **Communication**: Serialization involves the procedure of object serialization and transmission. This enables multiple computer systems to design, share and execute objects simultaneously.\n",
    "- **Caching**: The time consumed in building an object is more compared to the time required for de-serializing it. Serialization minimizes time consumption by caching the giant objects.\n",
    "- **Deep Copy**: Cloning process is made simple by using Serialization. An exact replica of an object is obtained by serializing the object to a byte array, and then de-serializing it.\n",
    "- **Portability**: The major advantage of Serialization is that it works across different architectures or Operating Systems.\n",
    "- **Persistence**: The State of any object can be directly stored by applying Serialization on to it and stored in a database so that it can be retrieved later.\n",
    "\n",
    "![fig_sd](https://miro.medium.com/max/1150/1*9zJJ65xk8agiQXlqd7nYUw.jpeg)\n",
    "Image Source: Phonlawat Khunphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serialization**\n",
    "\n",
    "We use the `dump()` that takes two arguments: \n",
    "* The data object to be serialized.\n",
    "* The file object to which it will be written (Byte format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "  \"name\": \"John\",\n",
    "  \"age\": 30,\n",
    "  \"married\": True,\n",
    "  \"divorced\": False,\n",
    "  \"children\": (\"Ann\",\"Billy\"),\n",
    "  \"pets\": None,\n",
    "  \"cars\": [\n",
    "    {\"model\": \"BMW 230\", \"mpg\": 27.5},\n",
    "    {\"model\": \"Ford Edge\", \"mpg\": 24.1}\n",
    "  ]\n",
    "}\n",
    "\n",
    "file_name = \"Sample.json\"\n",
    "with open(file_name, \"w\") as fid: \n",
    "     json.dump(x, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Sample.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deserializing JSON**\n",
    "\n",
    "* The Deserialization is opposite of Serialization, i.e. conversion of JSON object into their respective Python objects. \n",
    "* We use the `load()` function which is usually used to load from string, otherwise the root object is in list or dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name, \"r\") as fid: \n",
    "     z = json.load(fid)\n",
    "        \n",
    "print(z)\n",
    "print()\n",
    "for key in z:\n",
    "    print(\"{:>12}: {}\".format(key, z[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Scraping the NASA Astronomy Picture Of the Day (APOD) Webpage </font>\n",
    "\n",
    "- We want to be able to obtain from the webpage <a href=\"https://api.nasa.gov/planetary/apod\"> https://api.nasa.gov/planetary/apod</a>,  the Astronomy picture of the day for a given day and plot the image.\n",
    "- We access the webpage (using a set of parameters) and retrieve the content of the page as a JSON object.\n",
    "\n",
    "**Query Parameters**\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "| --- | --- | --- | --- |\n",
    "|`date` | YYYY-MM-DD | today | Date of the APOD image to retrieve |\n",
    "|`start_date` | YYYY-MM-DD | none | The start of a date range, when requesting date for a range of dates. Cannot be used with `date`. |\n",
    "|`end_date` | YYYY-MM-DD | today | The end of the date range, when used with `start_date`. |\n",
    "| `count` |\tint\t| none\t| If this is specified then count randomly chosen images will be returned. Cannot be used with `date` or `start_date` and `end_date`. |\n",
    "| `hd` | bool | False | Retrieve the URL for the high resolution image |\n",
    "| `api_key` | string | DEMO_KEY | <a href=\"https://api.nasa.gov/\">[https://api.nasa.gov/</a> key for expanded usage |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.nasa.gov/planetary/apod\"\n",
    "date = \"2020-12-26\"\n",
    "payload = {'api_key': \"DEMO_KEY\",\n",
    "          'date': date,\n",
    "          'hd': True}\n",
    "\n",
    "page_content = reqs.get(url, params=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data with JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if page_content.status_code == 200:\n",
    "   json_page = json.loads(page_content.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APOD variable is a dictionary of various keys and values. Let’s take a look at the keys of this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the keys and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(\"{} --> {}\".format(x, json_page[x]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if json_page[\"media_type\"] == \"image\":\n",
    "    io.imshow(io.imread(json_page[\"url\"]))\n",
    "    io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">If you want to download the file on your local system:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "url_name = json_page[\"url\"]\n",
    "loc_file_name = os.path.basename(url_name)\n",
    "\n",
    "urllib.request.urlretrieve(url_name, loc_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to view the image through a browser, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(json_page[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Obtaining Mars Rover Photos</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rover_url = 'https://api.nasa.gov/mars-photos/api/v1/rovers/curiosity/photos'\n",
    "\n",
    "payload = {'api_key': \"DEMO_KEY\",\n",
    "           'sol': 1000}\n",
    "\n",
    "response = reqs.get(rover_url, params=payload)\n",
    "response_dictionary = response.json()\n",
    "photos = response_dictionary['photos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(photos))\n",
    "print(len(photos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(photos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the URL of each photo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_photos = list()\n",
    "for photo in photos:\n",
    "    url_photos.append(photo['img_src'])\n",
    "\n",
    "print(url_photos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 20 pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "url_pictures = random.sample(url_photos, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the 20 photos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 5, figsize=(20, 20))\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(20):\n",
    "    ax[i].imshow(io.imread(url_pictures[i]))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Use the following code to list all the images in the provided year range:\n",
    "\n",
    "```python\n",
    "url = \"https://images-api.nasa.gov/search\"\n",
    "\n",
    "payload = {\n",
    "        \"q\": \"apollo\",\n",
    "        \"page\": \"1\",\n",
    "        \"media_type\": \"image\",\n",
    "        \"year_start\": \"2018\",\n",
    "        \"year_end\": \"2020\"}\n",
    "\n",
    "response = reqs.get(url, params=payload)\n",
    "response_dictionary = response.json()[\"collection\"][\"items\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "import requests as reqs\n",
    "\n",
    "url = \"https://images-api.nasa.gov/search\"\n",
    "\n",
    "params = {\n",
    "    \"q\": \"apollo\",\n",
    "    \"page\": \"1\",\n",
    "    \"media_type\": \"image\",\n",
    "    \"year_start\": \"2018\",\n",
    "    \"year_end\": \"2020\"\n",
    "}\n",
    "\n",
    "response = reqs.get(url, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "images = response.json()[\"collection\"][\"items\"]\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "for image in images:\n",
    "    thumbnail_url = image[\"links\"][0][\"href\"]\n",
    "    image_url = thumbnail_url[:thumbnail_url.rfind(\"~\")] + \"~orig.jpg\"\n",
    "    print(image_url)\n",
    "``` \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Scraping the Earth Observatory Natural Event Tracker (EONET) Webpage </font>\n",
    "\n",
    "- We want to be able to browse the webpage <a href=\"https://eonet.sci.gsfc.nasa.gov/api/v2.1/events\"> https://eonet.sci.gsfc.nasa.gov/api/v2.1/events</a>,  to identify natural events on Earth.\n",
    "\n",
    "**Query Parameters**\n",
    "\n",
    "| Parameter | Value(s) |  Description |\n",
    "| --- | --- | --- |\n",
    "|`source` | Source ID | Filter the returned events by the <a href=\"https://eonet.sci.gsfc.nasa.gov/api/v2.1/sources\">Source</a>. Multiple sources can be included in the parameter: comma separated, operates as a boolean OR. |\n",
    "|`status` | open or closed | Events that have ended are assigned a closed date and the existence of that date will allow you to filter for only-open or only-closed events. Omitting the status parameter will return only the currently open events. |\n",
    "| `limit` | int | Limits the number of events returned |\n",
    "| `days ` | int | Limit the number of prior days (including today) from which events will be returned. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://eonet.sci.gsfc.nasa.gov/api/v2.1/events\"\n",
    "payload = {'source': \"EO\",\n",
    "          'status': \"open\",\n",
    "          'limit': 6,\n",
    "          'days': 20}\n",
    "\n",
    "page_content = reqs.get(url, params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if page_content.status_code == 200:\n",
    "    json_page = json.loads(page_content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_page['events'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Web Scraping with Beautiful Soup</font>\n",
    "\n",
    "- Web scraping allows you to download the HTML of a website and extract the data that you need.\n",
    "- Beautiful Soup is a Python library for scraping data from websites.\n",
    "- Beautiful Soup creates a parse tree from parsed HTML and XML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = reqs.get(\"http://www.webcode.me\")\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a beautiful soup object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the the HTML content of the page using the `prettify` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the title section of the page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get attribute name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get attribute values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beginning navigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting specific tags**\n",
    "- HTML is made up of <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element\">tags</a>. It stores all of it’s data in them, and in the midst of all that clutter lies the data we need. Some of the tags are:\n",
    "     * `head`\n",
    "     * `body`\n",
    "     * `title`\n",
    "     * `p` - for paragraph\n",
    "     * `div` — indicates a division, or area, of the page.\n",
    "     * `b` — bolds any text inside.\n",
    "     * `i` — italicizes any text inside.\n",
    "     * `table` — creates a table.\n",
    "     * `form` — creates an input form.\n",
    "- The `find` method searches for the first tag with the needed name.\n",
    "- The `find_all` method searches for all tags with the needed tag name and returns them as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we want to find paragraph tags `<p>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mysoup.find('p')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mysoup.find_all('p')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the last paragraph only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paragraph in enumerate(mysoup.find_all('p'), start=1):\n",
    "    print(\"Paragraph Text {}: {}\".format(i, paragraph.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = mysoup.find_all('body')\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type body:       \", type(body))\n",
    "print(\"Type inner body: \", type(body[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(body[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('body')[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab the text**\n",
    "\n",
    "- Use the method `get_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching for tags by class and id**\n",
    "\n",
    "- Classes and ids are used by CSS to determine which HTML elements to apply certain styles to. \n",
    "- We can also use them when scraping to specify specific elements we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\"\n",
    "source = reqs.get(url)\n",
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `find_all` method to search for items by `class` or by `id`. \n",
    "- In the below example, we’ll search for any `p` tag that has the class `outer-text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for any tag that has the class `outer-text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")[-1].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for elements by id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(id=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(id=\"first\")[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using CSS Selectors**\n",
    "\n",
    "- <a href=\"https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors\">CSS (Cascading Style Sheets)</a> is a declarative language that controls how webpages look in the browser. \n",
    "- The browser applies CSS style declarations to selected elements to display them properly. \n",
    "- A style declaration contains the properties and their values, which determine how a webpage looks.\n",
    "- CSS selectors</a> are how the CSS language allows developers to specify HTML tags to style. \n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "- `p a` — finds all `a` tags inside of a `p` tag.\n",
    "- `body p a` — finds all `a` tags inside of a `p` tag inside of a `body` tag.\n",
    "- `html body` — finds all `body` tags inside of an `html` tag.\n",
    "- `p.outer-text` — finds all `p` tags with a class of `outer-text`.\n",
    "- `p#first` — finds all `p` tags with an id of `first`.\n",
    "- `body p.outer-text` — finds any `p` tags with a class of `outer-text` inside of a `body` tag.\n",
    "\n",
    "We can use the CSS selectors to search items inside webpages. `BeautifulSoup` objects support searching a page via CSS selectors using the `select` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `p` tags in our page that are inside of a `body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"body p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `b` tags in our page that are inside of a `p`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"p b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `b` tags inside of a `p` tag inside of a `body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"body p b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `p` tags with an id of `first`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select('p#first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Example: Extract the web link of the Astronomy Picture of the Day</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://apod.nasa.gov/apod/astropix.html\"\n",
    "source = reqs.get(url)\n",
    "mysoup = bso(source.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print basic information of the Image of the Day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('p').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_comments = mysoup.find_all('a')\n",
    "for a in href_comments:\n",
    "    print(a.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assume that the Picture of the Day is a video. If it is not the case, we will skip the next six cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.iframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(str(mysoup.iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.iframe['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = [a['src'] for a in mysoup.select('iframe[src]')]\n",
    "src_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tags = mysoup.find_all(src=True)\n",
    "src_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `href` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_tags = mysoup.find_all(href=True)\n",
    "href_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = [l.get('href') for l in mysoup.find_all('a')]\n",
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = [a['href'] for a in mysoup.select('a[href]')]\n",
    "link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Go to the webpage:\n",
    "\n",
    "[https://astg606.github.io/py_courses/summer_2021/](https://astg606.github.io/py_courses/summer_2021/)\n",
    "\n",
    "and extract the `Course Evaluation` web link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import requests as reqs\n",
    "\n",
    "from bs4 import BeautifulSoup as bso\n",
    "\n",
    "URL = \"https://astg606.github.io/py_courses/summer_2021/\"\n",
    "\n",
    "source = reqs.get(URL)\n",
    "if source.status_code == 200:\n",
    "    mysoup = bso(source.content, 'html.parser')\n",
    "    href_tags = mysoup.find_all(href=True)\n",
    "    for tag in href_tags:\n",
    "        if tag.get_text() == \"Course Evaluation\":\n",
    "            print(tag[\"href\"])\n",
    "else:\n",
    "    print(\"URL not accessible.\")\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Example: Weather Data for Greenbelt, Maryland</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://forecast.weather.gov/MapClick.php\"\n",
    "params = {'lat': 39.00079000000005,\n",
    "          'lon': -76.88055999999995}\n",
    "\n",
    "source = reqs.get(url, params=params)\n",
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Tonight's Forecast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day = mysoup.find(id=\"seven-day-forecast\")\n",
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
    "tonight = forecast_items[0]\n",
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = tonight.find(class_=\"period-name\").get_text()\n",
    "print(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
    "print(short_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tonight.find(class_=\"temp\").get_text()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tonight.find(\"img\")\n",
    "desc = img['title']\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting all Data**\n",
    "\n",
    "We use CSS selectors to extract everything at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select all items with the class `period-name` inside an item with the class `tombstone-container` in `seven_day`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "periods = [pt.get_text() for pt in period_tags]\n",
    "print(periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same technique to get the other fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_descs = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
    "print(short_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")\n",
    "#print(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
    "print(descs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the data into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_weather = pd.DataFrame({\n",
    "    \"period\": periods,\n",
    "    \"short_desc\": short_descs,\n",
    "    #\"temp\": temps,\n",
    "    \"desc\":descs\n",
    "})\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Forecast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_forecast = mysoup.find(id=\"detailed-forecast-body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_labels = det_forecast.find_all(class_=\"col-sm-2 forecast-label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_texts = det_forecast.find_all(class_=\"col-sm-10 forecast-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(forecast_labels, forecast_texts):\n",
    "    print(\"\\033[1m {:>15}: \\033[0m {:<}\".format(a.get_text(), b.get_text()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 \n",
    "\n",
    "- Go to the sitethe website `https://eonet.sci.gsfc.nasa.gov/api/v2.1/events`\n",
    "- Select a date range and the number of events you want to retrieve.\n",
    "- Creade a Pandas DataFrame that contains as columns the event type, date, latitude and longitude.\n",
    "\n",
    "```python\n",
    "url = \"https://eonet.sci.gsfc.nasa.gov/api/v2.1/events\"\n",
    "payload = {'source': \"EO\",\n",
    "          'status': \"open\",\n",
    "          'limit': 6,\n",
    "          'days': 100}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import json\n",
    "\n",
    "url = \"https://eonet.sci.gsfc.nasa.gov/api/v2.1/events\"\n",
    "payload = {'source': \"EO\",\n",
    "          'status': \"open\",\n",
    "          'limit': 6,\n",
    "          'days': 100}\n",
    "\n",
    "page_content = reqs.get(url, params=payload)\n",
    "\n",
    "if page_content.status_code == 200:\n",
    "    json_page = json.loads(page_content.text)\n",
    "\n",
    "for x in json_page:\n",
    "    print(x)\n",
    "\n",
    "list_events = json_page['events']\n",
    "\n",
    "print(f\"Number of events: {len(list_events)}\")\n",
    "print(f\"List of events: \\n {list_events}\")\n",
    "\n",
    "event_types = [evt['categories'][0]['title'] for evt in list_events]\n",
    "event_dates = [evt['geometries'][0]['date'] for evt in list_events]\n",
    "event_lons = [evt['geometries'][0]['coordinates'][0] for evt in list_events]\n",
    "event_lats = [evt['geometries'][0]['coordinates'][1] for evt in list_events]\n",
    "\n",
    "print()\n",
    "\n",
    "import pandas as pd\n",
    "df_events = pd.DataFrame({\n",
    "    \"Type\": event_types,\n",
    "    \"Dates\": event_dates,\n",
    "    #\"Latitudes\": event_lats,\n",
    "    \"Longitudes\":event_lons\n",
    "})\n",
    "df_events\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Example: MODIS Aerosol Optical Thickness</font>\n",
    "\n",
    "- Scientists use measurements from the MODIS sensor aboard NASA's Terra and Aqua satellites to map the amount of aerosol that is in the air all over the world. Because aerosols reflect visible and near-infrared light back to space, scientists can use satellites to make maps of where there are high concentrations of these particles.\n",
    "- Scientists call this measurement aerosol optical thickness (AOT). \n",
    "- It is a measure of how much light the airborne particles prevent from traveling through the atmosphere. \n",
    "- Aerosols absorb and scatter incoming sunlight, thus reducing visibility and increasing optical thickness. An optical thickness of less than 0.1 indicates a crystal clear sky with maximum visibility, whereas a value of 1 indicates the presence of aerosols so dense that people would have difficulty seeing the Sun, even at mid-day!\n",
    "\n",
    "\n",
    "In this example, we want to access the <a href=\"https://neo.sci.gsfc.nasa.gov/\">NASA Earth Observations (NEO)</a> website to obtain the AOT measurements for a given day or a range of days (from 2000 to present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the day range of interest:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "beg_date = '2019-12-30'\n",
    "end_date = '2019-12-31'\n",
    "\n",
    "pd_series = pd.date_range(start=beg_date, end=end_date, freq='D')\n",
    "dates = [dt.strftime('%Y-%m-%d') for dt in pd_series]\n",
    "\n",
    "url_base = \"https://neo.sci.gsfc.nasa.gov/view.php?datasetId=MODAL2_M_AER_OD&year=\"\n",
    "\n",
    "urls = [url_base+dt for dt in dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access the webpage for the first day:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = reqs.get(urls[0])\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the webpage and print its content:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather all the lines with `href` tag:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_tags = mysoup.find_all(href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the `http` address that has the word `CSV`. That will give us the remote location of the file we want to read.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in href_tags:\n",
    "    loc_url = tag[\"href\"]\n",
    "    if \"CSV\" in loc_url:\n",
    "        csv_url = loc_url\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `Pandas` to read the remote file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_url, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that `99999.0` corresponds to a missing value. Let us replace it with `NaN`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_url, index_col=0, na_values=99999.0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use `Xarray` to quickly visualize the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "da = xr.DataArray(df.values,\n",
    "                  coords=[[float(lat) for lat in df.index], \n",
    "                          [float(lon) for lon in df.columns]],\n",
    "                  dims=['latitude', 'longitude'])\n",
    "\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Application</font>\n",
    "\n",
    "- We want to get all book names on historic New York Time Best Sellers (Business section)\n",
    "- The purpose is to:\n",
    "     1. Help to compile my reading list in 2020\n",
    "     2. Serve as reference to use Python for simple web analytics\n",
    "- We use the Python packages: `Pandas`, `Requests` and `Baeutiful Soup`\n",
    "- We save data in `pickle` and `csv` formats.\n",
    "\n",
    "The example was taken from: <a href=\"https://towardsdatascience.com/building-my-2020-reading-list-with-a-simple-python-script-b610c7f2c223\">Building my 2020 reading list with a simple Python script</a> by Pan Wu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty Pandas dataframe\n",
    "nylist = pd.DataFrame()\n",
    "\n",
    "beg_year = 2013\n",
    "end_year = 2020\n",
    "for the_year in range(beg_year, end_year):\n",
    "    for the_month in range(1, 13):\n",
    "        cur_month = str(the_month).zfill(2) # month in two digits\n",
    "        # one need to get the URL pattern first, and then use Requests package to get the URL content\n",
    "        url = 'https://www.nytimes.com/books/best-sellers/{0}/{1}/01/business-books/'.format(the_year, cur_month)\n",
    "        page = reqs.get(url)\n",
    "        print(\" --  try: {0}, {1} -- \".format(the_year, cur_month))\n",
    "        \n",
    "        # Ensure proper result is returned\n",
    "        if page.status_code != 200:\n",
    "            print(\"      Missing data for Year {} and Month {}\".format(the_year, cur_month))\n",
    "            continue\n",
    "        \n",
    "        # one may want to use BeautifulSoup to parse the right elements out\n",
    "        soup = bso(page.text, 'html.parser')\n",
    "        \n",
    "        # the specific class names are unique for this URL and they don't change across all URLs\n",
    "        top_list = soup.findAll(\"ol\", {\"class\": \"css-12yzwg4\"})[0].findAll(\"div\", {\"class\": \"css-xe4cfy\"})\n",
    "        print(\"Year: {} - Month: {} - How many in the top list: {}\".format(the_year, the_month, len(top_list)))\n",
    "        \n",
    "        # loop through the Best Seller list in each Year-Month, and append the information into a pandas DataFrame\n",
    "        for i in range(len(top_list)):\n",
    "            book   = top_list[i].contents[0]\n",
    "            title  = book.findAll(\"h3\", {\"class\": \"css-5pe77f\"})[0].text\n",
    "            author = book.findAll(\"p\",  {\"class\": \"css-hjukut\"})[0].text\n",
    "            review = book.get(\"href\")\n",
    "            # print(\"{0}, {1}; review: {2}\".format(title, author, review))\n",
    "            one_item = pd.Series([the_year, the_month, title, author, i+1, review], \n",
    "                                 index=['year', 'month', 'title', 'author', 'rank', 'review'])\n",
    "            nylist = nylist.append(one_item, ignore_index=True, sort=False)\n",
    "\n",
    "# write out the result to a pickle file for easy analysis later.\n",
    "nylist.to_pickle(\"nylist.pkl\")\n",
    "nylist.to_csv(\"nylist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "- Write a Python script that reads the pickle file `nylist.pkl` or the csv file `nylist.csv` and prints its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
